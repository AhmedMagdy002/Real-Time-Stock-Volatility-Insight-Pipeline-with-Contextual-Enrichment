{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c847ac-c457-46ae-95bc-f8e60b2ce4fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Bucket created!\n",
      "✅ Job completed!\n",
      "\n",
      "Files in bucket:\n",
      "  - output.parquet/_SUCCESS\n",
      "  - output.parquet/part-00000-c1138f74-f87f-4e02-ba73-140ad38069cf-c000.snappy.parquet\n",
      "  - output.parquet/part-00001-c1138f74-f87f-4e02-ba73-140ad38069cf-c000.snappy.parquet\n",
      "  - output.parquet/part-00002-c1138f74-f87f-4e02-ba73-140ad38069cf-c000.snappy.parquet\n",
      "  - output.parquet/part-00003-c1138f74-f87f-4e02-ba73-140ad38069cf-c000.snappy.parquet\n",
      "  - output.parquet/part-00004-c1138f74-f87f-4e02-ba73-140ad38069cf-c000.snappy.parquet\n",
      "  - output.parquet/part-00005-c1138f74-f87f-4e02-ba73-140ad38069cf-c000.snappy.parquet\n",
      "  - output.parquet/part-00006-c1138f74-f87f-4e02-ba73-140ad38069cf-c000.snappy.parquet\n",
      "  - output.parquet/part-00007-c1138f74-f87f-4e02-ba73-140ad38069cf-c000.snappy.parquet\n",
      "  - output.parquet/part-00008-c1138f74-f87f-4e02-ba73-140ad38069cf-c000.snappy.parquet\n",
      "  - output.parquet/part-00009-c1138f74-f87f-4e02-ba73-140ad38069cf-c000.snappy.parquet\n",
      "  - output.parquet/part-00010-c1138f74-f87f-4e02-ba73-140ad38069cf-c000.snappy.parquet\n",
      "  - output.parquet/part-00011-c1138f74-f87f-4e02-ba73-140ad38069cf-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "%run /home/jovyan/work/testing.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adafd712-f11d-44a0-a186-9a8be0131af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Spark is ready!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TestS3\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.4\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://localstack:4566\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"test\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"test\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"✅ Spark is ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40e1617f-8d0c-407a-afa0-564c5638f5be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| id|doubled|\n",
      "+---+-------+\n",
      "| 16|     32|\n",
      "| 17|     34|\n",
      "| 18|     36|\n",
      "| 19|     38|\n",
      "| 20|     40|\n",
      "| 21|     42|\n",
      "| 22|     44|\n",
      "| 23|     46|\n",
      "| 24|     48|\n",
      "| 41|     82|\n",
      "| 42|     84|\n",
      "| 43|     86|\n",
      "| 44|     88|\n",
      "| 45|     90|\n",
      "| 46|     92|\n",
      "| 47|     94|\n",
      "| 48|     96|\n",
      "| 49|     98|\n",
      "| 66|    132|\n",
      "| 67|    134|\n",
      "+---+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read back the data\n",
    "df_read = spark.read.parquet(\"s3a://test-bucket/output.parquet\")\n",
    "df_read.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560e00eb-5b68-4ed3-9a87-11f5cedf56a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2268dd3-131b-41c5-a457-f13df5397c87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1b35b47a",
   "metadata": {},
   "source": [
    "### Kafka spark\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfc6d8c4-b887-4bad-add8-345b975247fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting boto3\n",
      "  Obtaining dependency information for boto3 from https://files.pythonhosted.org/packages/78/b6/c717416e67b1bef84be9c05f2686d353544eda2af5b43af06996a43f6481/boto3-1.39.12-py3-none-any.whl.metadata\n",
      "  Downloading boto3-1.39.12-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting botocore<1.40.0,>=1.39.12 (from boto3)\n",
      "  Obtaining dependency information for botocore<1.40.0,>=1.39.12 from https://files.pythonhosted.org/packages/c8/af/3b82594c5fa26464b548da35a6affe27a44993f65c4149ed8cf8a5df8387/botocore-1.39.12-py3-none-any.whl.metadata\n",
      "  Downloading botocore-1.39.12-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from boto3)\n",
      "  Obtaining dependency information for jmespath<2.0.0,>=0.7.1 from https://files.pythonhosted.org/packages/31/b4/b9b800c45527aadd64d5b442f9b932b00648617eb5d63d2c7a6587b7cafc/jmespath-1.0.1-py3-none-any.whl.metadata\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting s3transfer<0.14.0,>=0.13.0 (from boto3)\n",
      "  Obtaining dependency information for s3transfer<0.14.0,>=0.13.0 from https://files.pythonhosted.org/packages/6d/4f/d073e09df851cfa251ef7840007d04db3293a0482ce607d2b993926089be/s3transfer-0.13.1-py3-none-any.whl.metadata\n",
      "  Downloading s3transfer-0.13.1-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.11/site-packages (from botocore<1.40.0,>=1.39.12->boto3) (2.8.2)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /opt/conda/lib/python3.11/site-packages (from botocore<1.40.0,>=1.39.12->boto3) (2.0.4)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.40.0,>=1.39.12->boto3) (1.16.0)\n",
      "Downloading boto3-1.39.12-py3-none-any.whl (139 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.9/139.9 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading botocore-1.39.12-py3-none-any.whl (13.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.9/13.9 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Downloading s3transfer-0.13.1-py3-none-any.whl (85 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.3/85.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: jmespath, botocore, s3transfer, boto3\n",
      "Successfully installed boto3-1.39.12 botocore-1.39.12 jmespath-1.0.1 s3transfer-0.13.1\n"
     ]
    }
   ],
   "source": [
    "!pip install boto3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74aca7f8-c8c9-4153-b5c4-4438a157b5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket created!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import boto3\n",
    "s3 = boto3.client('s3', endpoint_url='http://localstack:4566', \n",
    "                  aws_access_key_id='test', aws_secret_access_key='test')\n",
    "s3.create_bucket(Bucket='test-bucket')\n",
    "print(\"Bucket created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b1d9a14-bfb5-4afd-bd8d-b9d8e6fd0446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kafka-python\n",
      "  Obtaining dependency information for kafka-python from https://files.pythonhosted.org/packages/e6/35/e8bfed5425e8fe685bd03ec3f5135ee8b88c11558baa59c0d12fbd2a20ae/kafka_python-2.2.15-py2.py3-none-any.whl.metadata\n",
      "  Downloading kafka_python-2.2.15-py2.py3-none-any.whl.metadata (10.0 kB)\n",
      "Downloading kafka_python-2.2.15-py2.py3-none-any.whl (309 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.8/309.8 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hInstalling collected packages: kafka-python\n",
      "Successfully installed kafka-python-2.2.15\n"
     ]
    }
   ],
   "source": [
    "!pip install kafka-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb6f4b82-00e4-4f5a-9704-4db910db3bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic created!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from kafka.admin import KafkaAdminClient, NewTopic\n",
    "\n",
    "admin = KafkaAdminClient(bootstrap_servers='kafka:29092')\n",
    "topic = NewTopic(name='test-topic', num_partitions=1, replication_factor=1)\n",
    "admin.create_topics([topic])\n",
    "print(\"Topic created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e47686bf-f20a-4b75-814e-4115b52267cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent: {'id': 0, 'name': 'Message 0', 'value': 0}\n",
      "Sent: {'id': 1, 'name': 'Message 1', 'value': 100}\n",
      "Sent: {'id': 2, 'name': 'Message 2', 'value': 200}\n",
      "Sent: {'id': 3, 'name': 'Message 3', 'value': 300}\n",
      "Sent: {'id': 4, 'name': 'Message 4', 'value': 400}\n",
      "Sent: {'id': 5, 'name': 'Message 5', 'value': 500}\n",
      "Sent: {'id': 6, 'name': 'Message 6', 'value': 600}\n",
      "Sent: {'id': 7, 'name': 'Message 7', 'value': 700}\n",
      "Sent: {'id': 8, 'name': 'Message 8', 'value': 800}\n",
      "Sent: {'id': 9, 'name': 'Message 9', 'value': 900}\n",
      "Done sending messages!\n"
     ]
    }
   ],
   "source": [
    "%run /home/jovyan/work/simple_producer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538079dc-4609-433e-8b3b-c2aabcfb1f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Spark session created\n",
      "📖 Reading from Kafka...\n",
      "\n",
      "📊 Data from Kafka:\n",
      "+---+---------+-----+\n",
      "| id|     name|value|\n",
      "+---+---------+-----+\n",
      "|  0|Message 0|    0|\n",
      "|  1|Message 1|  100|\n",
      "|  2|Message 2|  200|\n",
      "|  3|Message 3|  300|\n",
      "|  4|Message 4|  400|\n",
      "|  5|Message 5|  500|\n",
      "|  6|Message 6|  600|\n",
      "|  7|Message 7|  700|\n",
      "|  8|Message 8|  800|\n",
      "|  9|Message 9|  900|\n",
      "+---+---------+-----+\n",
      "\n",
      "\n",
      "💾 Writing to Delta Lake at: s3a://test-bucket/delta-table\n",
      "✅ Data saved to Delta Lake!\n",
      "\n",
      "📖 Reading back from Delta Lake:\n",
      "+---+---------+-----+\n",
      "| id|     name|value|\n",
      "+---+---------+-----+\n",
      "|  0|Message 0|    0|\n",
      "|  1|Message 1|  100|\n",
      "|  2|Message 2|  200|\n",
      "|  3|Message 3|  300|\n",
      "|  4|Message 4|  400|\n",
      "|  5|Message 5|  500|\n",
      "|  6|Message 6|  600|\n",
      "|  7|Message 7|  700|\n",
      "|  8|Message 8|  800|\n",
      "|  9|Message 9|  900|\n",
      "+---+---------+-----+\n",
      "\n",
      "\n",
      "✅ Test completed successfully!\n"
     ]
    }
   ],
   "source": [
    "%run /home/jovyan/work/simple_spark_consumer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587d7973-640b-4f56-8ba0-334748ae3c63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f91ad21-0237-4470-95b1-76e21da7a99c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fc5e868-5c51-445a-b6c7-cec0af444859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in s3://test-bucket/delta-table/:\n",
      " - delta-table/_delta_log/00000000000000000000.json (1248 bytes)\n",
      " - delta-table/part-00000-354a5e3f-e191-438b-bbac-aec33463ff8b-c000.snappy.parquet (1177 bytes)\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "# Configure boto3 for LocalStack\n",
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    endpoint_url='http://localstack:4566',\n",
    "    aws_access_key_id='test',\n",
    "    aws_secret_access_key='test',\n",
    "    region_name='us-east-1'\n",
    ")\n",
    "\n",
    "# List objects in the Delta table path\n",
    "try:\n",
    "    response = s3_client.list_objects_v2(Bucket='test-bucket', Prefix='delta-table/')\n",
    "    if 'Contents' in response:\n",
    "        print(\"Files in s3://test-bucket/delta-table/:\")\n",
    "        for obj in response['Contents']:\n",
    "            print(f\" - {obj['Key']} ({obj['Size']} bytes)\")\n",
    "    else:\n",
    "        print(\"No files found in s3://test-bucket/delta-table/ or bucket does not exist.\")\n",
    "except s3_client.exceptions.NoSuchBucket:\n",
    "    print(\"Bucket 'test-bucket' does not exist. Creating it...\")\n",
    "    s3_client.create_bucket(Bucket='test-bucket')\n",
    "    print(\"Bucket created. Rerun the Spark job to write the Delta table.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3461fa-5cdc-4560-8e78-7e020b78a94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark session with Delta support\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SimpleTest\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.jars.packages\", \n",
    "            \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1,\"\n",
    "            \"io.delta:delta-core_2.12:2.4.0,\"\n",
    "            \"org.apache.hadoop:hadoop-aws:3.3.4,\"\n",
    "            \"com.amazonaws:aws-java-sdk-bundle:1.12.262\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://localstack:4566\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"test\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"test\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .getOrCreate()\n",
    "output_path = \"s3a://test-bucket/delta-table\"\n",
    "print(\"\\n📖 Reading back from Delta Lake:\")\n",
    "delta_df = spark.read.format(\"delta\").load(output_path)\n",
    "delta_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3258614a-3415-4299-8156-089c4961974c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdec18f-eea3-4b91-853d-be3bfd2b6b02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "227f47f6-f8b5-4403-8286-087807389ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Spark session created\n",
      "📖 Reading from Kafka...\n",
      "\n",
      "📊 Data from Kafka:\n",
      "+---+---------+-----+\n",
      "| id|     name|value|\n",
      "+---+---------+-----+\n",
      "|  0|Message 0|    0|\n",
      "|  1|Message 1|  100|\n",
      "|  2|Message 2|  200|\n",
      "|  3|Message 3|  300|\n",
      "|  4|Message 4|  400|\n",
      "|  5|Message 5|  500|\n",
      "|  6|Message 6|  600|\n",
      "|  7|Message 7|  700|\n",
      "|  8|Message 8|  800|\n",
      "|  9|Message 9|  900|\n",
      "+---+---------+-----+\n",
      "\n",
      "\n",
      "💾 Writing to Delta Lake at: s3a://test-bucket/delta-tables/my_table\n",
      "✅ Data saved to Delta Lake!\n",
      "\n",
      "📖 Reading back from Delta Lake:\n",
      "+---+---------+-----+\n",
      "| id|     name|value|\n",
      "+---+---------+-----+\n",
      "|  0|Message 0|    0|\n",
      "|  1|Message 1|  100|\n",
      "|  2|Message 2|  200|\n",
      "|  3|Message 3|  300|\n",
      "|  4|Message 4|  400|\n",
      "|  5|Message 5|  500|\n",
      "|  6|Message 6|  600|\n",
      "|  7|Message 7|  700|\n",
      "|  8|Message 8|  800|\n",
      "|  9|Message 9|  900|\n",
      "+---+---------+-----+\n",
      "\n",
      "\n",
      "✅ Test completed successfully!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Create Spark session with Delta support\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SimpleTest\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.jars.packages\", \n",
    "            \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1,\"\n",
    "            \"io.delta:delta-core_2.12:2.4.0,\"\n",
    "            \"org.apache.hadoop:hadoop-aws:3.3.4,\"\n",
    "            \"com.amazonaws:aws-java-sdk-bundle:1.12.262\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://localstack:4566\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"test\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"test\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"✅ Spark session created\")\n",
    "\n",
    "# Read from Kafka\n",
    "print(\"📖 Reading from Kafka...\")\n",
    "df = spark \\\n",
    "    .read \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \\\n",
    "    .option(\"subscribe\", \"test-topic\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "# Parse JSON and select fields\n",
    "parsed_df = df.select(\n",
    "    col(\"value\").cast(\"string\")\n",
    ").select(\n",
    "    get_json_object(col(\"value\"), \"$.id\").alias(\"id\"),\n",
    "    get_json_object(col(\"value\"), \"$.name\").alias(\"name\"),\n",
    "    get_json_object(col(\"value\"), \"$.value\").alias(\"value\")\n",
    ")\n",
    "\n",
    "# Show data\n",
    "print(\"\\n📊 Data from Kafka:\")\n",
    "parsed_df.show()\n",
    "\n",
    "# Write to Delta Lake on S3\n",
    "#output_path = \"s3a://test-bucket/delta-table\"\n",
    "output_path = \"s3a://test-bucket/delta-tables/my_table\"\n",
    "print(f\"\\n💾 Writing to Delta Lake at: {output_path}\")\n",
    "parsed_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(output_path)\n",
    "\n",
    "print(\"✅ Data saved to Delta Lake!\")\n",
    "\n",
    "# Read back from Delta to verify\n",
    "print(\"\\n📖 Reading back from Delta Lake:\")\n",
    "delta_df = spark.read.format(\"delta\").load(output_path)\n",
    "delta_df.show()\n",
    "\n",
    "print(\"\\n✅ Test completed successfully!\")\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e5da1b-26c2-401d-8aa6-d8e11343e62f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
