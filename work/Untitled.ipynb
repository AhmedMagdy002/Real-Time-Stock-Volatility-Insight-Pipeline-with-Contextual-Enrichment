{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c847ac-c457-46ae-95bc-f8e60b2ce4fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Bucket created!\n",
      "✅ Job completed!\n",
      "\n",
      "Files in bucket:\n",
      "  - output.parquet/_SUCCESS\n",
      "  - output.parquet/part-00000-c1138f74-f87f-4e02-ba73-140ad38069cf-c000.snappy.parquet\n",
      "  - output.parquet/part-00001-c1138f74-f87f-4e02-ba73-140ad38069cf-c000.snappy.parquet\n",
      "  - output.parquet/part-00002-c1138f74-f87f-4e02-ba73-140ad38069cf-c000.snappy.parquet\n",
      "  - output.parquet/part-00003-c1138f74-f87f-4e02-ba73-140ad38069cf-c000.snappy.parquet\n",
      "  - output.parquet/part-00004-c1138f74-f87f-4e02-ba73-140ad38069cf-c000.snappy.parquet\n",
      "  - output.parquet/part-00005-c1138f74-f87f-4e02-ba73-140ad38069cf-c000.snappy.parquet\n",
      "  - output.parquet/part-00006-c1138f74-f87f-4e02-ba73-140ad38069cf-c000.snappy.parquet\n",
      "  - output.parquet/part-00007-c1138f74-f87f-4e02-ba73-140ad38069cf-c000.snappy.parquet\n",
      "  - output.parquet/part-00008-c1138f74-f87f-4e02-ba73-140ad38069cf-c000.snappy.parquet\n",
      "  - output.parquet/part-00009-c1138f74-f87f-4e02-ba73-140ad38069cf-c000.snappy.parquet\n",
      "  - output.parquet/part-00010-c1138f74-f87f-4e02-ba73-140ad38069cf-c000.snappy.parquet\n",
      "  - output.parquet/part-00011-c1138f74-f87f-4e02-ba73-140ad38069cf-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "%run /home/jovyan/work/testing.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adafd712-f11d-44a0-a186-9a8be0131af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Spark is ready!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TestS3\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.4\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://localstack:4566\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"test\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"test\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"✅ Spark is ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40e1617f-8d0c-407a-afa0-564c5638f5be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| id|doubled|\n",
      "+---+-------+\n",
      "| 16|     32|\n",
      "| 17|     34|\n",
      "| 18|     36|\n",
      "| 19|     38|\n",
      "| 20|     40|\n",
      "| 21|     42|\n",
      "| 22|     44|\n",
      "| 23|     46|\n",
      "| 24|     48|\n",
      "| 41|     82|\n",
      "| 42|     84|\n",
      "| 43|     86|\n",
      "| 44|     88|\n",
      "| 45|     90|\n",
      "| 46|     92|\n",
      "| 47|     94|\n",
      "| 48|     96|\n",
      "| 49|     98|\n",
      "| 66|    132|\n",
      "| 67|    134|\n",
      "+---+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read back the data\n",
    "df_read = spark.read.parquet(\"s3a://test-bucket/output.parquet\")\n",
    "df_read.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560e00eb-5b68-4ed3-9a87-11f5cedf56a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2268dd3-131b-41c5-a457-f13df5397c87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1b35b47a",
   "metadata": {},
   "source": [
    "### Kafka spark\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfc6d8c4-b887-4bad-add8-345b975247fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting boto3\n",
      "  Obtaining dependency information for boto3 from https://files.pythonhosted.org/packages/51/4a/5d33b6046d425c9b39d36a1171ea87a9c3b297ba116952b81033eae61260/boto3-1.40.11-py3-none-any.whl.metadata\n",
      "  Downloading boto3-1.40.11-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting botocore<1.41.0,>=1.40.11 (from boto3)\n",
      "  Obtaining dependency information for botocore<1.41.0,>=1.40.11 from https://files.pythonhosted.org/packages/2d/f9/400e0da61cbbcea7868458f3a447d1191a62ae5e2852d2acdfd4d51b2843/botocore-1.40.11-py3-none-any.whl.metadata\n",
      "  Downloading botocore-1.40.11-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from boto3)\n",
      "  Obtaining dependency information for jmespath<2.0.0,>=0.7.1 from https://files.pythonhosted.org/packages/31/b4/b9b800c45527aadd64d5b442f9b932b00648617eb5d63d2c7a6587b7cafc/jmespath-1.0.1-py3-none-any.whl.metadata\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting s3transfer<0.14.0,>=0.13.0 (from boto3)\n",
      "  Obtaining dependency information for s3transfer<0.14.0,>=0.13.0 from https://files.pythonhosted.org/packages/6d/4f/d073e09df851cfa251ef7840007d04db3293a0482ce607d2b993926089be/s3transfer-0.13.1-py3-none-any.whl.metadata\n",
      "  Downloading s3transfer-0.13.1-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.11/site-packages (from botocore<1.41.0,>=1.40.11->boto3) (2.8.2)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /opt/conda/lib/python3.11/site-packages (from botocore<1.41.0,>=1.40.11->boto3) (2.0.4)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.41.0,>=1.40.11->boto3) (1.16.0)\n",
      "Downloading boto3-1.40.11-py3-none-any.whl (140 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.1/140.1 kB\u001b[0m \u001b[31m557.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading botocore-1.40.11-py3-none-any.whl (14.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Downloading s3transfer-0.13.1-py3-none-any.whl (85 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.3/85.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: jmespath, botocore, s3transfer, boto3\n",
      "Successfully installed boto3-1.40.11 botocore-1.40.11 jmespath-1.0.1 s3transfer-0.13.1\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests) (2023.7.22)\n"
     ]
    }
   ],
   "source": [
    "!pip install boto3\n",
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74aca7f8-c8c9-4153-b5c4-4438a157b5f5",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mboto3\u001b[39;00m\n\u001b[1;32m      2\u001b[0m s3 \u001b[38;5;241m=\u001b[39m boto3\u001b[38;5;241m.\u001b[39mclient(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms3\u001b[39m\u001b[38;5;124m'\u001b[39m, endpoint_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp://localstack:4566\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m      3\u001b[0m                   aws_access_key_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m, aws_secret_access_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[43ms3\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_bucket\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBucket\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest-bucket\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBucket created!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/botocore/client.py:602\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    598\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    599\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpy_operation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() only accepts keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    600\u001b[0m     )\n\u001b[1;32m    601\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 602\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/botocore/context.py:123\u001b[0m, in \u001b[0;36mwith_current_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hook:\n\u001b[1;32m    122\u001b[0m     hook()\n\u001b[0;32m--> 123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/botocore/client.py:1060\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m   1056\u001b[0m     maybe_compress_request(\n\u001b[1;32m   1057\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeta\u001b[38;5;241m.\u001b[39mconfig, request_dict, operation_model\n\u001b[1;32m   1058\u001b[0m     )\n\u001b[1;32m   1059\u001b[0m     apply_request_checksum(request_dict)\n\u001b[0;32m-> 1060\u001b[0m     http, parsed_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1061\u001b[0m \u001b[43m        \u001b[49m\u001b[43moperation_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_context\u001b[49m\n\u001b[1;32m   1062\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1064\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeta\u001b[38;5;241m.\u001b[39mevents\u001b[38;5;241m.\u001b[39memit(\n\u001b[1;32m   1065\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter-call.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mservice_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moperation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1066\u001b[0m     http_response\u001b[38;5;241m=\u001b[39mhttp,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1069\u001b[0m     context\u001b[38;5;241m=\u001b[39mrequest_context,\n\u001b[1;32m   1070\u001b[0m )\n\u001b[1;32m   1072\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m300\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/botocore/client.py:1084\u001b[0m, in \u001b[0;36mBaseClient._make_request\u001b[0;34m(self, operation_model, request_dict, request_context)\u001b[0m\n\u001b[1;32m   1082\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_make_request\u001b[39m(\u001b[38;5;28mself\u001b[39m, operation_model, request_dict, request_context):\n\u001b[1;32m   1083\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1084\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_endpoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1085\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1086\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeta\u001b[38;5;241m.\u001b[39mevents\u001b[38;5;241m.\u001b[39memit(\n\u001b[1;32m   1087\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter-call-error.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_service_model\u001b[38;5;241m.\u001b[39mservice_id\u001b[38;5;241m.\u001b[39mhyphenize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moperation_model\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1088\u001b[0m             exception\u001b[38;5;241m=\u001b[39me,\n\u001b[1;32m   1089\u001b[0m             context\u001b[38;5;241m=\u001b[39mrequest_context,\n\u001b[1;32m   1090\u001b[0m         )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/botocore/endpoint.py:119\u001b[0m, in \u001b[0;36mEndpoint.make_request\u001b[0;34m(self, operation_model, request_dict)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake_request\u001b[39m(\u001b[38;5;28mself\u001b[39m, operation_model, request_dict):\n\u001b[1;32m    114\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMaking request for \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m with params: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    116\u001b[0m         operation_model,\n\u001b[1;32m    117\u001b[0m         request_dict,\n\u001b[1;32m    118\u001b[0m     )\n\u001b[0;32m--> 119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperation_model\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/botocore/endpoint.py:197\u001b[0m, in \u001b[0;36mEndpoint._send_request\u001b[0;34m(self, request_dict, operation_model)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_retries_context(context, attempts)\n\u001b[1;32m    196\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_request(request_dict, operation_model)\n\u001b[0;32m--> 197\u001b[0m success_response, exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperation_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_needs_retry(\n\u001b[1;32m    201\u001b[0m     attempts,\n\u001b[1;32m    202\u001b[0m     operation_model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    205\u001b[0m     exception,\n\u001b[1;32m    206\u001b[0m ):\n\u001b[1;32m    207\u001b[0m     attempts \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/botocore/endpoint.py:239\u001b[0m, in \u001b[0;36mEndpoint._get_response\u001b[0;34m(self, request, operation_model, context)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_response\u001b[39m(\u001b[38;5;28mself\u001b[39m, request, operation_model, context):\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;66;03m# This will return a tuple of (success_response, exception)\u001b[39;00m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# and success_response is itself a tuple of\u001b[39;00m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;66;03m# (http_response, parsed_dict).\u001b[39;00m\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;66;03m# If an exception occurs then the success_response is None.\u001b[39;00m\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;66;03m# If no exception occurs then exception is None.\u001b[39;00m\n\u001b[0;32m--> 239\u001b[0m     success_response, exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_get_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperation_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    242\u001b[0m     kwargs_to_emit \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    243\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresponse_dict\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    244\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparsed_response\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    245\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m'\u001b[39m: context,\n\u001b[1;32m    246\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexception\u001b[39m\u001b[38;5;124m'\u001b[39m: exception,\n\u001b[1;32m    247\u001b[0m     }\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m success_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/botocore/endpoint.py:279\u001b[0m, in \u001b[0;36mEndpoint._do_get_response\u001b[0;34m(self, request, operation_model, context)\u001b[0m\n\u001b[1;32m    277\u001b[0m     http_response \u001b[38;5;241m=\u001b[39m first_non_none_response(responses)\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m http_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 279\u001b[0m         http_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPClientError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m, e)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/botocore/endpoint.py:383\u001b[0m, in \u001b[0;36mEndpoint._send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_send\u001b[39m(\u001b[38;5;28mself\u001b[39m, request):\n\u001b[0;32m--> 383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhttp_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/botocore/httpsession.py:464\u001b[0m, in \u001b[0;36mURLLib3Session.send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    461\u001b[0m     conn\u001b[38;5;241m.\u001b[39mproxy_headers[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhost\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m host\n\u001b[1;32m    463\u001b[0m request_target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_request_target(request\u001b[38;5;241m.\u001b[39murl, proxy_url)\n\u001b[0;32m--> 464\u001b[0m urllib_response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_target\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mRetry\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[43m    \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    472\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_chunked\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    476\u001b[0m http_response \u001b[38;5;241m=\u001b[39m botocore\u001b[38;5;241m.\u001b[39mawsrequest\u001b[38;5;241m.\u001b[39mAWSResponse(\n\u001b[1;32m    477\u001b[0m     request\u001b[38;5;241m.\u001b[39murl,\n\u001b[1;32m    478\u001b[0m     urllib_response\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[1;32m    479\u001b[0m     urllib_response\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    480\u001b[0m     urllib_response,\n\u001b[1;32m    481\u001b[0m )\n\u001b[1;32m    483\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m request\u001b[38;5;241m.\u001b[39mstream_output:\n\u001b[1;32m    484\u001b[0m     \u001b[38;5;66;03m# Cause the raw stream to be exhausted immediately. We do it\u001b[39;00m\n\u001b[1;32m    485\u001b[0m     \u001b[38;5;66;03m# this way instead of using preload_content because\u001b[39;00m\n\u001b[1;32m    486\u001b[0m     \u001b[38;5;66;03m# preload_content will never buffer chunked responses\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/urllib3/connectionpool.py:790\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    787\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    789\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 790\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    806\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/urllib3/connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 536\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/urllib3/connection.py:461\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[1;32m    460\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 461\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    464\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/http/client.py:1378\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1376\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1377\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1378\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1379\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1380\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "import boto3\n",
    "s3 = boto3.client('s3', endpoint_url='http://localstack:4566', \n",
    "                  aws_access_key_id='test', aws_secret_access_key='test')\n",
    "s3.create_bucket(Bucket='test-bucket')\n",
    "print(\"Bucket created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b1d9a14-bfb5-4afd-bd8d-b9d8e6fd0446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kafka-python\n",
      "  Obtaining dependency information for kafka-python from https://files.pythonhosted.org/packages/e6/35/e8bfed5425e8fe685bd03ec3f5135ee8b88c11558baa59c0d12fbd2a20ae/kafka_python-2.2.15-py2.py3-none-any.whl.metadata\n",
      "  Downloading kafka_python-2.2.15-py2.py3-none-any.whl.metadata (10.0 kB)\n",
      "Downloading kafka_python-2.2.15-py2.py3-none-any.whl (309 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.8/309.8 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hInstalling collected packages: kafka-python\n",
      "Successfully installed kafka-python-2.2.15\n"
     ]
    }
   ],
   "source": [
    "!pip install kafka-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb6f4b82-00e4-4f5a-9704-4db910db3bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic created!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from kafka.admin import KafkaAdminClient, NewTopic\n",
    "\n",
    "admin = KafkaAdminClient(bootstrap_servers='kafka:29092')\n",
    "topic = NewTopic(name='test-topic', num_partitions=1, replication_factor=1)\n",
    "admin.create_topics([topic])\n",
    "print(\"Topic created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e47686bf-f20a-4b75-814e-4115b52267cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent: {'id': 0, 'name': 'Message 0', 'value': 0}\n",
      "Sent: {'id': 1, 'name': 'Message 1', 'value': 100}\n",
      "Sent: {'id': 2, 'name': 'Message 2', 'value': 200}\n",
      "Sent: {'id': 3, 'name': 'Message 3', 'value': 300}\n",
      "Sent: {'id': 4, 'name': 'Message 4', 'value': 400}\n",
      "Sent: {'id': 5, 'name': 'Message 5', 'value': 500}\n",
      "Sent: {'id': 6, 'name': 'Message 6', 'value': 600}\n",
      "Sent: {'id': 7, 'name': 'Message 7', 'value': 700}\n",
      "Sent: {'id': 8, 'name': 'Message 8', 'value': 800}\n",
      "Sent: {'id': 9, 'name': 'Message 9', 'value': 900}\n",
      "Done sending messages!\n"
     ]
    }
   ],
   "source": [
    "%run /home/jovyan/work/simple_producer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "538079dc-4609-433e-8b3b-c2aabcfb1f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Spark session created\n",
      "📖 Reading from Kafka...\n",
      "\n",
      "📊 Data from Kafka:\n",
      "+---+---------+-----+\n",
      "| id|     name|value|\n",
      "+---+---------+-----+\n",
      "|  0|Message 0|    0|\n",
      "|  1|Message 1|  100|\n",
      "|  2|Message 2|  200|\n",
      "|  3|Message 3|  300|\n",
      "|  4|Message 4|  400|\n",
      "|  5|Message 5|  500|\n",
      "|  6|Message 6|  600|\n",
      "|  7|Message 7|  700|\n",
      "|  8|Message 8|  800|\n",
      "|  9|Message 9|  900|\n",
      "+---+---------+-----+\n",
      "\n",
      "\n",
      "💾 Writing to Delta Lake at: s3a://test-bucket/delta-tables/my_table\n",
      "✅ Data saved to Delta Lake!\n",
      "\n",
      "📖 Reading back from Delta Lake:\n",
      "+---+---------+-----+\n",
      "| id|     name|value|\n",
      "+---+---------+-----+\n",
      "|  0|Message 0|    0|\n",
      "|  1|Message 1|  100|\n",
      "|  2|Message 2|  200|\n",
      "|  3|Message 3|  300|\n",
      "|  4|Message 4|  400|\n",
      "|  5|Message 5|  500|\n",
      "|  6|Message 6|  600|\n",
      "|  7|Message 7|  700|\n",
      "|  8|Message 8|  800|\n",
      "|  9|Message 9|  900|\n",
      "+---+---------+-----+\n",
      "\n",
      "\n",
      "✅ Test completed successfully!\n"
     ]
    }
   ],
   "source": [
    "%run /home/jovyan/work/simple_spark_consumer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587d7973-640b-4f56-8ba0-334748ae3c63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f91ad21-0237-4470-95b1-76e21da7a99c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fc5e868-5c51-445a-b6c7-cec0af444859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in s3://test-bucket/delta-table/:\n",
      " - delta-table/_delta_log/00000000000000000000.json (1248 bytes)\n",
      " - delta-table/part-00000-354a5e3f-e191-438b-bbac-aec33463ff8b-c000.snappy.parquet (1177 bytes)\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "# Configure boto3 for LocalStack\n",
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    endpoint_url='http://localstack:4566',\n",
    "    aws_access_key_id='test',\n",
    "    aws_secret_access_key='test',\n",
    "    region_name='us-east-1'\n",
    ")\n",
    "\n",
    "# List objects in the Delta table path\n",
    "try:\n",
    "    response = s3_client.list_objects_v2(Bucket='test-bucket', Prefix='delta-table/')\n",
    "    if 'Contents' in response:\n",
    "        print(\"Files in s3://test-bucket/delta-table/:\")\n",
    "        for obj in response['Contents']:\n",
    "            print(f\" - {obj['Key']} ({obj['Size']} bytes)\")\n",
    "    else:\n",
    "        print(\"No files found in s3://test-bucket/delta-table/ or bucket does not exist.\")\n",
    "except s3_client.exceptions.NoSuchBucket:\n",
    "    print(\"Bucket 'test-bucket' does not exist. Creating it...\")\n",
    "    s3_client.create_bucket(Bucket='test-bucket')\n",
    "    print(\"Bucket created. Rerun the Spark job to write the Delta table.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3461fa-5cdc-4560-8e78-7e020b78a94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark session with Delta support\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SimpleTest\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.jars.packages\", \n",
    "            \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1,\"\n",
    "            \"io.delta:delta-core_2.12:2.4.0,\"\n",
    "            \"org.apache.hadoop:hadoop-aws:3.3.4,\"\n",
    "            \"com.amazonaws:aws-java-sdk-bundle:1.12.262\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://localstack:4566\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"test\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"test\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .getOrCreate()\n",
    "output_path = \"s3a://test-bucket/delta-table\"\n",
    "print(\"\\n📖 Reading back from Delta Lake:\")\n",
    "delta_df = spark.read.format(\"delta\").load(output_path)\n",
    "delta_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3258614a-3415-4299-8156-089c4961974c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdec18f-eea3-4b91-853d-be3bfd2b6b02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "227f47f6-f8b5-4403-8286-087807389ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Spark session created\n",
      "📖 Reading from Kafka...\n",
      "\n",
      "📊 Data from Kafka:\n",
      "+---+---------+-----+\n",
      "| id|     name|value|\n",
      "+---+---------+-----+\n",
      "|  0|Message 0|    0|\n",
      "|  1|Message 1|  100|\n",
      "|  2|Message 2|  200|\n",
      "|  3|Message 3|  300|\n",
      "|  4|Message 4|  400|\n",
      "|  5|Message 5|  500|\n",
      "|  6|Message 6|  600|\n",
      "|  7|Message 7|  700|\n",
      "|  8|Message 8|  800|\n",
      "|  9|Message 9|  900|\n",
      "+---+---------+-----+\n",
      "\n",
      "\n",
      "💾 Writing to Delta Lake at: s3a://test-bucket/delta-tables/my_table\n",
      "✅ Data saved to Delta Lake!\n",
      "\n",
      "📖 Reading back from Delta Lake:\n",
      "+---+---------+-----+\n",
      "| id|     name|value|\n",
      "+---+---------+-----+\n",
      "|  0|Message 0|    0|\n",
      "|  1|Message 1|  100|\n",
      "|  2|Message 2|  200|\n",
      "|  3|Message 3|  300|\n",
      "|  4|Message 4|  400|\n",
      "|  5|Message 5|  500|\n",
      "|  6|Message 6|  600|\n",
      "|  7|Message 7|  700|\n",
      "|  8|Message 8|  800|\n",
      "|  9|Message 9|  900|\n",
      "+---+---------+-----+\n",
      "\n",
      "\n",
      "✅ Test completed successfully!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Create Spark session with Delta support\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SimpleTest\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.jars.packages\", \n",
    "            \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1,\"\n",
    "            \"io.delta:delta-core_2.12:2.4.0,\"\n",
    "            \"org.apache.hadoop:hadoop-aws:3.3.4,\"\n",
    "            \"com.amazonaws:aws-java-sdk-bundle:1.12.262\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://localstack:4566\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"test\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"test\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"✅ Spark session created\")\n",
    "\n",
    "# Read from Kafka\n",
    "print(\"📖 Reading from Kafka...\")\n",
    "df = spark \\\n",
    "    .read \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \\\n",
    "    .option(\"subscribe\", \"test-topic\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "# Parse JSON and select fields\n",
    "parsed_df = df.select(\n",
    "    col(\"value\").cast(\"string\")\n",
    ").select(\n",
    "    get_json_object(col(\"value\"), \"$.id\").alias(\"id\"),\n",
    "    get_json_object(col(\"value\"), \"$.name\").alias(\"name\"),\n",
    "    get_json_object(col(\"value\"), \"$.value\").alias(\"value\")\n",
    ")\n",
    "\n",
    "# Show data\n",
    "print(\"\\n📊 Data from Kafka:\")\n",
    "parsed_df.show()\n",
    "\n",
    "# Write to Delta Lake on S3\n",
    "#output_path = \"s3a://test-bucket/delta-table\"\n",
    "output_path = \"s3a://test-bucket/delta-tables/my_table\"\n",
    "print(f\"\\n💾 Writing to Delta Lake at: {output_path}\")\n",
    "parsed_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(output_path)\n",
    "\n",
    "print(\"✅ Data saved to Delta Lake!\")\n",
    "\n",
    "# Read back from Delta to verify\n",
    "print(\"\\n📖 Reading back from Delta Lake:\")\n",
    "delta_df = spark.read.format(\"delta\").load(output_path)\n",
    "delta_df.show()\n",
    "\n",
    "print(\"\\n✅ Test completed successfully!\")\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e5da1b-26c2-401d-8aa6-d8e11343e62f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c1fb65-c847-42f8-82a2-873c301aba04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "228c6eac-f4fd-4a63-a868-f99cdef7bc52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Sent: {'symbol': 'GOOGL', 'price': 289.41, 'volume': 877, 'timestamp': '2025-08-04T15:39:32.196996', 'exchange': 'XNAS'}\n",
      "✅ Sent: {'symbol': 'GOOGL', 'price': 298.55, 'volume': 36, 'timestamp': '2025-08-04T15:39:33.355669', 'exchange': 'XNAS'}\n",
      "✅ Sent: {'symbol': 'MSFT', 'price': 275.08, 'volume': 850, 'timestamp': '2025-08-04T15:39:34.356757', 'exchange': 'XNAS'}\n",
      "✅ Sent: {'symbol': 'GOOGL', 'price': 205.54, 'volume': 737, 'timestamp': '2025-08-04T15:39:35.358286', 'exchange': 'XNAS'}\n",
      "✅ Sent: {'symbol': 'TSLA', 'price': 152.65, 'volume': 780, 'timestamp': '2025-08-04T15:39:36.359188', 'exchange': 'XNAS'}\n",
      "✅ Sent: {'symbol': 'TSLA', 'price': 246.13, 'volume': 401, 'timestamp': '2025-08-04T15:39:37.359791', 'exchange': 'XNAS'}\n",
      "✅ Sent: {'symbol': 'MSFT', 'price': 209.88, 'volume': 114, 'timestamp': '2025-08-04T15:39:38.360518', 'exchange': 'XNAS'}\n",
      "✅ Sent: {'symbol': 'AAPL', 'price': 189.29, 'volume': 299, 'timestamp': '2025-08-04T15:39:39.361496', 'exchange': 'XNAS'}\n",
      "✅ Sent: {'symbol': 'AAPL', 'price': 231.23, 'volume': 487, 'timestamp': '2025-08-04T15:39:40.362404', 'exchange': 'XNAS'}\n",
      "✅ Sent: {'symbol': 'TSLA', 'price': 291.49, 'volume': 32, 'timestamp': '2025-08-04T15:39:41.363468', 'exchange': 'XNAS'}\n",
      "✅ Sent: {'symbol': 'GOOGL', 'price': 197.71, 'volume': 962, 'timestamp': '2025-08-04T15:39:42.364053', 'exchange': 'XNAS'}\n",
      "✅ Sent: {'symbol': 'GOOGL', 'price': 265.85, 'volume': 746, 'timestamp': '2025-08-04T15:39:43.364715', 'exchange': 'XNAS'}\n",
      "✅ Sent: {'symbol': 'AAPL', 'price': 291.7, 'volume': 691, 'timestamp': '2025-08-04T15:39:44.365142', 'exchange': 'XNAS'}\n",
      "✅ Sent: {'symbol': 'GOOGL', 'price': 219.37, 'volume': 608, 'timestamp': '2025-08-04T15:39:45.366003', 'exchange': 'XNAS'}\n",
      "✅ Sent: {'symbol': 'MSFT', 'price': 112.68, 'volume': 619, 'timestamp': '2025-08-04T15:39:46.367047', 'exchange': 'XNAS'}\n",
      "✅ Sent: {'symbol': 'AAPL', 'price': 280.35, 'volume': 32, 'timestamp': '2025-08-04T15:39:47.367468', 'exchange': 'XNAS'}\n",
      "✅ Sent: {'symbol': 'TSLA', 'price': 203.66, 'volume': 769, 'timestamp': '2025-08-04T15:39:48.368080', 'exchange': 'XNAS'}\n",
      "✅ Sent: {'symbol': 'GOOGL', 'price': 217.16, 'volume': 553, 'timestamp': '2025-08-04T15:39:49.370209', 'exchange': 'XNAS'}\n",
      "✅ Sent: {'symbol': 'MSFT', 'price': 248.3, 'volume': 812, 'timestamp': '2025-08-04T15:39:50.370690', 'exchange': 'XNAS'}\n",
      "✅ Sent: {'symbol': 'GOOGL', 'price': 167.79, 'volume': 883, 'timestamp': '2025-08-04T15:39:51.371120', 'exchange': 'XNAS'}\n",
      "🚀 Finished sending simulated trades.\n"
     ]
    }
   ],
   "source": [
    "%run /home/jovyan/work/prod.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52012d4f-71c9-4045-adae-f856a982fe24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Spark session created\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'StructField' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/work/stock_consumer.py:65\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Spark session created\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Define schema matching prod.py\u001b[39;00m\n\u001b[1;32m     64\u001b[0m schema \u001b[38;5;241m=\u001b[39m StructType([\n\u001b[0;32m---> 65\u001b[0m     \u001b[43mStructField\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msymbol\u001b[39m\u001b[38;5;124m\"\u001b[39m, StringType()),\n\u001b[1;32m     66\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprice\u001b[39m\u001b[38;5;124m\"\u001b[39m, FloatType()),\n\u001b[1;32m     67\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvolume\u001b[39m\u001b[38;5;124m\"\u001b[39m, IntegerType()),\n\u001b[1;32m     68\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m, StringType()),\n\u001b[1;32m     69\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexchange\u001b[39m\u001b[38;5;124m\"\u001b[39m, StringType())\n\u001b[1;32m     70\u001b[0m ])\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# Read stream from Kafka\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m📖 Reading stream from Kafka...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'StructField' is not defined"
     ]
    }
   ],
   "source": [
    "%run /home/jovyan/work/stock_consumer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "04ff9ddf-f933-47ae-a218-836ef3b750a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Spark session created\n",
      "📖 Reading from Kafka...\n",
      "\n",
      "🔍 Raw Kafka data:\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "|raw_value                                                                                                         |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "|{\"symbol\": \"MSFT\", \"price\": 154.31, \"volume\": 622, \"timestamp\": \"2025-08-04T17:39:06.433532\", \"exchange\": \"XNAS\"} |\n",
      "|{\"symbol\": \"GOOGL\", \"price\": 188.03, \"volume\": 656, \"timestamp\": \"2025-08-04T17:39:07.545608\", \"exchange\": \"XNAS\"}|\n",
      "|{\"symbol\": \"TSLA\", \"price\": 174.87, \"volume\": 519, \"timestamp\": \"2025-08-04T17:39:08.546064\", \"exchange\": \"XNAS\"} |\n",
      "|{\"symbol\": \"AAPL\", \"price\": 166.18, \"volume\": 881, \"timestamp\": \"2025-08-04T17:39:09.546507\", \"exchange\": \"XNAS\"} |\n",
      "|{\"symbol\": \"TSLA\", \"price\": 168.09, \"volume\": 163, \"timestamp\": \"2025-08-04T17:39:10.547317\", \"exchange\": \"XNAS\"} |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "📊 Parsed data:\n",
      "+------+------+------+--------------------+--------+\n",
      "|symbol| price|volume|           timestamp|exchange|\n",
      "+------+------+------+--------------------+--------+\n",
      "|  MSFT|154.31|   622|2025-08-04T17:39:...|    XNAS|\n",
      "| GOOGL|188.03|   656|2025-08-04T17:39:...|    XNAS|\n",
      "|  TSLA|174.87|   519|2025-08-04T17:39:...|    XNAS|\n",
      "|  AAPL|166.18|   881|2025-08-04T17:39:...|    XNAS|\n",
      "|  TSLA|168.09|   163|2025-08-04T17:39:...|    XNAS|\n",
      "|  AAPL|293.89|   883|2025-08-04T17:39:...|    XNAS|\n",
      "|  MSFT|125.53|   881|2025-08-04T17:39:...|    XNAS|\n",
      "| GOOGL|212.36|   853|2025-08-04T17:39:...|    XNAS|\n",
      "|  TSLA|172.61|   370|2025-08-04T17:39:...|    XNAS|\n",
      "|  MSFT| 274.2|   276|2025-08-04T17:39:...|    XNAS|\n",
      "|  TSLA|248.99|   898|2025-08-04T17:39:...|    XNAS|\n",
      "|  MSFT|232.12|   656|2025-08-04T17:39:...|    XNAS|\n",
      "| GOOGL|183.06|   261|2025-08-04T17:39:...|    XNAS|\n",
      "|  TSLA|250.19|   127|2025-08-04T17:39:...|    XNAS|\n",
      "|  TSLA|216.91|   636|2025-08-04T17:39:...|    XNAS|\n",
      "|  AAPL|114.12|   259|2025-08-04T17:39:...|    XNAS|\n",
      "|  AAPL|104.07|   685|2025-08-04T17:39:...|    XNAS|\n",
      "|  MSFT|206.21|   795|2025-08-04T17:39:...|    XNAS|\n",
      "|  AAPL|233.73|   236|2025-08-04T17:39:...|    XNAS|\n",
      "|  TSLA|258.76|   837|2025-08-04T17:39:...|    XNAS|\n",
      "+------+------+------+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "📦 Processing 80 records\n",
      "🌍 Exchanges found: ['XNAS']\n",
      "🌤️ Fetching weather for XNAS (New York)\n",
      "\n",
      "🌟 Enriched data:\n",
      "+------+------+------+--------------------+--------+-----------+--------+---------+\n",
      "|symbol| price|volume|           timestamp|exchange|temperature|humidity|condition|\n",
      "+------+------+------+--------------------+--------+-----------+--------+---------+\n",
      "|  MSFT|154.31|   622|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "| GOOGL|188.03|   656|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "|  TSLA|174.87|   519|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "|  AAPL|166.18|   881|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "|  TSLA|168.09|   163|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "|  AAPL|293.89|   883|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "|  MSFT|125.53|   881|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "| GOOGL|212.36|   853|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "|  TSLA|172.61|   370|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "|  MSFT| 274.2|   276|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "|  TSLA|248.99|   898|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "|  MSFT|232.12|   656|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "| GOOGL|183.06|   261|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "|  TSLA|250.19|   127|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "|  TSLA|216.91|   636|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "|  AAPL|114.12|   259|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "|  AAPL|104.07|   685|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "|  MSFT|206.21|   795|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "|  AAPL|233.73|   236|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "|  TSLA|258.76|   837|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "+------+------+------+--------------------+--------+-----------+--------+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "\n",
      "💾 Writing to Delta Lake...\n",
      "✅ Data written to Delta Lake!\n",
      "\n",
      "📖 Reading back from Delta Lake:\n",
      "+------+------+------+--------------------+--------+-----------+--------+---------+\n",
      "|symbol| price|volume|           timestamp|exchange|temperature|humidity|condition|\n",
      "+------+------+------+--------------------+--------+-----------+--------+---------+\n",
      "|  MSFT|154.31|   622|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "| GOOGL|188.03|   656|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "|  TSLA|174.87|   519|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "|  AAPL|166.18|   881|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "|  TSLA|168.09|   163|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "|  AAPL|293.89|   883|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "|  MSFT|125.53|   881|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "| GOOGL|212.36|   853|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "|  TSLA|172.61|   370|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "|  MSFT| 274.2|   276|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "|  TSLA|248.99|   898|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "|  MSFT|232.12|   656|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "| GOOGL|183.06|   261|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "|  TSLA|250.19|   127|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "|  TSLA|216.91|   636|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "|  AAPL|114.12|   259|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "|  AAPL|104.07|   685|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "|  MSFT|206.21|   795|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "|  AAPL|233.73|   236|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "|  TSLA|258.76|   837|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "+------+------+------+--------------------+--------+-----------+--------+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "✅ Processing completed!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_json, expr\n",
    "from pyspark.sql.types import StructType, StringType, FloatType, IntegerType\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# Exchange → City mapping\n",
    "exchange_city_map = {\n",
    "    \"XNAS\": \"New York\",\n",
    "    \"XNYS\": \"New York\", \n",
    "    \"XLON\": \"London\",\n",
    "    \"XTKS\": \"Tokyo\"\n",
    "}\n",
    "\n",
    "# Cache for weather per city\n",
    "weather_cache = {}\n",
    "CACHE_TTL_SECONDS = 600  # 10 minutes\n",
    "\n",
    "def fetch_weather_cached(city):\n",
    "    now = time.time()\n",
    "    cached = weather_cache.get(city)\n",
    "\n",
    "    if cached and now - cached[\"fetched_at\"] < CACHE_TTL_SECONDS:\n",
    "        return cached[\"data\"]\n",
    "\n",
    "    try:\n",
    "        url = f\"http://api.openweathermap.org/data/2.5/weather?q={city}&appid=34debbcec75c0e6f7351fcda60b141be&units=metric\"\n",
    "        response = requests.get(url, timeout=5)\n",
    "        data = response.json()\n",
    "        weather = {\n",
    "            \"temperature\": data[\"main\"][\"temp\"],\n",
    "            \"humidity\": data[\"main\"][\"humidity\"],\n",
    "            \"condition\": data[\"weather\"][0][\"description\"]\n",
    "        }\n",
    "        # Save to cache\n",
    "        weather_cache[city] = {\n",
    "            \"fetched_at\": now,\n",
    "            \"data\": weather\n",
    "        }\n",
    "        return weather\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Weather fetch failed for {city}: {e}\")\n",
    "        return {\"temperature\": None, \"humidity\": None, \"condition\": None}\n",
    "\n",
    "# Initialize Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"StockWeatherEnrichment\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.jars.packages\", \n",
    "            \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1,\"\n",
    "            \"io.delta:delta-core_2.12:2.4.0,\"\n",
    "            \"org.apache.hadoop:hadoop-aws:3.3.4,\"\n",
    "            \"com.amazonaws:aws-java-sdk-bundle:1.12.262\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://localstack:4566\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"test\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"test\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "print(\"✅ Spark session created\")\n",
    "\n",
    "# CORRECT schema matching your producer data\n",
    "schema = StructType() \\\n",
    "    .add(\"symbol\", StringType()) \\\n",
    "    .add(\"price\", FloatType()) \\\n",
    "    .add(\"volume\", IntegerType()) \\\n",
    "    .add(\"timestamp\", StringType()) \\\n",
    "    .add(\"exchange\", StringType())\n",
    "\n",
    "print(\"📖 Reading from Kafka...\")\n",
    "\n",
    "# Read from Kafka (batch mode for testing)\n",
    "df = spark \\\n",
    "    .read \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \\\n",
    "    .option(\"subscribe\", \"test-topic\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "# First, let's see the raw data\n",
    "print(\"\\n🔍 Raw Kafka data:\")\n",
    "df.selectExpr(\"CAST(value AS STRING) as raw_value\").show(5, truncate=False)\n",
    "\n",
    "# Parse JSON with correct schema\n",
    "parsed_df = df.selectExpr(\"CAST(value AS STRING) as json_str\") \\\n",
    "    .select(from_json(\"json_str\", schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\")\n",
    "\n",
    "print(\"\\n📊 Parsed data:\")\n",
    "parsed_df.show()\n",
    "\n",
    "# Test enrichment function\n",
    "def enrich_with_weather_batch(input_df):\n",
    "    if input_df.count() == 0:\n",
    "        print(\"⚠️ No data to process\")\n",
    "        return input_df\n",
    "    \n",
    "    print(f\"📦 Processing {input_df.count()} records\")\n",
    "    \n",
    "    # Get unique exchanges\n",
    "    exchanges = [row['exchange'] for row in input_df.select(\"exchange\").distinct().collect()]\n",
    "    print(f\"🌍 Exchanges found: {exchanges}\")\n",
    "    \n",
    "    # Fetch weather for each exchange\n",
    "    weather_data = {}\n",
    "    for ex in exchanges:\n",
    "        city = exchange_city_map.get(ex, \"New York\")\n",
    "        print(f\"🌤️ Fetching weather for {ex} ({city})\")\n",
    "        weather_data[ex] = fetch_weather_cached(city)\n",
    "    \n",
    "    # Add weather columns using broadcast\n",
    "    from pyspark.sql.functions import lit, when\n",
    "    \n",
    "    enriched_df = input_df\n",
    "    \n",
    "    # Add weather columns based on exchange\n",
    "    for exchange, weather in weather_data.items():\n",
    "        enriched_df = enriched_df \\\n",
    "            .withColumn(\"temperature\", \n",
    "                when(col(\"exchange\") == exchange, lit(weather.get(\"temperature\")))\n",
    "                .otherwise(col(\"temperature\") if \"temperature\" in enriched_df.columns else lit(None))) \\\n",
    "            .withColumn(\"humidity\",\n",
    "                when(col(\"exchange\") == exchange, lit(weather.get(\"humidity\")))\n",
    "                .otherwise(col(\"humidity\") if \"humidity\" in enriched_df.columns else lit(None))) \\\n",
    "            .withColumn(\"condition\",\n",
    "                when(col(\"exchange\") == exchange, lit(weather.get(\"condition\")))\n",
    "                .otherwise(col(\"condition\") if \"condition\" in enriched_df.columns else lit(None)))\n",
    "    \n",
    "    return enriched_df\n",
    "\n",
    "# Process the data\n",
    "if parsed_df.count() > 0:\n",
    "    enriched_df = enrich_with_weather_batch(parsed_df)\n",
    "    \n",
    "    print(\"\\n🌟 Enriched data:\")\n",
    "    enriched_df.show()\n",
    "    \n",
    "    # Write to Delta Lake\n",
    "    print(\"\\n💾 Writing to Delta Lake...\")\n",
    "    enriched_df.write.format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .save(\"s3a://test-bucket/delta-tables/enriched_trades\")\n",
    "    \n",
    "    print(\"✅ Data written to Delta Lake!\")\n",
    "    \n",
    "    # Read back to verify\n",
    "    print(\"\\n📖 Reading back from Delta Lake:\")\n",
    "    delta_df = spark.read.format(\"delta\").load(\"s3a://test-bucket/delta-tables/enriched_trades\")\n",
    "    delta_df.show()\n",
    "    \n",
    "else:\n",
    "    print(\"⚠️ No data found in Kafka topic\")\n",
    "\n",
    "print(\"✅ Processing completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bf84ea-c0d7-4ea7-896f-d02a0f8ade87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e697d0a9-565c-4684-a734-5b603fe772d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e852a4c0-256d-4137-8b31-cda66a3bbd03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Spark Streaming session created\n",
      "🌊 Starting streaming from Kafka...\n",
      "🌊 Streaming query started! Processing every 5 seconds...\n",
      "💡 Run the producer in another terminal to see real-time processing\n",
      "🌍 Weather data will be fetched for multiple cities based on exchanges\n",
      "🌊 Processing streaming batch 5 with 10 records\n",
      "🌤️ Fetching weather for XNYS (New York)\n",
      "🌤️ Fetching weather for XLON (London)\n",
      "✅ Batch 5 written to streaming storage with date partitioning\n",
      "🌊 Processing streaming batch 6 with 15 records\n",
      "🌤️ Fetching weather for XLON (London)\n",
      "🌤️ Fetching weather for XNAS (New York)\n",
      "🌤️ Fetching weather for XNYS (New York)\n",
      "✅ Batch 6 written to streaming storage with date partitioning\n",
      "🌊 Processing streaming batch 7 with 10 records\n",
      "🌤️ Fetching weather for XLON (London)\n",
      "🌤️ Fetching weather for XNYS (New York)\n",
      "🌤️ Fetching weather for XNAS (New York)\n",
      "✅ Batch 7 written to streaming storage with date partitioning\n",
      "🌊 Processing streaming batch 8 with 5 records\n",
      "🌤️ Fetching weather for XLON (London)\n",
      "🌤️ Fetching weather for XNYS (New York)\n",
      "🌤️ Fetching weather for XNAS (New York)\n",
      "✅ Batch 8 written to streaming storage with date partitioning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🛑 Stopping streaming...\n",
      "✅ Streaming stopped\n"
     ]
    }
   ],
   "source": [
    "# streaming_processor.py\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_json, lit, when, date_format, avg, sum as spark_sum, count\n",
    "from pyspark.sql.types import StructType, StringType, FloatType, IntegerType\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# Weather enrichment\n",
    "exchange_city_map = {\n",
    "    \"XNAS\": \"New York\",\n",
    "    \"XNYS\": \"New York\",\n",
    "    \"XLON\": \"London\",\n",
    "    \"XTKS\": \"Tokyo\"\n",
    "}\n",
    "\n",
    "weather_cache = {}\n",
    "CACHE_TTL_SECONDS = 600\n",
    "\n",
    "def fetch_weather_cached(city):\n",
    "    now = time.time()\n",
    "    cached = weather_cache.get(city)\n",
    "    \n",
    "    if cached and now - cached[\"fetched_at\"] < CACHE_TTL_SECONDS:\n",
    "        return cached[\"data\"]\n",
    "    \n",
    "    try:\n",
    "        url = f\"http://api.openweathermap.org/data/2.5/weather?q={city}&appid=34debbcec75c0e6f7351fcda60b141be&units=metric\"\n",
    "        response = requests.get(url, timeout=5)\n",
    "        data = response.json()\n",
    "        weather = {\n",
    "            \"temperature\": float(data[\"main\"][\"temp\"]),\n",
    "            \"humidity\": int(data[\"main\"][\"humidity\"]),\n",
    "            \"condition\": data[\"weather\"][0][\"description\"]\n",
    "        }\n",
    "        weather_cache[city] = {\"fetched_at\": now, \"data\": weather}\n",
    "        return weather\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Weather fetch failed for {city}: {e}\")\n",
    "        return {\"temperature\": None, \"humidity\": None, \"condition\": \"unknown\"}\n",
    "\n",
    "# Initialize Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"FinTechStreamingPipeline\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.jars.packages\", \n",
    "            \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1,\"\n",
    "            \"io.delta:delta-core_2.12:2.4.0,\"\n",
    "            \"org.apache.hadoop:hadoop-aws:3.3.4,\"\n",
    "            \"com.amazonaws:aws-java-sdk-bundle:1.12.262\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://localstack:4566\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"test\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"test\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "print(\"✅ Spark Streaming session created\")\n",
    "\n",
    "# Schema for incoming data\n",
    "schema = StructType() \\\n",
    "    .add(\"symbol\", StringType()) \\\n",
    "    .add(\"price\", FloatType()) \\\n",
    "    .add(\"volume\", IntegerType()) \\\n",
    "    .add(\"timestamp\", StringType()) \\\n",
    "    .add(\"exchange\", StringType())\n",
    "\n",
    "print(\"🌊 Starting streaming from Kafka...\")\n",
    "\n",
    "# STREAMING: Read from Kafka continuously\n",
    "streaming_df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \\\n",
    "    .option(\"subscribe\", \"test-topic\") \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .option(\"failOnDataLoss\", \"false\") \\\n",
    "    .load()\n",
    "\n",
    "# Parse streaming data\n",
    "parsed_stream = streaming_df.selectExpr(\"CAST(value AS STRING) as json_str\") \\\n",
    "    .select(from_json(\"json_str\", schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\") \\\n",
    "    .withColumn(\"processing_time\", lit(int(time.time())))\n",
    "\n",
    "# Enrichment function for streaming batches\n",
    "def enrich_and_store_stream(batch_df, batch_id):\n",
    "    if batch_df.count() == 0:\n",
    "        return\n",
    "    \n",
    "    print(f\"🌊 Processing streaming batch {batch_id} with {batch_df.count()} records\")\n",
    "    \n",
    "    # Get weather data for unique exchanges\n",
    "    exchanges = [row['exchange'] for row in batch_df.select(\"exchange\").distinct().collect()]\n",
    "    weather_data = {}\n",
    "    \n",
    "    for ex in exchanges:\n",
    "        city = exchange_city_map.get(ex, \"New York\")\n",
    "        print(f\"🌤️ Fetching weather for {ex} ({city})\")\n",
    "        weather_data[ex] = fetch_weather_cached(city)\n",
    "    \n",
    "    # ✅ SIMPLIFIED: Only add weather columns (no city)\n",
    "    enriched_df = batch_df\n",
    "    for col_name in [\"temperature\", \"humidity\", \"condition\"]:\n",
    "        enriched_df = enriched_df.withColumn(col_name, lit(None))\n",
    "    \n",
    "    for exchange, weather in weather_data.items():\n",
    "        enriched_df = enriched_df \\\n",
    "            .withColumn(\"temperature\", when(col(\"exchange\") == exchange, lit(weather.get(\"temperature\"))).otherwise(col(\"temperature\"))) \\\n",
    "            .withColumn(\"humidity\", when(col(\"exchange\") == exchange, lit(weather.get(\"humidity\"))).otherwise(col(\"humidity\"))) \\\n",
    "            .withColumn(\"condition\", when(col(\"exchange\") == exchange, lit(weather.get(\"condition\"))).otherwise(col(\"condition\")))\n",
    "    \n",
    "    # ✅ ADD DATE PARTITIONING COLUMN\n",
    "    enriched_df = enriched_df \\\n",
    "        .withColumn(\"date\", date_format(col(\"timestamp\"), \"yyyy-MM-dd\"))\n",
    "    \n",
    "    # ✅ WRITE WITH DATE PARTITIONING ONLY\n",
    "    enriched_df.write.format(\"delta\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .partitionBy(\"date\") \\\n",
    "        .save(\"s3a://test-bucket/delta-tables/streaming_trades\")\n",
    "    \n",
    "    print(f\"✅ Batch {batch_id} written to streaming storage with date partitioning\")\n",
    "\n",
    "# Start the streaming query\n",
    "streaming_query = parsed_stream.writeStream \\\n",
    "    .foreachBatch(enrich_and_store_stream) \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .trigger(processingTime='5 seconds') \\\n",
    "    .option(\"checkpointLocation\", \"s3a://test-bucket/checkpoints/streaming\") \\\n",
    "    .start()\n",
    "\n",
    "print(\"🌊 Streaming query started! Processing every 5 seconds...\")\n",
    "print(\"💡 Run the producer in another terminal to see real-time processing\")\n",
    "print(\"🌍 Weather data will be fetched for multiple cities based on exchanges\")\n",
    "\n",
    "# Keep streaming running\n",
    "try:\n",
    "    streaming_query.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n🛑 Stopping streaming...\")\n",
    "    streaming_query.stop()\n",
    "    print(\"✅ Streaming stopped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b66cb7a-bad9-426b-a906-bb63d913987e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📖 Reading back from Delta Lake:\n",
      "+------+------+------+--------------------+--------+---------------+-----------+--------+---------------+----------+\n",
      "|symbol| price|volume|           timestamp|exchange|processing_time|temperature|humidity|      condition|      date|\n",
      "+------+------+------+--------------------+--------+---------------+-----------+--------+---------------+----------+\n",
      "|   JPM|106.87|   777|2025-08-06T20:27:...|    XNYS|     1754512047|      23.99|      63|          smoke|2025-08-06|\n",
      "|  TSLA|285.96|   684|2025-08-06T20:27:...|    XNAS|     1754512047|      23.99|      63|          smoke|2025-08-06|\n",
      "|  AAPL| 213.2|   125|2025-08-06T20:27:...|    XNAS|     1754512047|      23.99|      63|          smoke|2025-08-06|\n",
      "|   XOM|132.37|   354|2025-08-06T20:27:...|    XNYS|     1754512047|      23.99|      63|          smoke|2025-08-06|\n",
      "|  TSLA|205.22|   795|2025-08-06T20:27:...|    XNAS|     1754512047|      23.99|      63|          smoke|2025-08-06|\n",
      "|  TSLA|124.18|   654|2025-08-06T20:27:...|    XNAS|     1754512047|      23.99|      63|          smoke|2025-08-06|\n",
      "|   BAC|257.97|   377|2025-08-06T20:27:...|    XNYS|     1754512047|      23.99|      63|          smoke|2025-08-06|\n",
      "|  HSBA| 274.6|   608|2025-08-06T20:27:...|    XLON|     1754512047|      19.19|      60|overcast clouds|2025-08-06|\n",
      "|   AZN|175.29|   538|2025-08-06T20:27:...|    XLON|     1754512047|      19.19|      60|overcast clouds|2025-08-06|\n",
      "| GOOGL|121.26|   820|2025-08-06T20:27:...|    XNAS|     1754512047|      23.99|      63|          smoke|2025-08-06|\n",
      "|  MSFT|294.53|    98|2025-08-06T20:27:...|    XNAS|     1754512047|      23.99|      63|          smoke|2025-08-06|\n",
      "|  MSFT|176.57|   194|2025-08-06T20:27:...|    XNAS|     1754512047|      23.99|      63|          smoke|2025-08-06|\n",
      "|    BP|140.86|   281|2025-08-06T20:27:...|    XLON|     1754512047|      19.19|      60|overcast clouds|2025-08-06|\n",
      "|  AAPL|293.63|   539|2025-08-06T20:27:...|    XNAS|     1754512047|      23.99|      63|          smoke|2025-08-06|\n",
      "| GOOGL|155.45|   552|2025-08-06T20:27:...|    XNAS|     1754512047|      23.99|      63|          smoke|2025-08-06|\n",
      "| GOOGL|151.66|   966|2025-08-06T20:27:...|    XNAS|     1754512047|      23.99|      63|          smoke|2025-08-06|\n",
      "| GOOGL|127.95|   880|2025-08-06T20:27:...|    XNAS|     1754512047|      23.99|      63|          smoke|2025-08-06|\n",
      "|  VODL|146.76|    51|2025-08-06T20:27:...|    XLON|     1754512047|      19.19|      60|overcast clouds|2025-08-06|\n",
      "|  VODL|108.03|   114|2025-08-06T20:27:...|    XLON|     1754512047|      19.19|      60|overcast clouds|2025-08-06|\n",
      "+------+------+------+--------------------+--------+---------------+-----------+--------+---------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "    print(\"\\n📖 Reading back from Delta Lake:\")\n",
    "    delta_df = spark.read.format(\"delta\").load(\"s3a://test-bucket/delta-tables/streaming_trades\")\n",
    "    delta_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8689b720-5374-47df-a8c9-8db3510f0c6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Spark Batch session created\n",
      "📊 Starting batch processing...\n",
      "📈 Processing 19 records from streaming storage\n",
      "📊 Hourly aggregations:\n",
      "+------+--------+-------------+----------+---------------+------------------+---------+---------+------------+-----------+---------------+------------+--------------------+\n",
      "|symbol|exchange|         hour|      date|      condition|         avg_price|min_price|max_price|total_volume|trade_count|avg_temperature|avg_humidity|  batch_processed_at|\n",
      "+------+--------+-------------+----------+---------------+------------------+---------+---------+------------+-----------+---------------+------------+--------------------+\n",
      "|  HSBA|    XLON|2025-08-06 20|2025-08-06|overcast clouds| 274.6000061035156|    274.6|    274.6|         608|          1|          19.19|        60.0|2025-08-06T20:29:...|\n",
      "|   JPM|    XNYS|2025-08-06 20|2025-08-06|          smoke|106.87000274658203|   106.87|   106.87|         777|          1|          23.99|        63.0|2025-08-06T20:29:...|\n",
      "| GOOGL|    XNAS|2025-08-06 20|2025-08-06|          smoke|139.07999992370605|   121.26|   155.45|        3218|          4|          23.99|        63.0|2025-08-06T20:29:...|\n",
      "|   BAC|    XNYS|2025-08-06 20|2025-08-06|          smoke| 257.9700012207031|   257.97|   257.97|         377|          1|          23.99|        63.0|2025-08-06T20:29:...|\n",
      "|   AZN|    XLON|2025-08-06 20|2025-08-06|overcast clouds| 175.2899932861328|   175.29|   175.29|         538|          1|          19.19|        60.0|2025-08-06T20:29:...|\n",
      "|  MSFT|    XNAS|2025-08-06 20|2025-08-06|          smoke| 235.5500030517578|   176.57|   294.53|         292|          2|          23.99|        63.0|2025-08-06T20:29:...|\n",
      "|  VODL|    XLON|2025-08-06 20|2025-08-06|overcast clouds| 127.3949966430664|   108.03|   146.76|         165|          2|          19.19|        60.0|2025-08-06T20:29:...|\n",
      "|  AAPL|    XNAS|2025-08-06 20|2025-08-06|          smoke|253.41500091552734|    213.2|   293.63|         664|          2|          23.99|        63.0|2025-08-06T20:29:...|\n",
      "|    BP|    XLON|2025-08-06 20|2025-08-06|overcast clouds|140.86000061035156|   140.86|   140.86|         281|          1|          19.19|        60.0|2025-08-06T20:29:...|\n",
      "|  TSLA|    XNAS|2025-08-06 20|2025-08-06|          smoke|  205.119997660319|   124.18|   285.96|        2133|          3|          23.99|        63.0|2025-08-06T20:29:...|\n",
      "+------+--------+-------------+----------+---------------+------------------+---------+---------+------------+-----------+---------------+------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "📅 Daily aggregations:\n",
      "+------+--------+----------+------------------+---------+---------+------------+-----------+---------------+------------+--------------------+\n",
      "|symbol|exchange|      date|         avg_price|min_price|max_price|total_volume|trade_count|avg_temperature|avg_humidity|  batch_processed_at|\n",
      "+------+--------+----------+------------------+---------+---------+------------+-----------+---------------+------------+--------------------+\n",
      "|   AZN|    XLON|2025-08-06| 175.2899932861328|   175.29|   175.29|         538|          1|          19.19|        60.0|2025-08-06T20:29:...|\n",
      "|  HSBA|    XLON|2025-08-06| 274.6000061035156|    274.6|    274.6|         608|          1|          19.19|        60.0|2025-08-06T20:29:...|\n",
      "|  TSLA|    XNAS|2025-08-06|  205.119997660319|   124.18|   285.96|        2133|          3|          23.99|        63.0|2025-08-06T20:29:...|\n",
      "|   BAC|    XNYS|2025-08-06| 257.9700012207031|   257.97|   257.97|         377|          1|          23.99|        63.0|2025-08-06T20:29:...|\n",
      "|  AAPL|    XNAS|2025-08-06|253.41500091552734|    213.2|   293.63|         664|          2|          23.99|        63.0|2025-08-06T20:29:...|\n",
      "|  MSFT|    XNAS|2025-08-06| 235.5500030517578|   176.57|   294.53|         292|          2|          23.99|        63.0|2025-08-06T20:29:...|\n",
      "|   XOM|    XNYS|2025-08-06| 132.3699951171875|   132.37|   132.37|         354|          1|          23.99|        63.0|2025-08-06T20:29:...|\n",
      "|  VODL|    XLON|2025-08-06| 127.3949966430664|   108.03|   146.76|         165|          2|          19.19|        60.0|2025-08-06T20:29:...|\n",
      "|   JPM|    XNYS|2025-08-06|106.87000274658203|   106.87|   106.87|         777|          1|          23.99|        63.0|2025-08-06T20:29:...|\n",
      "|    BP|    XLON|2025-08-06|140.86000061035156|   140.86|   140.86|         281|          1|          19.19|        60.0|2025-08-06T20:29:...|\n",
      "+------+--------+----------+------------------+---------+---------+------------+-----------+---------------+------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "✅ Batch processing completed with date partitioning!\n"
     ]
    }
   ],
   "source": [
    "# batch_processor.py\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, date_format, avg, sum as spark_sum, count, max as spark_max, min as spark_min, lit\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Initialize Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"FinTechBatchProcessor\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.jars.packages\", \n",
    "            \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1,\"\n",
    "            \"io.delta:delta-core_2.12:2.4.0,\"\n",
    "            \"org.apache.hadoop:hadoop-aws:3.3.4,\"\n",
    "            \"com.amazonaws:aws-java-sdk-bundle:1.12.262\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://localstack:4566\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"test\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"test\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "print(\"✅ Spark Batch session created\")\n",
    "\n",
    "def run_batch_aggregations(target_date=None):\n",
    "    \"\"\"\n",
    "    Run batch aggregations for a specific date.\n",
    "    If no date specified, process all available data.\n",
    "    \"\"\"\n",
    "    print(\"📊 Starting batch processing...\")\n",
    "    \n",
    "    # Read streaming data\n",
    "    streaming_data = spark.read.format(\"delta\") \\\n",
    "        .load(\"s3a://test-bucket/delta-tables/streaming_trades\")\n",
    "    \n",
    "    # Filter by specific date if provided\n",
    "    if target_date:\n",
    "        streaming_data = streaming_data.filter(col(\"date\") == target_date)\n",
    "        print(f\"📅 Processing data for date: {target_date}\")\n",
    "    \n",
    "    if streaming_data.count() == 0:\n",
    "        print(\"⚠️ No streaming data found for batch processing\")\n",
    "        return\n",
    "    \n",
    "    print(f\"📈 Processing {streaming_data.count()} records from streaming storage\")\n",
    "    \n",
    "    # ✅ SIMPLIFIED HOURLY AGGREGATIONS (no city column)\n",
    "    hourly_summary = streaming_data \\\n",
    "        .withColumn(\"hour\", date_format(col(\"timestamp\"), \"yyyy-MM-dd HH\")) \\\n",
    "        .groupBy(\"symbol\", \"exchange\", \"hour\", \"date\", \"condition\") \\\n",
    "        .agg(\n",
    "            avg(\"price\").alias(\"avg_price\"),\n",
    "            spark_min(\"price\").alias(\"min_price\"),\n",
    "            spark_max(\"price\").alias(\"max_price\"),\n",
    "            spark_sum(\"volume\").alias(\"total_volume\"),\n",
    "            count(\"*\").alias(\"trade_count\"),\n",
    "            avg(\"temperature\").alias(\"avg_temperature\"),\n",
    "            avg(\"humidity\").alias(\"avg_humidity\")\n",
    "        ) \\\n",
    "        .withColumn(\"batch_processed_at\", lit(datetime.now().isoformat()))\n",
    "    \n",
    "    print(\"📊 Hourly aggregations:\")\n",
    "    hourly_summary.show(10)\n",
    "    \n",
    "    # ✅ WRITE WITH DATE PARTITIONING AND PROPER MODE\n",
    "    if target_date:\n",
    "        # Replace only specific date\n",
    "        hourly_summary.write.format(\"delta\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .option(\"replaceWhere\", f\"date = '{target_date}'\") \\\n",
    "            .save(\"s3a://test-bucket/delta-tables/hourly_summaries\")\n",
    "    else:\n",
    "        # Replace all data\n",
    "        hourly_summary.write.format(\"delta\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .partitionBy(\"date\") \\\n",
    "            .save(\"s3a://test-bucket/delta-tables/hourly_summaries\")\n",
    "    \n",
    "    # ✅ SIMPLIFIED DAILY AGGREGATIONS (no city column)\n",
    "    daily_summary = streaming_data \\\n",
    "        .groupBy(\"symbol\", \"exchange\", \"date\") \\\n",
    "        .agg(\n",
    "            avg(\"price\").alias(\"avg_price\"),\n",
    "            spark_min(\"price\").alias(\"min_price\"),\n",
    "            spark_max(\"price\").alias(\"max_price\"),\n",
    "            spark_sum(\"volume\").alias(\"total_volume\"),\n",
    "            count(\"*\").alias(\"trade_count\"),\n",
    "            avg(\"temperature\").alias(\"avg_temperature\"),\n",
    "            avg(\"humidity\").alias(\"avg_humidity\")\n",
    "        ) \\\n",
    "        .withColumn(\"batch_processed_at\", lit(datetime.now().isoformat()))\n",
    "    \n",
    "    print(\"📅 Daily aggregations:\")\n",
    "    daily_summary.show(10)\n",
    "    \n",
    "    # ✅ WRITE WITH DATE PARTITIONING\n",
    "    if target_date:\n",
    "        # Replace only specific date\n",
    "        daily_summary.write.format(\"delta\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .option(\"replaceWhere\", f\"date = '{target_date}'\") \\\n",
    "            .save(\"s3a://test-bucket/delta-tables/daily_summaries\")\n",
    "    else:\n",
    "        # Replace all data\n",
    "        daily_summary.write.format(\"delta\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .partitionBy(\"date\") \\\n",
    "            .save(\"s3a://test-bucket/delta-tables/daily_summaries\")\n",
    "    \n",
    "    print(\"✅ Batch processing completed with date partitioning!\")\n",
    "\n",
    "def run_incremental_batch():\n",
    "    \"\"\"\n",
    "    Process only yesterday's data (typical production scenario)\n",
    "    \"\"\"\n",
    "    yesterday = (datetime.now() - timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "    print(f\"📅 Running incremental batch for {yesterday}\")\n",
    "    run_batch_aggregations(target_date=yesterday)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run for all data\n",
    "    run_batch_aggregations()\n",
    "    \n",
    "    # Or run incremental (uncomment this line)\n",
    "    # run_incremental_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bf825d-be9f-475f-a583-10bbc79c7771",
   "metadata": {},
   "source": [
    "separated hourly and daily "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "499fe2d8-44a9-453f-a8ff-80c19b42d74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scripts/daily_processor.py\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, date_format, avg, sum as spark_sum, count, max as spark_max, min as spark_min, lit\n",
    "from datetime import datetime, timedelta\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def create_spark_session():\n",
    "    return SparkSession.builder \\\n",
    "        .appName(\"FinTechDailyProcessor\") \\\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "        .config(\"spark.jars.packages\", \n",
    "                \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1,\"\n",
    "                \"io.delta:delta-core_2.12:2.4.0,\"\n",
    "                \"org.apache.hadoop:hadoop-aws:3.3.4,\"\n",
    "                \"com.amazonaws:aws-java-sdk-bundle:1.12.262\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://localstack:4566\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", \"test\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", \"test\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "def run_daily_processing():\n",
    "    \"\"\"\n",
    "    Process yesterday's complete day for daily summaries.\n",
    "    \"\"\"\n",
    "    spark = create_spark_session()\n",
    "    spark.sparkContext.setLogLevel(\"WARN\")\n",
    "    \n",
    "    try:\n",
    "        # Calculate yesterday\n",
    "        yesterday = (datetime.now() - timedelta(days=0)).strftime(\"%Y-%m-%d\")\n",
    "        \n",
    "        logger.info(f\"📅 Processing daily data for: {yesterday}\")\n",
    "        \n",
    "        # Read all streaming data for yesterday\n",
    "        streaming_data = spark.read.format(\"delta\") \\\n",
    "            .load(\"s3a://test-bucket/delta-tables/streaming_trades\") \\\n",
    "            .filter(col(\"date\") == yesterday)\n",
    "        \n",
    "        record_count = streaming_data.count()\n",
    "        \n",
    "        if record_count == 0:\n",
    "            logger.warning(f\"⚠️ No streaming data found for date: {yesterday}\")\n",
    "            return\n",
    "        \n",
    "        logger.info(f\"📈 Processing {record_count} records for date: {yesterday}\")\n",
    "        \n",
    "        # Create daily aggregation\n",
    "        daily_summary = streaming_data \\\n",
    "            .groupBy(\"symbol\", \"exchange\", \"date\") \\\n",
    "            .agg(\n",
    "                avg(\"price\").alias(\"avg_price\"),\n",
    "                spark_min(\"price\").alias(\"min_price\"),\n",
    "                spark_max(\"price\").alias(\"max_price\"),\n",
    "                spark_sum(\"volume\").alias(\"total_volume\"),\n",
    "                count(\"*\").alias(\"trade_count\"),\n",
    "                avg(\"temperature\").alias(\"avg_temperature\"),\n",
    "                avg(\"humidity\").alias(\"avg_humidity\")\n",
    "            ) \\\n",
    "            .withColumn(\"batch_processed_at\", lit(datetime.now().isoformat()))\n",
    "        \n",
    "        daily_count = daily_summary.count()\n",
    "        logger.info(f\"📅 Created {daily_count} daily summary records\")\n",
    "        \n",
    "        if daily_count > 0:\n",
    "            # Replace only yesterday's daily summary\n",
    "            daily_summary.write.format(\"delta\") \\\n",
    "                .mode(\"overwrite\") \\\n",
    "                .option(\"replaceWhere\", f\"date = '{yesterday}'\") \\\n",
    "                .save(\"s3a://test-bucket/delta-tables/daily_summaries\")\n",
    "            \n",
    "            logger.info(f\"✅ Daily summary updated for {yesterday}\")\n",
    "        \n",
    "        logger.info(f\"🎉 Daily processing completed for {yesterday}!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Daily processing failed: {str(e)}\")\n",
    "        raise\n",
    "    finally:\n",
    "        spark.stop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_daily_processing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f5427c-2fa6-4092-837e-d3b410ede6c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6ce7a52-e066-45d7-aabf-e966fe3b8385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scripts/hourly_processor.py\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, date_format, avg, sum as spark_sum, count, max as spark_max, min as spark_min, lit\n",
    "from datetime import datetime, timedelta\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def create_spark_session():\n",
    "    return SparkSession.builder \\\n",
    "        .appName(\"FinTechHourlyProcessor\") \\\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "        .config(\"spark.jars.packages\", \n",
    "                \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1,\"\n",
    "                \"io.delta:delta-core_2.12:2.4.0,\"\n",
    "                \"org.apache.hadoop:hadoop-aws:3.3.4,\"\n",
    "                \"com.amazonaws:aws-java-sdk-bundle:1.12.262\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://localstack:4566\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", \"test\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", \"test\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "def run_hourly_processing():\n",
    "    \"\"\"\n",
    "    Process the previous completed hour.\n",
    "    E.g., if it's 10:05, process 09:00-09:59\n",
    "    \"\"\"\n",
    "    spark = create_spark_session()\n",
    "    spark.sparkContext.setLogLevel(\"WARN\")\n",
    "    \n",
    "    try:\n",
    "        # Calculate previous hour\n",
    "        current_time = datetime.now()\n",
    "        previous_hour = current_time.replace(minute=0, second=0, microsecond=0) #- timedelta(hours=1)\n",
    "        target_hour = previous_hour.strftime(\"%Y-%m-%d %H\")\n",
    "        target_date = previous_hour.strftime(\"%Y-%m-%d\")\n",
    "        \n",
    "        logger.info(f\"🕐 Processing hourly data for: {target_hour}\")\n",
    "        \n",
    "        # Read streaming data for the specific hour\n",
    "        streaming_data = spark.read.format(\"delta\") \\\n",
    "            .load(\"s3a://test-bucket/delta-tables/streaming_trades\") \\\n",
    "            .filter(col(\"date\") == target_date) \\\n",
    "            .filter(date_format(col(\"timestamp\"), \"yyyy-MM-dd HH\") == target_hour)\n",
    "        \n",
    "        record_count = streaming_data.count()\n",
    "        \n",
    "        if record_count == 0:\n",
    "            logger.warning(f\"⚠️ No streaming data found for hour: {target_hour}\")\n",
    "            return\n",
    "        \n",
    "        logger.info(f\"📈 Processing {record_count} records for hour: {target_hour}\")\n",
    "        \n",
    "        # Create hourly aggregation\n",
    "        hourly_summary = streaming_data \\\n",
    "            .withColumn(\"hour\", lit(target_hour)) \\\n",
    "            .groupBy(\"symbol\", \"exchange\", \"hour\", \"date\", \"condition\") \\\n",
    "            .agg(\n",
    "                avg(\"price\").alias(\"avg_price\"),\n",
    "                spark_min(\"price\").alias(\"min_price\"),\n",
    "                spark_max(\"price\").alias(\"max_price\"),\n",
    "                spark_sum(\"volume\").alias(\"total_volume\"),\n",
    "                count(\"*\").alias(\"trade_count\"),\n",
    "                avg(\"temperature\").alias(\"avg_temperature\"),\n",
    "                avg(\"humidity\").alias(\"avg_humidity\")\n",
    "            ) \\\n",
    "            .withColumn(\"batch_processed_at\", lit(datetime.now().isoformat()))\n",
    "        \n",
    "        hourly_count = hourly_summary.count()\n",
    "        logger.info(f\"📊 Created {hourly_count} hourly summary records\")\n",
    "        \n",
    "        if hourly_count > 0:\n",
    "            # Use merge/upsert to replace only this specific hour\n",
    "            hourly_summary.write.format(\"delta\") \\\n",
    "                .mode(\"overwrite\") \\\n",
    "                .option(\"replaceWhere\", f\"hour = '{target_hour}'\") \\\n",
    "                .save(\"s3a://test-bucket/delta-tables/hourly_summaries\")\n",
    "            \n",
    "            logger.info(f\"✅ Hourly summary updated for {target_hour}\")\n",
    "        \n",
    "        logger.info(f\"🎉 Hourly processing completed for {target_hour}!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Hourly processing failed: {str(e)}\")\n",
    "        raise\n",
    "    finally:\n",
    "        spark.stop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_hourly_processing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "436f36e6-2aec-400d-8293-ba3d3ac683a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📖 Reading back from Delta Lake:\n",
      "+------+--------+-------------+----------+---------------+------------------+---------+---------+------------+-----------+------------------+------------+--------------------+\n",
      "|symbol|exchange|         hour|      date|      condition|         avg_price|min_price|max_price|total_volume|trade_count|   avg_temperature|avg_humidity|  batch_processed_at|\n",
      "+------+--------+-------------+----------+---------------+------------------+---------+---------+------------+-----------+------------------+------------+--------------------+\n",
      "|   BAC|    XNYS|2025-08-16 15|2025-08-16|  broken clouds|224.82499313354492|   166.53|   288.61|        1404|          4|             26.89|        68.0|2025-08-16T15:24:...|\n",
      "|  MSFT|    XNAS|2025-08-16 15|2025-08-16|  broken clouds|196.22000122070312|   156.67|   225.38|        1067|          3|             26.89|        68.0|2025-08-16T15:24:...|\n",
      "|    BP|    XLON|2025-08-16 15|2025-08-16|overcast clouds| 278.0850067138672|   273.64|   282.53|         955|          2|             20.13|        64.0|2025-08-16T15:24:...|\n",
      "|  TSLA|    XNAS|2025-08-16 15|2025-08-16|  broken clouds|159.69571467808314|    119.5|   242.65|        4295|          7|26.889999999999993|        68.0|2025-08-16T15:24:...|\n",
      "|  AAPL|    XNAS|2025-08-16 15|2025-08-16|  broken clouds| 182.3550033569336|   120.97|   240.82|        2689|          4|             26.89|        68.0|2025-08-16T15:24:...|\n",
      "|   AZN|    XLON|2025-08-16 15|2025-08-16|overcast clouds|135.07000350952148|    108.4|   161.74|         394|          2|             20.13|        64.0|2025-08-16T15:24:...|\n",
      "| GOOGL|    XNAS|2025-08-16 15|2025-08-16|  broken clouds|231.49667358398438|   144.62|   284.98|        2695|          3|             26.89|        68.0|2025-08-16T15:24:...|\n",
      "|    GE|    XNYS|2025-08-16 15|2025-08-16|  broken clouds|138.29666646321616|   102.67|   162.91|        2279|          3|             26.89|        68.0|2025-08-16T15:24:...|\n",
      "|   XOM|    XNYS|2025-08-16 15|2025-08-16|  broken clouds|237.50799560546875|   142.93|   288.25|        3510|          5|26.889999999999997|        68.0|2025-08-16T15:24:...|\n",
      "|  HSBA|    XLON|2025-08-16 15|2025-08-16|overcast clouds|203.24250411987305|   131.64|   285.26|        2850|          4|             20.13|        64.0|2025-08-16T15:24:...|\n",
      "|   JPM|    XNYS|2025-08-16 15|2025-08-16|  broken clouds| 190.0449981689453|    124.3|   255.79|         947|          2|             26.89|        68.0|2025-08-16T15:24:...|\n",
      "|  VODL|    XLON|2025-08-16 15|2025-08-16|overcast clouds|100.97000122070312|   100.97|   100.97|         287|          1|             20.13|        64.0|2025-08-16T15:24:...|\n",
      "+------+--------+-------------+----------+---------------+------------------+---------+---------+------------+-----------+------------------+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_json, lit, when, date_format, avg, sum as spark_sum, count\n",
    "from pyspark.sql.types import StructType, StringType, FloatType, IntegerType\n",
    "import requests\n",
    "import time\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"FinTechStreamingPipeline\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.jars.packages\", \n",
    "            \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1,\"\n",
    "            \"io.delta:delta-core_2.12:2.4.0,\"\n",
    "            \"org.apache.hadoop:hadoop-aws:3.3.4,\"\n",
    "            \"com.amazonaws:aws-java-sdk-bundle:1.12.262\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://localstack:4566\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"test\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"test\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .getOrCreate()\n",
    "print(\"\\n📖 Reading back from Delta Lake:\")\n",
    "delta_df = spark.read.format(\"delta\").load(\"s3a://test-bucket/delta-tables/hourly_summaries\")\n",
    "delta_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "233eb8eb-9e2b-4c78-b684-6189bd65ce94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Spark Streaming session created\n",
      "🌊 Starting streaming from Kafka...\n",
      "🌊 Streaming query started! Processing every 5 seconds...\n",
      "💡 Run the producer in another terminal to see real-time processing\n",
      "🌍 Weather data will be fetched for multiple cities based on exchanges\n",
      "🌊 Processing streaming batch 2 with 3 records\n",
      "🌤️ Fetching weather for XLON (London)\n",
      "🌤️ Fetching weather for XNYS (New York)\n",
      "🌤️ Fetching weather for XNAS (New York)\n",
      "✅ Batch 2 written to streaming storage with date partitioning\n",
      "🌊 Processing streaming batch 3 with 14 records\n",
      "🌤️ Fetching weather for XLON (London)\n",
      "🌤️ Fetching weather for XNYS (New York)\n",
      "🌤️ Fetching weather for XNAS (New York)\n",
      "✅ Batch 3 written to streaming storage with date partitioning\n",
      "🌊 Processing streaming batch 4 with 10 records\n",
      "🌤️ Fetching weather for XNAS (New York)\n",
      "🌤️ Fetching weather for XLON (London)\n",
      "🌤️ Fetching weather for XNYS (New York)\n",
      "✅ Batch 4 written to streaming storage with date partitioning\n",
      "🌊 Processing streaming batch 5 with 10 records\n",
      "🌤️ Fetching weather for XNAS (New York)\n",
      "🌤️ Fetching weather for XLON (London)\n",
      "🌤️ Fetching weather for XNYS (New York)\n",
      "✅ Batch 5 written to streaming storage with date partitioning\n",
      "🌊 Processing streaming batch 6 with 3 records\n",
      "🌤️ Fetching weather for XNAS (New York)\n",
      "🌤️ Fetching weather for XNYS (New York)\n",
      "✅ Batch 6 written to streaming storage with date partitioning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🛑 Stopping streaming...\n",
      "✅ Streaming stopped\n"
     ]
    }
   ],
   "source": [
    "# streaming_processor.py\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_json, lit, when, date_format, to_timestamp\n",
    "from pyspark.sql.types import StructType, StringType, FloatType, IntegerType, TimestampType\n",
    "import requests, time\n",
    "\n",
    "# Weather enrichment (unchanged)\n",
    "exchange_city_map = {\n",
    "    \"XNAS\": \"New York\",\n",
    "    \"XNYS\": \"New York\",\n",
    "    \"XLON\": \"London\",\n",
    "    \"XTKS\": \"Tokyo\"\n",
    "}\n",
    "\n",
    "weather_cache = {}\n",
    "CACHE_TTL_SECONDS = 600\n",
    "\n",
    "def fetch_weather_cached(city):\n",
    "    now = time.time()\n",
    "    cached = weather_cache.get(city)\n",
    "    \n",
    "    if cached and now - cached[\"fetched_at\"] < CACHE_TTL_SECONDS:\n",
    "        return cached[\"data\"]\n",
    "    \n",
    "    try:\n",
    "        url = f\"http://api.openweathermap.org/data/2.5/weather?q={city}&appid=34debbcec75c0e6f7351fcda60b141be&units=metric\"\n",
    "        response = requests.get(url, timeout=5)\n",
    "        data = response.json()\n",
    "        weather = {\n",
    "            \"temperature\": float(data[\"main\"][\"temp\"]),\n",
    "            \"humidity\": int(data[\"main\"][\"humidity\"]),\n",
    "            \"condition\": data[\"weather\"][0][\"description\"]\n",
    "        }\n",
    "        weather_cache[city] = {\"fetched_at\": now, \"data\": weather}\n",
    "        return weather\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Weather fetch failed for {city}: {e}\")\n",
    "        return {\"temperature\": None, \"humidity\": None, \"condition\": \"unknown\"}\n",
    "\n",
    "# Initialize Spark (unchanged)\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"FinTechStreamingPipeline\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.jars.packages\", \n",
    "            \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1,\"\n",
    "            \"io.delta:delta-core_2.12:2.4.0,\"\n",
    "            \"org.apache.hadoop:hadoop-aws:3.3.4,\"\n",
    "            \"com.amazonaws:aws-java-sdk-bundle:1.12.262\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://localstack:4566\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"test\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"test\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "print(\"✅ Spark Streaming session created\")\n",
    "\n",
    "# Corrected schema with TimestampType\n",
    "schema = StructType() \\\n",
    "    .add(\"symbol\", StringType()) \\\n",
    "    .add(\"price\", FloatType()) \\\n",
    "    .add(\"volume\", IntegerType()) \\\n",
    "    .add(\"timestamp\", TimestampType()) \\\n",
    "    .add(\"exchange\", StringType())\n",
    "\n",
    "print(\"🌊 Starting streaming from Kafka...\")\n",
    "\n",
    "# Read from Kafka (unchanged)\n",
    "streaming_df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \\\n",
    "    .option(\"subscribe\", \"test-topic\") \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .option(\"failOnDataLoss\", \"false\") \\\n",
    "    .load()\n",
    "\n",
    "# Parse streaming data and convert timestamp\n",
    "parsed_stream = streaming_df.selectExpr(\"CAST(value AS STRING) as json_str\") \\\n",
    "    .select(from_json(\"json_str\", schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\") \\\n",
    "    .withColumn(\"timestamp\", to_timestamp(col(\"timestamp\"))) \\\n",
    "    .withColumn(\"processing_time\", lit(int(time.time())))\n",
    "\n",
    "# Enrichment function (unchanged)\n",
    "def enrich_and_store_stream(batch_df, batch_id):\n",
    "    if batch_df.count() == 0:\n",
    "        return\n",
    "    \n",
    "    print(f\"🌊 Processing streaming batch {batch_id} with {batch_df.count()} records\")\n",
    "    \n",
    "    exchanges = [row['exchange'] for row in batch_df.select(\"exchange\").distinct().collect()]\n",
    "    weather_data = {}\n",
    "    \n",
    "    for ex in exchanges:\n",
    "        city = exchange_city_map.get(ex, \"New York\")\n",
    "        print(f\"🌤️ Fetching weather for {ex} ({city})\")\n",
    "        weather_data[ex] = fetch_weather_cached(city)\n",
    "    \n",
    "    enriched_df = batch_df\n",
    "    for col_name in [\"temperature\", \"humidity\", \"condition\"]:\n",
    "        enriched_df = enriched_df.withColumn(col_name, lit(None))\n",
    "    \n",
    "    for exchange, weather in weather_data.items():\n",
    "        enriched_df = enriched_df \\\n",
    "            .withColumn(\"temperature\", when(col(\"exchange\") == exchange, lit(weather.get(\"temperature\"))).otherwise(col(\"temperature\"))) \\\n",
    "            .withColumn(\"humidity\", when(col(\"exchange\") == exchange, lit(weather.get(\"humidity\"))).otherwise(col(\"humidity\"))) \\\n",
    "            .withColumn(\"condition\", when(col(\"exchange\") == exchange, lit(weather.get(\"condition\"))).otherwise(col(\"condition\")))\n",
    "    \n",
    "    enriched_df = enriched_df \\\n",
    "        .withColumn(\"date\", date_format(col(\"timestamp\"), \"yyyy-MM-dd\"))\n",
    "    \n",
    "    # Handle schema changes with mergeSchema option\n",
    "    enriched_df.write.format(\"delta\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .option(\"mergeSchema\", \"true\") \\\n",
    "        .partitionBy(\"date\") \\\n",
    "        .save(\"s3a://test-bucket/delta-tables/streaming_trades\")\n",
    "    \n",
    "    print(f\"✅ Batch {batch_id} written to streaming storage with date partitioning\")\n",
    "# Start the streaming query (unchanged)\n",
    "streaming_query = parsed_stream.writeStream \\\n",
    "    .foreachBatch(enrich_and_store_stream) \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .trigger(processingTime='5 seconds') \\\n",
    "    .option(\"checkpointLocation\", \"s3a://test-bucket/checkpoints/streaming\") \\\n",
    "    .start()\n",
    "\n",
    "print(\"🌊 Streaming query started! Processing every 5 seconds...\")\n",
    "print(\"💡 Run the producer in another terminal to see real-time processing\")\n",
    "print(\"🌍 Weather data will be fetched for multiple cities based on exchanges\")\n",
    "\n",
    "try:\n",
    "    streaming_query.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n🛑 Stopping streaming...\")\n",
    "    streaming_query.stop()\n",
    "    print(\"✅ Streaming stopped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85bbfc65-56ca-401b-b11f-ac1535d41762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scripts/hourly_processor.py\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, date_format, avg, sum as spark_sum, count, max as spark_max, min as spark_min, lit\n",
    "from datetime import datetime, timedelta\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def create_spark_session():\n",
    "    return SparkSession.builder \\\n",
    "        .appName(\"FinTechHourlyProcessor\") \\\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "        .config(\"spark.jars.packages\", \n",
    "                \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1,\"\n",
    "                \"io.delta:delta-core_2.12:2.4.0,\"\n",
    "                \"org.apache.hadoop:hadoop-aws:3.3.4,\"\n",
    "                \"com.amazonaws:aws-java-sdk-bundle:1.12.262\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://localstack:4566\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", \"test\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", \"test\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "def run_hourly_processing():\n",
    "    \"\"\"\n",
    "    Process the previous completed hour.\n",
    "    E.g., if it's 10:05, process 09:00-09:59\n",
    "    \"\"\"\n",
    "    spark = create_spark_session()\n",
    "    spark.sparkContext.setLogLevel(\"WARN\")\n",
    "    \n",
    "    try:\n",
    "        # Calculate previous hour\n",
    "        current_time = datetime.now()\n",
    "        previous_hour = current_time.replace(minute=0, second=0, microsecond=0) #- timedelta(hours=1)\n",
    "        target_hour = previous_hour.strftime(\"%Y-%m-%d %H\")\n",
    "        target_date = previous_hour.strftime(\"%Y-%m-%d\")\n",
    "        \n",
    "        logger.info(f\"🕐 Processing hourly data for: {target_hour}\")\n",
    "        \n",
    "        # Read streaming data for the specific hour\n",
    "        streaming_data = spark.read.format(\"delta\") \\\n",
    "            .load(\"s3a://test-bucket/delta-tables/streaming_trades\") \\\n",
    "            .filter(col(\"date\") == target_date) \\\n",
    "            .filter(date_format(col(\"timestamp\"), \"yyyy-MM-dd HH\") == target_hour)\n",
    "        \n",
    "        record_count = streaming_data.count()\n",
    "        \n",
    "        if record_count == 0:\n",
    "            logger.warning(f\"⚠️ No streaming data found for hour: {target_hour}\")\n",
    "            return\n",
    "        \n",
    "        logger.info(f\"📈 Processing {record_count} records for hour: {target_hour}\")\n",
    "        \n",
    "        # Create hourly aggregation\n",
    "        hourly_summary = streaming_data \\\n",
    "            .withColumn(\"hour\", lit(target_hour)) \\\n",
    "            .groupBy(\"symbol\", \"exchange\", \"hour\", \"date\", \"condition\") \\\n",
    "            .agg(\n",
    "                avg(\"price\").alias(\"avg_price\"),\n",
    "                spark_min(\"price\").alias(\"min_price\"),\n",
    "                spark_max(\"price\").alias(\"max_price\"),\n",
    "                spark_sum(\"volume\").alias(\"total_volume\"),\n",
    "                count(\"*\").alias(\"trade_count\"),\n",
    "                avg(\"temperature\").alias(\"avg_temperature\"),\n",
    "                avg(\"humidity\").alias(\"avg_humidity\")\n",
    "            ) \\\n",
    "            .withColumn(\"batch_processed_at\", lit(datetime.now().isoformat()))\n",
    "        \n",
    "        hourly_count = hourly_summary.count()\n",
    "        logger.info(f\"📊 Created {hourly_count} hourly summary records\")\n",
    "        \n",
    "        if hourly_count > 0:\n",
    "            # Use merge/upsert to replace only this specific hour\n",
    "            hourly_summary.write.format(\"delta\") \\\n",
    "                .mode(\"overwrite\") \\\n",
    "                .option(\"replaceWhere\", f\"hour = '{target_hour}'\") \\\n",
    "                .save(\"s3a://test-bucket/delta-tables/hourly_summaries\")\n",
    "            \n",
    "            logger.info(f\"✅ Hourly summary updated for {target_hour}\")\n",
    "        \n",
    "        logger.info(f\"🎉 Hourly processing completed for {target_hour}!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Hourly processing failed: {str(e)}\")\n",
    "        raise\n",
    "    finally:\n",
    "        spark.stop()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ce8f2d-9d62-4aab-89dc-c56879173c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scripts/daily_processor.py\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, date_format, avg, sum as spark_sum, count, max as spark_max, min as spark_min, lit\n",
    "from datetime import datetime, timedelta\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def create_spark_session():\n",
    "    return SparkSession.builder \\\n",
    "        .appName(\"FinTechDailyProcessor\") \\\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "        .config(\"spark.jars.packages\", \n",
    "                \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1,\"\n",
    "                \"io.delta:delta-core_2.12:2.4.0,\"\n",
    "                \"org.apache.hadoop:hadoop-aws:3.3.4,\"\n",
    "                \"com.amazonaws:aws-java-sdk-bundle:1.12.262\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://localstack:4566\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", \"test\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", \"test\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "def run_daily_processing():\n",
    "    \"\"\"\n",
    "    Process yesterday's complete day for daily summaries.\n",
    "    \"\"\"\n",
    "    spark = create_spark_session()\n",
    "    spark.sparkContext.setLogLevel(\"WARN\")\n",
    "    \n",
    "    try:\n",
    "        # Calculate yesterday\n",
    "        yesterday = (datetime.now() - timedelta(days=0)).strftime(\"%Y-%m-%d\")\n",
    "        \n",
    "        logger.info(f\"📅 Processing daily data for: {yesterday}\")\n",
    "        \n",
    "        # Read all streaming data for yesterday\n",
    "        streaming_data = spark.read.format(\"delta\") \\\n",
    "            .load(\"s3a://test-bucket/delta-tables/streaming_trades\") \\\n",
    "            .filter(col(\"date\") == yesterday)\n",
    "        \n",
    "        record_count = streaming_data.count()\n",
    "        \n",
    "        if record_count == 0:\n",
    "            logger.warning(f\"⚠️ No streaming data found for date: {yesterday}\")\n",
    "            return\n",
    "        \n",
    "        logger.info(f\"📈 Processing {record_count} records for date: {yesterday}\")\n",
    "        \n",
    "        # Create daily aggregation\n",
    "        daily_summary = streaming_data \\\n",
    "            .groupBy(\"symbol\", \"exchange\", \"date\") \\\n",
    "            .agg(\n",
    "                avg(\"price\").alias(\"avg_price\"),\n",
    "                spark_min(\"price\").alias(\"min_price\"),\n",
    "                spark_max(\"price\").alias(\"max_price\"),\n",
    "                spark_sum(\"volume\").alias(\"total_volume\"),\n",
    "                count(\"*\").alias(\"trade_count\"),\n",
    "                avg(\"temperature\").alias(\"avg_temperature\"),\n",
    "                avg(\"humidity\").alias(\"avg_humidity\")\n",
    "            ) \\\n",
    "            .withColumn(\"batch_processed_at\", lit(datetime.now().isoformat()))\n",
    "        \n",
    "        daily_count = daily_summary.count()\n",
    "        logger.info(f\"📅 Created {daily_count} daily summary records\")\n",
    "        \n",
    "        if daily_count > 0:\n",
    "            # Replace only yesterday's daily summary\n",
    "            daily_summary.write.format(\"delta\") \\\n",
    "                .mode(\"overwrite\") \\\n",
    "                .option(\"replaceWhere\", f\"date = '{yesterday}'\") \\\n",
    "                .save(\"s3a://test-bucket/delta-tables/daily_summaries\")\n",
    "            \n",
    "            logger.info(f\"✅ Daily summary updated for {yesterday}\")\n",
    "        \n",
    "        logger.info(f\"🎉 Daily processing completed for {yesterday}!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Daily processing failed: {str(e)}\")\n",
    "        raise\n",
    "    finally:\n",
    "        spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ee1403-4cdf-4ec3-815d-23985b160bad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e861c00-2eea-4113-a547-73250bbc0a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Wrote 20321 records to streaming_trades\n",
      "✅ Hourly summary for 2025-08-10 00 written\n",
      "✅ Hourly summary for 2025-08-10 01 written\n",
      "✅ Hourly summary for 2025-08-10 02 written\n",
      "✅ Hourly summary for 2025-08-10 03 written\n",
      "✅ Hourly summary for 2025-08-10 04 written\n",
      "✅ Hourly summary for 2025-08-10 05 written\n",
      "✅ Hourly summary for 2025-08-10 06 written\n",
      "✅ Hourly summary for 2025-08-10 07 written\n",
      "✅ Hourly summary for 2025-08-10 08 written\n",
      "✅ Hourly summary for 2025-08-10 09 written\n",
      "✅ Hourly summary for 2025-08-10 10 written\n",
      "✅ Hourly summary for 2025-08-10 11 written\n",
      "✅ Hourly summary for 2025-08-10 12 written\n",
      "✅ Hourly summary for 2025-08-10 13 written\n",
      "✅ Hourly summary for 2025-08-10 14 written\n",
      "✅ Hourly summary for 2025-08-10 15 written\n",
      "✅ Hourly summary for 2025-08-10 16 written\n",
      "✅ Hourly summary for 2025-08-10 17 written\n",
      "✅ Hourly summary for 2025-08-10 18 written\n",
      "✅ Hourly summary for 2025-08-10 19 written\n",
      "✅ Hourly summary for 2025-08-10 20 written\n",
      "✅ Hourly summary for 2025-08-10 21 written\n",
      "✅ Hourly summary for 2025-08-10 22 written\n",
      "✅ Hourly summary for 2025-08-10 23 written\n",
      "✅ Hourly summary for 2025-08-11 00 written\n",
      "✅ Hourly summary for 2025-08-11 01 written\n",
      "✅ Hourly summary for 2025-08-11 02 written\n",
      "✅ Hourly summary for 2025-08-11 03 written\n",
      "✅ Hourly summary for 2025-08-11 04 written\n",
      "✅ Hourly summary for 2025-08-11 05 written\n",
      "✅ Hourly summary for 2025-08-11 06 written\n",
      "✅ Hourly summary for 2025-08-11 07 written\n",
      "✅ Hourly summary for 2025-08-11 08 written\n",
      "✅ Hourly summary for 2025-08-11 09 written\n",
      "✅ Hourly summary for 2025-08-11 10 written\n",
      "✅ Hourly summary for 2025-08-11 11 written\n",
      "✅ Hourly summary for 2025-08-11 12 written\n",
      "✅ Hourly summary for 2025-08-11 13 written\n",
      "✅ Hourly summary for 2025-08-11 14 written\n",
      "✅ Hourly summary for 2025-08-11 15 written\n",
      "✅ Hourly summary for 2025-08-11 16 written\n",
      "✅ Hourly summary for 2025-08-11 17 written\n",
      "✅ Hourly summary for 2025-08-11 18 written\n",
      "✅ Hourly summary for 2025-08-11 19 written\n",
      "✅ Hourly summary for 2025-08-11 20 written\n",
      "✅ Hourly summary for 2025-08-11 21 written\n",
      "✅ Hourly summary for 2025-08-11 22 written\n",
      "✅ Hourly summary for 2025-08-11 23 written\n",
      "✅ Hourly summary for 2025-08-12 00 written\n",
      "✅ Hourly summary for 2025-08-12 01 written\n",
      "✅ Hourly summary for 2025-08-12 02 written\n",
      "✅ Hourly summary for 2025-08-12 03 written\n",
      "✅ Hourly summary for 2025-08-12 04 written\n",
      "✅ Hourly summary for 2025-08-12 05 written\n",
      "✅ Hourly summary for 2025-08-12 06 written\n",
      "✅ Hourly summary for 2025-08-12 07 written\n",
      "✅ Hourly summary for 2025-08-12 08 written\n",
      "✅ Hourly summary for 2025-08-12 09 written\n",
      "✅ Hourly summary for 2025-08-12 10 written\n",
      "✅ Hourly summary for 2025-08-12 11 written\n",
      "✅ Hourly summary for 2025-08-12 12 written\n",
      "✅ Hourly summary for 2025-08-12 13 written\n",
      "✅ Hourly summary for 2025-08-12 14 written\n",
      "✅ Hourly summary for 2025-08-12 15 written\n",
      "✅ Hourly summary for 2025-08-12 16 written\n",
      "✅ Hourly summary for 2025-08-12 17 written\n",
      "✅ Hourly summary for 2025-08-12 18 written\n",
      "✅ Hourly summary for 2025-08-12 19 written\n",
      "✅ Hourly summary for 2025-08-12 20 written\n",
      "✅ Hourly summary for 2025-08-12 21 written\n",
      "✅ Hourly summary for 2025-08-12 22 written\n",
      "✅ Hourly summary for 2025-08-12 23 written\n",
      "✅ Hourly summary for 2025-08-13 00 written\n",
      "✅ Hourly summary for 2025-08-13 01 written\n",
      "✅ Hourly summary for 2025-08-13 02 written\n",
      "✅ Hourly summary for 2025-08-13 03 written\n",
      "✅ Hourly summary for 2025-08-13 04 written\n",
      "✅ Hourly summary for 2025-08-13 05 written\n",
      "✅ Hourly summary for 2025-08-13 06 written\n",
      "✅ Hourly summary for 2025-08-13 07 written\n",
      "✅ Hourly summary for 2025-08-13 08 written\n",
      "✅ Hourly summary for 2025-08-13 09 written\n",
      "✅ Hourly summary for 2025-08-13 10 written\n",
      "✅ Hourly summary for 2025-08-13 11 written\n",
      "✅ Hourly summary for 2025-08-13 12 written\n",
      "✅ Hourly summary for 2025-08-13 13 written\n",
      "✅ Hourly summary for 2025-08-13 14 written\n",
      "✅ Hourly summary for 2025-08-13 15 written\n",
      "✅ Hourly summary for 2025-08-13 16 written\n",
      "✅ Hourly summary for 2025-08-13 17 written\n",
      "✅ Hourly summary for 2025-08-13 18 written\n",
      "✅ Hourly summary for 2025-08-13 19 written\n",
      "✅ Hourly summary for 2025-08-13 20 written\n",
      "✅ Hourly summary for 2025-08-13 21 written\n",
      "✅ Hourly summary for 2025-08-13 22 written\n",
      "✅ Hourly summary for 2025-08-13 23 written\n",
      "✅ Hourly summary for 2025-08-14 00 written\n",
      "✅ Hourly summary for 2025-08-14 01 written\n",
      "✅ Hourly summary for 2025-08-14 02 written\n",
      "✅ Hourly summary for 2025-08-14 03 written\n",
      "✅ Hourly summary for 2025-08-14 04 written\n",
      "✅ Hourly summary for 2025-08-14 05 written\n",
      "✅ Hourly summary for 2025-08-14 06 written\n",
      "✅ Hourly summary for 2025-08-14 07 written\n",
      "✅ Hourly summary for 2025-08-14 08 written\n",
      "✅ Hourly summary for 2025-08-14 09 written\n",
      "✅ Hourly summary for 2025-08-14 10 written\n",
      "✅ Hourly summary for 2025-08-14 11 written\n",
      "✅ Hourly summary for 2025-08-14 12 written\n",
      "✅ Hourly summary for 2025-08-14 13 written\n",
      "✅ Hourly summary for 2025-08-14 14 written\n",
      "✅ Hourly summary for 2025-08-14 15 written\n",
      "✅ Hourly summary for 2025-08-14 16 written\n",
      "✅ Hourly summary for 2025-08-14 17 written\n",
      "✅ Hourly summary for 2025-08-14 18 written\n",
      "✅ Hourly summary for 2025-08-14 19 written\n",
      "✅ Hourly summary for 2025-08-14 20 written\n",
      "✅ Hourly summary for 2025-08-14 21 written\n",
      "✅ Hourly summary for 2025-08-14 22 written\n",
      "✅ Hourly summary for 2025-08-14 23 written\n",
      "✅ Hourly summary for 2025-08-15 00 written\n",
      "✅ Hourly summary for 2025-08-15 01 written\n",
      "✅ Hourly summary for 2025-08-15 02 written\n",
      "✅ Hourly summary for 2025-08-15 03 written\n",
      "✅ Hourly summary for 2025-08-15 04 written\n",
      "✅ Hourly summary for 2025-08-15 05 written\n",
      "✅ Hourly summary for 2025-08-15 06 written\n",
      "✅ Hourly summary for 2025-08-15 07 written\n",
      "✅ Hourly summary for 2025-08-15 08 written\n",
      "✅ Hourly summary for 2025-08-15 09 written\n",
      "✅ Hourly summary for 2025-08-15 10 written\n",
      "✅ Hourly summary for 2025-08-15 11 written\n",
      "✅ Hourly summary for 2025-08-15 12 written\n",
      "✅ Hourly summary for 2025-08-15 13 written\n",
      "✅ Hourly summary for 2025-08-15 14 written\n",
      "✅ Hourly summary for 2025-08-15 15 written\n",
      "✅ Hourly summary for 2025-08-15 16 written\n",
      "✅ Hourly summary for 2025-08-15 17 written\n",
      "✅ Hourly summary for 2025-08-15 18 written\n",
      "✅ Hourly summary for 2025-08-15 19 written\n",
      "✅ Hourly summary for 2025-08-15 20 written\n",
      "✅ Hourly summary for 2025-08-15 21 written\n",
      "✅ Hourly summary for 2025-08-15 22 written\n",
      "✅ Hourly summary for 2025-08-15 23 written\n",
      "✅ Hourly summary for 2025-08-16 00 written\n",
      "✅ Hourly summary for 2025-08-16 01 written\n",
      "✅ Hourly summary for 2025-08-16 02 written\n",
      "✅ Hourly summary for 2025-08-16 03 written\n",
      "✅ Hourly summary for 2025-08-16 04 written\n",
      "✅ Hourly summary for 2025-08-16 05 written\n",
      "✅ Hourly summary for 2025-08-16 06 written\n",
      "✅ Hourly summary for 2025-08-16 07 written\n",
      "✅ Hourly summary for 2025-08-16 08 written\n",
      "✅ Hourly summary for 2025-08-16 09 written\n",
      "✅ Hourly summary for 2025-08-16 10 written\n",
      "✅ Hourly summary for 2025-08-16 11 written\n",
      "✅ Hourly summary for 2025-08-16 12 written\n",
      "✅ Hourly summary for 2025-08-16 13 written\n",
      "✅ Hourly summary for 2025-08-16 14 written\n",
      "✅ Hourly summary for 2025-08-16 15 written\n",
      "✅ Hourly summary for 2025-08-16 16 written\n",
      "✅ Hourly summary for 2025-08-16 17 written\n",
      "✅ Hourly summary for 2025-08-16 18 written\n",
      "✅ Hourly summary for 2025-08-16 19 written\n",
      "✅ Hourly summary for 2025-08-16 20 written\n",
      "✅ Hourly summary for 2025-08-16 21 written\n",
      "✅ Hourly summary for 2025-08-16 22 written\n",
      "✅ Hourly summary for 2025-08-16 23 written\n",
      "✅ Hourly aggregations completed\n",
      "✅ Daily summary for 2025-08-10 written\n",
      "✅ Daily summary for 2025-08-11 written\n",
      "✅ Daily summary for 2025-08-12 written\n",
      "✅ Daily summary for 2025-08-13 written\n",
      "✅ Daily summary for 2025-08-14 written\n",
      "✅ Daily summary for 2025-08-15 written\n",
      "✅ Daily summary for 2025-08-16 written\n",
      "✅ Daily aggregations completed\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, to_timestamp, date_format, avg, min as spark_min, max as spark_max, sum as spark_sum, count\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, IntegerType, TimestampType, LongType, DoubleType\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "\n",
    "# Initialize Spark session (same as streaming_processor.py, without Kafka)\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"GenerateSampleDataAndAggregations\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.jars.packages\", \n",
    "            \"io.delta:delta-core_2.12:2.4.0,\"\n",
    "            \"org.apache.hadoop:hadoop-aws:3.3.4,\"\n",
    "            \"com.amazonaws:aws-java-sdk-bundle:1.12.262\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://localstack:4566\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"test\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"test\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "# Define symbols and exchanges (match prod.py, add XTKS for streaming_processor.py)\n",
    "symbols = {\n",
    "    'XNAS': ['AAPL', 'MSFT', 'TSLA', 'GOOGL'],\n",
    "    'XNYS': ['JPM', 'GE', 'XOM', 'BAC'],\n",
    "    'XLON': ['BP', 'VODL', 'HSBA', 'AZN'],\n",
    "    'XTKS': ['TM', 'SONY', 'HMC', 'MUFG']\n",
    "}\n",
    "exchanges = list(symbols.keys())\n",
    "weather_conditions = ['clear', 'rain', 'cloudy', 'snow']\n",
    "\n",
    "# Set date range: August 10–16, 2025 (7 days before August 17, 2025)\n",
    "start_date = datetime(2025, 8, 10)\n",
    "\n",
    "# Generate raw trade data for streaming_trades\n",
    "data = []\n",
    "for day in range(7):  # August 10–16, 2025\n",
    "    for hour in range(24):\n",
    "        timestamp = start_date + timedelta(days=day, hours=hour)\n",
    "        for exchange in exchanges:\n",
    "            for symbol in symbols[exchange]:\n",
    "                for _ in range(random.randint(5, 10)):  # 5-10 trades per symbol per hour\n",
    "                    trade_time = timestamp + timedelta(seconds=random.randint(0, 3600))\n",
    "                    data.append((\n",
    "                        symbol,\n",
    "                        round(random.uniform(100, 300), 2),  # Match prod.py price range\n",
    "                        random.randint(10, 1000),  # Match prod.py volume range\n",
    "                        trade_time.isoformat(),  # String to match prod.py\n",
    "                        exchange,\n",
    "                        round(random.uniform(15, 30), 1),  # Match streaming_processor.py weather\n",
    "                        random.randint(40, 80),\n",
    "                        random.choice(weather_conditions),\n",
    "                        trade_time.strftime(\"%Y-%m-%d\"),\n",
    "                        int(trade_time.timestamp())\n",
    "                    ))\n",
    "\n",
    "# Corrected schema with StructField\n",
    "schema = StructType([\n",
    "    StructField(\"symbol\", StringType(), True),\n",
    "    StructField(\"price\", FloatType(), True),\n",
    "    StructField(\"volume\", IntegerType(), True),\n",
    "    StructField(\"timestamp\", StringType(), True),  # String to match prod.py\n",
    "    StructField(\"exchange\", StringType(), True),\n",
    "    StructField(\"temperature\", DoubleType(), True),\n",
    "    StructField(\"humidity\", IntegerType(), True),\n",
    "    StructField(\"condition\", StringType(), True),\n",
    "    StructField(\"date\", StringType(), True),\n",
    "    StructField(\"processing_time\", LongType(), True)\n",
    "])\n",
    "\n",
    "raw_df = spark.createDataFrame(data, schema=schema) \\\n",
    "    .withColumn(\"timestamp\", to_timestamp(col(\"timestamp\")))  # Convert to TimestampType for streaming_processor.py\n",
    "\n",
    "raw_df.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .partitionBy(\"date\") \\\n",
    "    .save(\"s3a://test-bucket/delta-tables/streaming_trades\")\n",
    "print(f\"✅ Wrote {raw_df.count()} records to streaming_trades\")\n",
    "\n",
    "# Apply hourly aggregations (match hourly_processor.py)\n",
    "for day in range(7):\n",
    "    target_date = (start_date + timedelta(days=day)).strftime(\"%Y-%m-%d\")\n",
    "    for hour in range(24):\n",
    "        target_time = start_date + timedelta(days=day, hours=hour)\n",
    "        target_hour = target_time.strftime(\"%Y-%m-%d %H\")\n",
    "        \n",
    "        streaming_data = spark.read.format(\"delta\") \\\n",
    "            .load(\"s3a://test-bucket/delta-tables/streaming_trades\") \\\n",
    "            .filter(col(\"date\") == target_date) \\\n",
    "            .filter(date_format(col(\"timestamp\"), \"yyyy-MM-dd HH\") == target_hour)\n",
    "        \n",
    "        if streaming_data.count() > 0:\n",
    "            hourly_summary = streaming_data \\\n",
    "                .withColumn(\"hour\", lit(target_hour)) \\\n",
    "                .groupBy(\"symbol\", \"exchange\", \"hour\", \"date\", \"condition\") \\\n",
    "                .agg(\n",
    "                    avg(\"price\").alias(\"avg_price\"),\n",
    "                    spark_min(\"price\").alias(\"min_price\"),\n",
    "                    spark_max(\"price\").alias(\"max_price\"),\n",
    "                    spark_sum(\"volume\").alias(\"total_volume\"),\n",
    "                    count(\"*\").alias(\"trade_count\"),\n",
    "                    avg(\"temperature\").alias(\"avg_temperature\"),\n",
    "                    avg(\"humidity\").alias(\"avg_humidity\")\n",
    "                ) \\\n",
    "                .withColumn(\"batch_processed_at\", lit(datetime.now().isoformat()))\n",
    "            \n",
    "            hourly_summary.write.format(\"delta\") \\\n",
    "                .mode(\"overwrite\") \\\n",
    "                .option(\"replaceWhere\", f\"hour = '{target_hour}'\") \\\n",
    "                .save(\"s3a://test-bucket/delta-tables/hourly_summaries\")\n",
    "            print(f\"✅ Hourly summary for {target_hour} written\")\n",
    "\n",
    "print(\"✅ Hourly aggregations completed\")\n",
    "\n",
    "# Apply daily aggregations (match daily_processor.py)\n",
    "for day in range(7):\n",
    "    target_date = (start_date + timedelta(days=day)).strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    streaming_data = spark.read.format(\"delta\") \\\n",
    "        .load(\"s3a://test-bucket/delta-tables/streaming_trades\") \\\n",
    "        .filter(col(\"date\") == target_date)\n",
    "    \n",
    "    if streaming_data.count() > 0:\n",
    "        daily_summary = streaming_data \\\n",
    "            .groupBy(\"symbol\", \"exchange\", \"date\") \\\n",
    "            .agg(\n",
    "                avg(\"price\").alias(\"avg_price\"),\n",
    "                spark_min(\"price\").alias(\"min_price\"),\n",
    "                spark_max(\"price\").alias(\"max_price\"),\n",
    "                spark_sum(\"volume\").alias(\"total_volume\"),\n",
    "                count(\"*\").alias(\"trade_count\"),\n",
    "                avg(\"temperature\").alias(\"avg_temperature\"),\n",
    "                avg(\"humidity\").alias(\"avg_humidity\")\n",
    "            ) \\\n",
    "            .withColumn(\"batch_processed_at\", lit(datetime.now().isoformat()))\n",
    "        \n",
    "        daily_summary.write.format(\"delta\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .option(\"replaceWhere\", f\"date = '{target_date}'\") \\\n",
    "            .save(\"s3a://test-bucket/delta-tables/daily_summaries\")\n",
    "        print(f\"✅ Daily summary for {target_date} written\")\n",
    "\n",
    "print(\"✅ Daily aggregations completed\")\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed57e71-a643-4ed9-ad6a-04712d7ccd70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
