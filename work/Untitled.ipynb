{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c847ac-c457-46ae-95bc-f8e60b2ce4fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Bucket created!\n",
      "‚úÖ Job completed!\n",
      "\n",
      "Files in bucket:\n",
      "  - output.parquet/_SUCCESS\n",
      "  - output.parquet/part-00000-c1138f74-f87f-4e02-ba73-140ad38069cf-c000.snappy.parquet\n",
      "  - output.parquet/part-00001-c1138f74-f87f-4e02-ba73-140ad38069cf-c000.snappy.parquet\n",
      "  - output.parquet/part-00002-c1138f74-f87f-4e02-ba73-140ad38069cf-c000.snappy.parquet\n",
      "  - output.parquet/part-00003-c1138f74-f87f-4e02-ba73-140ad38069cf-c000.snappy.parquet\n",
      "  - output.parquet/part-00004-c1138f74-f87f-4e02-ba73-140ad38069cf-c000.snappy.parquet\n",
      "  - output.parquet/part-00005-c1138f74-f87f-4e02-ba73-140ad38069cf-c000.snappy.parquet\n",
      "  - output.parquet/part-00006-c1138f74-f87f-4e02-ba73-140ad38069cf-c000.snappy.parquet\n",
      "  - output.parquet/part-00007-c1138f74-f87f-4e02-ba73-140ad38069cf-c000.snappy.parquet\n",
      "  - output.parquet/part-00008-c1138f74-f87f-4e02-ba73-140ad38069cf-c000.snappy.parquet\n",
      "  - output.parquet/part-00009-c1138f74-f87f-4e02-ba73-140ad38069cf-c000.snappy.parquet\n",
      "  - output.parquet/part-00010-c1138f74-f87f-4e02-ba73-140ad38069cf-c000.snappy.parquet\n",
      "  - output.parquet/part-00011-c1138f74-f87f-4e02-ba73-140ad38069cf-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "%run /home/jovyan/work/testing.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adafd712-f11d-44a0-a186-9a8be0131af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Spark is ready!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TestS3\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.4\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://localstack:4566\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"test\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"test\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"‚úÖ Spark is ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40e1617f-8d0c-407a-afa0-564c5638f5be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| id|doubled|\n",
      "+---+-------+\n",
      "| 16|     32|\n",
      "| 17|     34|\n",
      "| 18|     36|\n",
      "| 19|     38|\n",
      "| 20|     40|\n",
      "| 21|     42|\n",
      "| 22|     44|\n",
      "| 23|     46|\n",
      "| 24|     48|\n",
      "| 41|     82|\n",
      "| 42|     84|\n",
      "| 43|     86|\n",
      "| 44|     88|\n",
      "| 45|     90|\n",
      "| 46|     92|\n",
      "| 47|     94|\n",
      "| 48|     96|\n",
      "| 49|     98|\n",
      "| 66|    132|\n",
      "| 67|    134|\n",
      "+---+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read back the data\n",
    "df_read = spark.read.parquet(\"s3a://test-bucket/output.parquet\")\n",
    "df_read.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560e00eb-5b68-4ed3-9a87-11f5cedf56a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2268dd3-131b-41c5-a457-f13df5397c87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1b35b47a",
   "metadata": {},
   "source": [
    "### Kafka spark\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfc6d8c4-b887-4bad-add8-345b975247fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting boto3\n",
      "  Obtaining dependency information for boto3 from https://files.pythonhosted.org/packages/b6/6d/79fad38fcd7e1fc6961061b46cc87706c5c946088bc4620abf0d0aa49420/boto3-1.40.9-py3-none-any.whl.metadata\n",
      "  Downloading boto3-1.40.9-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting botocore<1.41.0,>=1.40.9 (from boto3)\n",
      "  Obtaining dependency information for botocore<1.41.0,>=1.40.9 from https://files.pythonhosted.org/packages/02/e9/367e81e114deb92a6e0d5740f0bff4548af710be318af65265b9aad72237/botocore-1.40.9-py3-none-any.whl.metadata\n",
      "  Downloading botocore-1.40.9-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from boto3)\n",
      "  Obtaining dependency information for jmespath<2.0.0,>=0.7.1 from https://files.pythonhosted.org/packages/31/b4/b9b800c45527aadd64d5b442f9b932b00648617eb5d63d2c7a6587b7cafc/jmespath-1.0.1-py3-none-any.whl.metadata\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting s3transfer<0.14.0,>=0.13.0 (from boto3)\n",
      "  Obtaining dependency information for s3transfer<0.14.0,>=0.13.0 from https://files.pythonhosted.org/packages/6d/4f/d073e09df851cfa251ef7840007d04db3293a0482ce607d2b993926089be/s3transfer-0.13.1-py3-none-any.whl.metadata\n",
      "  Downloading s3transfer-0.13.1-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.11/site-packages (from botocore<1.41.0,>=1.40.9->boto3) (2.8.2)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /opt/conda/lib/python3.11/site-packages (from botocore<1.41.0,>=1.40.9->boto3) (2.0.4)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.41.0,>=1.40.9->boto3) (1.16.0)\n",
      "Downloading boto3-1.40.9-py3-none-any.whl (140 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m140.1/140.1 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading botocore-1.40.9-py3-none-any.whl (14.0 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Downloading s3transfer-0.13.1-py3-none-any.whl (85 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m85.3/85.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: jmespath, botocore, s3transfer, boto3\n",
      "Successfully installed boto3-1.40.9 botocore-1.40.9 jmespath-1.0.1 s3transfer-0.13.1\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests) (2023.7.22)\n"
     ]
    }
   ],
   "source": [
    "!pip install boto3\n",
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74aca7f8-c8c9-4153-b5c4-4438a157b5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket created!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import boto3\n",
    "s3 = boto3.client('s3', endpoint_url='http://localstack:4566', \n",
    "                  aws_access_key_id='test', aws_secret_access_key='test')\n",
    "s3.create_bucket(Bucket='test-bucket')\n",
    "print(\"Bucket created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b1d9a14-bfb5-4afd-bd8d-b9d8e6fd0446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kafka-python\n",
      "  Obtaining dependency information for kafka-python from https://files.pythonhosted.org/packages/e6/35/e8bfed5425e8fe685bd03ec3f5135ee8b88c11558baa59c0d12fbd2a20ae/kafka_python-2.2.15-py2.py3-none-any.whl.metadata\n",
      "  Downloading kafka_python-2.2.15-py2.py3-none-any.whl.metadata (10.0 kB)\n",
      "Downloading kafka_python-2.2.15-py2.py3-none-any.whl (309 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m309.8/309.8 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hInstalling collected packages: kafka-python\n",
      "Successfully installed kafka-python-2.2.15\n"
     ]
    }
   ],
   "source": [
    "!pip install kafka-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb6f4b82-00e4-4f5a-9704-4db910db3bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic created!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from kafka.admin import KafkaAdminClient, NewTopic\n",
    "\n",
    "admin = KafkaAdminClient(bootstrap_servers='kafka:29092')\n",
    "topic = NewTopic(name='test-topic', num_partitions=1, replication_factor=1)\n",
    "admin.create_topics([topic])\n",
    "print(\"Topic created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e47686bf-f20a-4b75-814e-4115b52267cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent: {'id': 0, 'name': 'Message 0', 'value': 0}\n",
      "Sent: {'id': 1, 'name': 'Message 1', 'value': 100}\n",
      "Sent: {'id': 2, 'name': 'Message 2', 'value': 200}\n",
      "Sent: {'id': 3, 'name': 'Message 3', 'value': 300}\n",
      "Sent: {'id': 4, 'name': 'Message 4', 'value': 400}\n",
      "Sent: {'id': 5, 'name': 'Message 5', 'value': 500}\n",
      "Sent: {'id': 6, 'name': 'Message 6', 'value': 600}\n",
      "Sent: {'id': 7, 'name': 'Message 7', 'value': 700}\n",
      "Sent: {'id': 8, 'name': 'Message 8', 'value': 800}\n",
      "Sent: {'id': 9, 'name': 'Message 9', 'value': 900}\n",
      "Done sending messages!\n"
     ]
    }
   ],
   "source": [
    "%run /home/jovyan/work/simple_producer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "538079dc-4609-433e-8b3b-c2aabcfb1f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Spark session created\n",
      "üìñ Reading from Kafka...\n",
      "\n",
      "üìä Data from Kafka:\n",
      "+---+---------+-----+\n",
      "| id|     name|value|\n",
      "+---+---------+-----+\n",
      "|  0|Message 0|    0|\n",
      "|  1|Message 1|  100|\n",
      "|  2|Message 2|  200|\n",
      "|  3|Message 3|  300|\n",
      "|  4|Message 4|  400|\n",
      "|  5|Message 5|  500|\n",
      "|  6|Message 6|  600|\n",
      "|  7|Message 7|  700|\n",
      "|  8|Message 8|  800|\n",
      "|  9|Message 9|  900|\n",
      "+---+---------+-----+\n",
      "\n",
      "\n",
      "üíæ Writing to Delta Lake at: s3a://test-bucket/delta-tables/my_table\n",
      "‚úÖ Data saved to Delta Lake!\n",
      "\n",
      "üìñ Reading back from Delta Lake:\n",
      "+---+---------+-----+\n",
      "| id|     name|value|\n",
      "+---+---------+-----+\n",
      "|  0|Message 0|    0|\n",
      "|  1|Message 1|  100|\n",
      "|  2|Message 2|  200|\n",
      "|  3|Message 3|  300|\n",
      "|  4|Message 4|  400|\n",
      "|  5|Message 5|  500|\n",
      "|  6|Message 6|  600|\n",
      "|  7|Message 7|  700|\n",
      "|  8|Message 8|  800|\n",
      "|  9|Message 9|  900|\n",
      "+---+---------+-----+\n",
      "\n",
      "\n",
      "‚úÖ Test completed successfully!\n"
     ]
    }
   ],
   "source": [
    "%run /home/jovyan/work/simple_spark_consumer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587d7973-640b-4f56-8ba0-334748ae3c63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f91ad21-0237-4470-95b1-76e21da7a99c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fc5e868-5c51-445a-b6c7-cec0af444859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in s3://test-bucket/delta-table/:\n",
      " - delta-table/_delta_log/00000000000000000000.json (1248 bytes)\n",
      " - delta-table/part-00000-354a5e3f-e191-438b-bbac-aec33463ff8b-c000.snappy.parquet (1177 bytes)\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "# Configure boto3 for LocalStack\n",
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    endpoint_url='http://localstack:4566',\n",
    "    aws_access_key_id='test',\n",
    "    aws_secret_access_key='test',\n",
    "    region_name='us-east-1'\n",
    ")\n",
    "\n",
    "# List objects in the Delta table path\n",
    "try:\n",
    "    response = s3_client.list_objects_v2(Bucket='test-bucket', Prefix='delta-table/')\n",
    "    if 'Contents' in response:\n",
    "        print(\"Files in s3://test-bucket/delta-table/:\")\n",
    "        for obj in response['Contents']:\n",
    "            print(f\" - {obj['Key']} ({obj['Size']} bytes)\")\n",
    "    else:\n",
    "        print(\"No files found in s3://test-bucket/delta-table/ or bucket does not exist.\")\n",
    "except s3_client.exceptions.NoSuchBucket:\n",
    "    print(\"Bucket 'test-bucket' does not exist. Creating it...\")\n",
    "    s3_client.create_bucket(Bucket='test-bucket')\n",
    "    print(\"Bucket created. Rerun the Spark job to write the Delta table.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3461fa-5cdc-4560-8e78-7e020b78a94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark session with Delta support\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SimpleTest\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.jars.packages\", \n",
    "            \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1,\"\n",
    "            \"io.delta:delta-core_2.12:2.4.0,\"\n",
    "            \"org.apache.hadoop:hadoop-aws:3.3.4,\"\n",
    "            \"com.amazonaws:aws-java-sdk-bundle:1.12.262\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://localstack:4566\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"test\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"test\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .getOrCreate()\n",
    "output_path = \"s3a://test-bucket/delta-table\"\n",
    "print(\"\\nüìñ Reading back from Delta Lake:\")\n",
    "delta_df = spark.read.format(\"delta\").load(output_path)\n",
    "delta_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3258614a-3415-4299-8156-089c4961974c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdec18f-eea3-4b91-853d-be3bfd2b6b02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "227f47f6-f8b5-4403-8286-087807389ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Spark session created\n",
      "üìñ Reading from Kafka...\n",
      "\n",
      "üìä Data from Kafka:\n",
      "+---+---------+-----+\n",
      "| id|     name|value|\n",
      "+---+---------+-----+\n",
      "|  0|Message 0|    0|\n",
      "|  1|Message 1|  100|\n",
      "|  2|Message 2|  200|\n",
      "|  3|Message 3|  300|\n",
      "|  4|Message 4|  400|\n",
      "|  5|Message 5|  500|\n",
      "|  6|Message 6|  600|\n",
      "|  7|Message 7|  700|\n",
      "|  8|Message 8|  800|\n",
      "|  9|Message 9|  900|\n",
      "+---+---------+-----+\n",
      "\n",
      "\n",
      "üíæ Writing to Delta Lake at: s3a://test-bucket/delta-tables/my_table\n",
      "‚úÖ Data saved to Delta Lake!\n",
      "\n",
      "üìñ Reading back from Delta Lake:\n",
      "+---+---------+-----+\n",
      "| id|     name|value|\n",
      "+---+---------+-----+\n",
      "|  0|Message 0|    0|\n",
      "|  1|Message 1|  100|\n",
      "|  2|Message 2|  200|\n",
      "|  3|Message 3|  300|\n",
      "|  4|Message 4|  400|\n",
      "|  5|Message 5|  500|\n",
      "|  6|Message 6|  600|\n",
      "|  7|Message 7|  700|\n",
      "|  8|Message 8|  800|\n",
      "|  9|Message 9|  900|\n",
      "+---+---------+-----+\n",
      "\n",
      "\n",
      "‚úÖ Test completed successfully!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Create Spark session with Delta support\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SimpleTest\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.jars.packages\", \n",
    "            \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1,\"\n",
    "            \"io.delta:delta-core_2.12:2.4.0,\"\n",
    "            \"org.apache.hadoop:hadoop-aws:3.3.4,\"\n",
    "            \"com.amazonaws:aws-java-sdk-bundle:1.12.262\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://localstack:4566\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"test\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"test\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"‚úÖ Spark session created\")\n",
    "\n",
    "# Read from Kafka\n",
    "print(\"üìñ Reading from Kafka...\")\n",
    "df = spark \\\n",
    "    .read \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \\\n",
    "    .option(\"subscribe\", \"test-topic\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "# Parse JSON and select fields\n",
    "parsed_df = df.select(\n",
    "    col(\"value\").cast(\"string\")\n",
    ").select(\n",
    "    get_json_object(col(\"value\"), \"$.id\").alias(\"id\"),\n",
    "    get_json_object(col(\"value\"), \"$.name\").alias(\"name\"),\n",
    "    get_json_object(col(\"value\"), \"$.value\").alias(\"value\")\n",
    ")\n",
    "\n",
    "# Show data\n",
    "print(\"\\nüìä Data from Kafka:\")\n",
    "parsed_df.show()\n",
    "\n",
    "# Write to Delta Lake on S3\n",
    "#output_path = \"s3a://test-bucket/delta-table\"\n",
    "output_path = \"s3a://test-bucket/delta-tables/my_table\"\n",
    "print(f\"\\nüíæ Writing to Delta Lake at: {output_path}\")\n",
    "parsed_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(output_path)\n",
    "\n",
    "print(\"‚úÖ Data saved to Delta Lake!\")\n",
    "\n",
    "# Read back from Delta to verify\n",
    "print(\"\\nüìñ Reading back from Delta Lake:\")\n",
    "delta_df = spark.read.format(\"delta\").load(output_path)\n",
    "delta_df.show()\n",
    "\n",
    "print(\"\\n‚úÖ Test completed successfully!\")\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e5da1b-26c2-401d-8aa6-d8e11343e62f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c1fb65-c847-42f8-82a2-873c301aba04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "228c6eac-f4fd-4a63-a868-f99cdef7bc52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Sent: {'symbol': 'GOOGL', 'price': 289.41, 'volume': 877, 'timestamp': '2025-08-04T15:39:32.196996', 'exchange': 'XNAS'}\n",
      "‚úÖ Sent: {'symbol': 'GOOGL', 'price': 298.55, 'volume': 36, 'timestamp': '2025-08-04T15:39:33.355669', 'exchange': 'XNAS'}\n",
      "‚úÖ Sent: {'symbol': 'MSFT', 'price': 275.08, 'volume': 850, 'timestamp': '2025-08-04T15:39:34.356757', 'exchange': 'XNAS'}\n",
      "‚úÖ Sent: {'symbol': 'GOOGL', 'price': 205.54, 'volume': 737, 'timestamp': '2025-08-04T15:39:35.358286', 'exchange': 'XNAS'}\n",
      "‚úÖ Sent: {'symbol': 'TSLA', 'price': 152.65, 'volume': 780, 'timestamp': '2025-08-04T15:39:36.359188', 'exchange': 'XNAS'}\n",
      "‚úÖ Sent: {'symbol': 'TSLA', 'price': 246.13, 'volume': 401, 'timestamp': '2025-08-04T15:39:37.359791', 'exchange': 'XNAS'}\n",
      "‚úÖ Sent: {'symbol': 'MSFT', 'price': 209.88, 'volume': 114, 'timestamp': '2025-08-04T15:39:38.360518', 'exchange': 'XNAS'}\n",
      "‚úÖ Sent: {'symbol': 'AAPL', 'price': 189.29, 'volume': 299, 'timestamp': '2025-08-04T15:39:39.361496', 'exchange': 'XNAS'}\n",
      "‚úÖ Sent: {'symbol': 'AAPL', 'price': 231.23, 'volume': 487, 'timestamp': '2025-08-04T15:39:40.362404', 'exchange': 'XNAS'}\n",
      "‚úÖ Sent: {'symbol': 'TSLA', 'price': 291.49, 'volume': 32, 'timestamp': '2025-08-04T15:39:41.363468', 'exchange': 'XNAS'}\n",
      "‚úÖ Sent: {'symbol': 'GOOGL', 'price': 197.71, 'volume': 962, 'timestamp': '2025-08-04T15:39:42.364053', 'exchange': 'XNAS'}\n",
      "‚úÖ Sent: {'symbol': 'GOOGL', 'price': 265.85, 'volume': 746, 'timestamp': '2025-08-04T15:39:43.364715', 'exchange': 'XNAS'}\n",
      "‚úÖ Sent: {'symbol': 'AAPL', 'price': 291.7, 'volume': 691, 'timestamp': '2025-08-04T15:39:44.365142', 'exchange': 'XNAS'}\n",
      "‚úÖ Sent: {'symbol': 'GOOGL', 'price': 219.37, 'volume': 608, 'timestamp': '2025-08-04T15:39:45.366003', 'exchange': 'XNAS'}\n",
      "‚úÖ Sent: {'symbol': 'MSFT', 'price': 112.68, 'volume': 619, 'timestamp': '2025-08-04T15:39:46.367047', 'exchange': 'XNAS'}\n",
      "‚úÖ Sent: {'symbol': 'AAPL', 'price': 280.35, 'volume': 32, 'timestamp': '2025-08-04T15:39:47.367468', 'exchange': 'XNAS'}\n",
      "‚úÖ Sent: {'symbol': 'TSLA', 'price': 203.66, 'volume': 769, 'timestamp': '2025-08-04T15:39:48.368080', 'exchange': 'XNAS'}\n",
      "‚úÖ Sent: {'symbol': 'GOOGL', 'price': 217.16, 'volume': 553, 'timestamp': '2025-08-04T15:39:49.370209', 'exchange': 'XNAS'}\n",
      "‚úÖ Sent: {'symbol': 'MSFT', 'price': 248.3, 'volume': 812, 'timestamp': '2025-08-04T15:39:50.370690', 'exchange': 'XNAS'}\n",
      "‚úÖ Sent: {'symbol': 'GOOGL', 'price': 167.79, 'volume': 883, 'timestamp': '2025-08-04T15:39:51.371120', 'exchange': 'XNAS'}\n",
      "üöÄ Finished sending simulated trades.\n"
     ]
    }
   ],
   "source": [
    "%run /home/jovyan/work/prod.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52012d4f-71c9-4045-adae-f856a982fe24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Spark session created\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'StructField' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/work/stock_consumer.py:65\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚úÖ Spark session created\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Define schema matching prod.py\u001b[39;00m\n\u001b[1;32m     64\u001b[0m schema \u001b[38;5;241m=\u001b[39m StructType([\n\u001b[0;32m---> 65\u001b[0m     \u001b[43mStructField\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msymbol\u001b[39m\u001b[38;5;124m\"\u001b[39m, StringType()),\n\u001b[1;32m     66\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprice\u001b[39m\u001b[38;5;124m\"\u001b[39m, FloatType()),\n\u001b[1;32m     67\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvolume\u001b[39m\u001b[38;5;124m\"\u001b[39m, IntegerType()),\n\u001b[1;32m     68\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m, StringType()),\n\u001b[1;32m     69\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexchange\u001b[39m\u001b[38;5;124m\"\u001b[39m, StringType())\n\u001b[1;32m     70\u001b[0m ])\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# Read stream from Kafka\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müìñ Reading stream from Kafka...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'StructField' is not defined"
     ]
    }
   ],
   "source": [
    "%run /home/jovyan/work/stock_consumer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "04ff9ddf-f933-47ae-a218-836ef3b750a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Spark session created\n",
      "üìñ Reading from Kafka...\n",
      "\n",
      "üîç Raw Kafka data:\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "|raw_value                                                                                                         |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "|{\"symbol\": \"MSFT\", \"price\": 154.31, \"volume\": 622, \"timestamp\": \"2025-08-04T17:39:06.433532\", \"exchange\": \"XNAS\"} |\n",
      "|{\"symbol\": \"GOOGL\", \"price\": 188.03, \"volume\": 656, \"timestamp\": \"2025-08-04T17:39:07.545608\", \"exchange\": \"XNAS\"}|\n",
      "|{\"symbol\": \"TSLA\", \"price\": 174.87, \"volume\": 519, \"timestamp\": \"2025-08-04T17:39:08.546064\", \"exchange\": \"XNAS\"} |\n",
      "|{\"symbol\": \"AAPL\", \"price\": 166.18, \"volume\": 881, \"timestamp\": \"2025-08-04T17:39:09.546507\", \"exchange\": \"XNAS\"} |\n",
      "|{\"symbol\": \"TSLA\", \"price\": 168.09, \"volume\": 163, \"timestamp\": \"2025-08-04T17:39:10.547317\", \"exchange\": \"XNAS\"} |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "üìä Parsed data:\n",
      "+------+------+------+--------------------+--------+\n",
      "|symbol| price|volume|           timestamp|exchange|\n",
      "+------+------+------+--------------------+--------+\n",
      "|  MSFT|154.31|   622|2025-08-04T17:39:...|    XNAS|\n",
      "| GOOGL|188.03|   656|2025-08-04T17:39:...|    XNAS|\n",
      "|  TSLA|174.87|   519|2025-08-04T17:39:...|    XNAS|\n",
      "|  AAPL|166.18|   881|2025-08-04T17:39:...|    XNAS|\n",
      "|  TSLA|168.09|   163|2025-08-04T17:39:...|    XNAS|\n",
      "|  AAPL|293.89|   883|2025-08-04T17:39:...|    XNAS|\n",
      "|  MSFT|125.53|   881|2025-08-04T17:39:...|    XNAS|\n",
      "| GOOGL|212.36|   853|2025-08-04T17:39:...|    XNAS|\n",
      "|  TSLA|172.61|   370|2025-08-04T17:39:...|    XNAS|\n",
      "|  MSFT| 274.2|   276|2025-08-04T17:39:...|    XNAS|\n",
      "|  TSLA|248.99|   898|2025-08-04T17:39:...|    XNAS|\n",
      "|  MSFT|232.12|   656|2025-08-04T17:39:...|    XNAS|\n",
      "| GOOGL|183.06|   261|2025-08-04T17:39:...|    XNAS|\n",
      "|  TSLA|250.19|   127|2025-08-04T17:39:...|    XNAS|\n",
      "|  TSLA|216.91|   636|2025-08-04T17:39:...|    XNAS|\n",
      "|  AAPL|114.12|   259|2025-08-04T17:39:...|    XNAS|\n",
      "|  AAPL|104.07|   685|2025-08-04T17:39:...|    XNAS|\n",
      "|  MSFT|206.21|   795|2025-08-04T17:39:...|    XNAS|\n",
      "|  AAPL|233.73|   236|2025-08-04T17:39:...|    XNAS|\n",
      "|  TSLA|258.76|   837|2025-08-04T17:39:...|    XNAS|\n",
      "+------+------+------+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "üì¶ Processing 80 records\n",
      "üåç Exchanges found: ['XNAS']\n",
      "üå§Ô∏è Fetching weather for XNAS (New York)\n",
      "\n",
      "üåü Enriched data:\n",
      "+------+------+------+--------------------+--------+-----------+--------+---------+\n",
      "|symbol| price|volume|           timestamp|exchange|temperature|humidity|condition|\n",
      "+------+------+------+--------------------+--------+-----------+--------+---------+\n",
      "|  MSFT|154.31|   622|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "| GOOGL|188.03|   656|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "|  TSLA|174.87|   519|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "|  AAPL|166.18|   881|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "|  TSLA|168.09|   163|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "|  AAPL|293.89|   883|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "|  MSFT|125.53|   881|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "| GOOGL|212.36|   853|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "|  TSLA|172.61|   370|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "|  MSFT| 274.2|   276|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "|  TSLA|248.99|   898|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "|  MSFT|232.12|   656|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "| GOOGL|183.06|   261|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "|  TSLA|250.19|   127|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "|  TSLA|216.91|   636|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "|  AAPL|114.12|   259|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "|  AAPL|104.07|   685|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "|  MSFT|206.21|   795|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "|  AAPL|233.73|   236|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "|  TSLA|258.76|   837|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "+------+------+------+--------------------+--------+-----------+--------+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "\n",
      "üíæ Writing to Delta Lake...\n",
      "‚úÖ Data written to Delta Lake!\n",
      "\n",
      "üìñ Reading back from Delta Lake:\n",
      "+------+------+------+--------------------+--------+-----------+--------+---------+\n",
      "|symbol| price|volume|           timestamp|exchange|temperature|humidity|condition|\n",
      "+------+------+------+--------------------+--------+-----------+--------+---------+\n",
      "|  MSFT|154.31|   622|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "| GOOGL|188.03|   656|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "|  TSLA|174.87|   519|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "|  AAPL|166.18|   881|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "|  TSLA|168.09|   163|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "|  AAPL|293.89|   883|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "|  MSFT|125.53|   881|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "| GOOGL|212.36|   853|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "|  TSLA|172.61|   370|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "|  MSFT| 274.2|   276|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "|  TSLA|248.99|   898|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "|  MSFT|232.12|   656|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "| GOOGL|183.06|   261|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "|  TSLA|250.19|   127|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "|  TSLA|216.91|   636|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "|  AAPL|114.12|   259|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "|  AAPL|104.07|   685|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "|  MSFT|206.21|   795|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "|  AAPL|233.73|   236|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "|  TSLA|258.76|   837|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "+------+------+------+--------------------+--------+-----------+--------+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "‚úÖ Processing completed!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_json, expr\n",
    "from pyspark.sql.types import StructType, StringType, FloatType, IntegerType\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# Exchange ‚Üí City mapping\n",
    "exchange_city_map = {\n",
    "    \"XNAS\": \"New York\",\n",
    "    \"XNYS\": \"New York\", \n",
    "    \"XLON\": \"London\",\n",
    "    \"XTKS\": \"Tokyo\"\n",
    "}\n",
    "\n",
    "# Cache for weather per city\n",
    "weather_cache = {}\n",
    "CACHE_TTL_SECONDS = 600  # 10 minutes\n",
    "\n",
    "def fetch_weather_cached(city):\n",
    "    now = time.time()\n",
    "    cached = weather_cache.get(city)\n",
    "\n",
    "    if cached and now - cached[\"fetched_at\"] < CACHE_TTL_SECONDS:\n",
    "        return cached[\"data\"]\n",
    "\n",
    "    try:\n",
    "        url = f\"http://api.openweathermap.org/data/2.5/weather?q={city}&appid=34debbcec75c0e6f7351fcda60b141be&units=metric\"\n",
    "        response = requests.get(url, timeout=5)\n",
    "        data = response.json()\n",
    "        weather = {\n",
    "            \"temperature\": data[\"main\"][\"temp\"],\n",
    "            \"humidity\": data[\"main\"][\"humidity\"],\n",
    "            \"condition\": data[\"weather\"][0][\"description\"]\n",
    "        }\n",
    "        # Save to cache\n",
    "        weather_cache[city] = {\n",
    "            \"fetched_at\": now,\n",
    "            \"data\": weather\n",
    "        }\n",
    "        return weather\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Weather fetch failed for {city}: {e}\")\n",
    "        return {\"temperature\": None, \"humidity\": None, \"condition\": None}\n",
    "\n",
    "# Initialize Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"StockWeatherEnrichment\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.jars.packages\", \n",
    "            \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1,\"\n",
    "            \"io.delta:delta-core_2.12:2.4.0,\"\n",
    "            \"org.apache.hadoop:hadoop-aws:3.3.4,\"\n",
    "            \"com.amazonaws:aws-java-sdk-bundle:1.12.262\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://localstack:4566\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"test\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"test\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "print(\"‚úÖ Spark session created\")\n",
    "\n",
    "# CORRECT schema matching your producer data\n",
    "schema = StructType() \\\n",
    "    .add(\"symbol\", StringType()) \\\n",
    "    .add(\"price\", FloatType()) \\\n",
    "    .add(\"volume\", IntegerType()) \\\n",
    "    .add(\"timestamp\", StringType()) \\\n",
    "    .add(\"exchange\", StringType())\n",
    "\n",
    "print(\"üìñ Reading from Kafka...\")\n",
    "\n",
    "# Read from Kafka (batch mode for testing)\n",
    "df = spark \\\n",
    "    .read \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \\\n",
    "    .option(\"subscribe\", \"test-topic\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "# First, let's see the raw data\n",
    "print(\"\\nüîç Raw Kafka data:\")\n",
    "df.selectExpr(\"CAST(value AS STRING) as raw_value\").show(5, truncate=False)\n",
    "\n",
    "# Parse JSON with correct schema\n",
    "parsed_df = df.selectExpr(\"CAST(value AS STRING) as json_str\") \\\n",
    "    .select(from_json(\"json_str\", schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\")\n",
    "\n",
    "print(\"\\nüìä Parsed data:\")\n",
    "parsed_df.show()\n",
    "\n",
    "# Test enrichment function\n",
    "def enrich_with_weather_batch(input_df):\n",
    "    if input_df.count() == 0:\n",
    "        print(\"‚ö†Ô∏è No data to process\")\n",
    "        return input_df\n",
    "    \n",
    "    print(f\"üì¶ Processing {input_df.count()} records\")\n",
    "    \n",
    "    # Get unique exchanges\n",
    "    exchanges = [row['exchange'] for row in input_df.select(\"exchange\").distinct().collect()]\n",
    "    print(f\"üåç Exchanges found: {exchanges}\")\n",
    "    \n",
    "    # Fetch weather for each exchange\n",
    "    weather_data = {}\n",
    "    for ex in exchanges:\n",
    "        city = exchange_city_map.get(ex, \"New York\")\n",
    "        print(f\"üå§Ô∏è Fetching weather for {ex} ({city})\")\n",
    "        weather_data[ex] = fetch_weather_cached(city)\n",
    "    \n",
    "    # Add weather columns using broadcast\n",
    "    from pyspark.sql.functions import lit, when\n",
    "    \n",
    "    enriched_df = input_df\n",
    "    \n",
    "    # Add weather columns based on exchange\n",
    "    for exchange, weather in weather_data.items():\n",
    "        enriched_df = enriched_df \\\n",
    "            .withColumn(\"temperature\", \n",
    "                when(col(\"exchange\") == exchange, lit(weather.get(\"temperature\")))\n",
    "                .otherwise(col(\"temperature\") if \"temperature\" in enriched_df.columns else lit(None))) \\\n",
    "            .withColumn(\"humidity\",\n",
    "                when(col(\"exchange\") == exchange, lit(weather.get(\"humidity\")))\n",
    "                .otherwise(col(\"humidity\") if \"humidity\" in enriched_df.columns else lit(None))) \\\n",
    "            .withColumn(\"condition\",\n",
    "                when(col(\"exchange\") == exchange, lit(weather.get(\"condition\")))\n",
    "                .otherwise(col(\"condition\") if \"condition\" in enriched_df.columns else lit(None)))\n",
    "    \n",
    "    return enriched_df\n",
    "\n",
    "# Process the data\n",
    "if parsed_df.count() > 0:\n",
    "    enriched_df = enrich_with_weather_batch(parsed_df)\n",
    "    \n",
    "    print(\"\\nüåü Enriched data:\")\n",
    "    enriched_df.show()\n",
    "    \n",
    "    # Write to Delta Lake\n",
    "    print(\"\\nüíæ Writing to Delta Lake...\")\n",
    "    enriched_df.write.format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .save(\"s3a://test-bucket/delta-tables/enriched_trades\")\n",
    "    \n",
    "    print(\"‚úÖ Data written to Delta Lake!\")\n",
    "    \n",
    "    # Read back to verify\n",
    "    print(\"\\nüìñ Reading back from Delta Lake:\")\n",
    "    delta_df = spark.read.format(\"delta\").load(\"s3a://test-bucket/delta-tables/enriched_trades\")\n",
    "    delta_df.show()\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No data found in Kafka topic\")\n",
    "\n",
    "print(\"‚úÖ Processing completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bf84ea-c0d7-4ea7-896f-d02a0f8ade87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e697d0a9-565c-4684-a734-5b603fe772d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e852a4c0-256d-4137-8b31-cda66a3bbd03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Spark Streaming session created\n",
      "üåä Starting streaming from Kafka...\n",
      "üåä Streaming query started! Processing every 5 seconds...\n",
      "üí° Run the producer in another terminal to see real-time processing\n",
      "üåç Weather data will be fetched for multiple cities based on exchanges\n",
      "üåä Processing streaming batch 5 with 10 records\n",
      "üå§Ô∏è Fetching weather for XNYS (New York)\n",
      "üå§Ô∏è Fetching weather for XLON (London)\n",
      "‚úÖ Batch 5 written to streaming storage with date partitioning\n",
      "üåä Processing streaming batch 6 with 15 records\n",
      "üå§Ô∏è Fetching weather for XLON (London)\n",
      "üå§Ô∏è Fetching weather for XNAS (New York)\n",
      "üå§Ô∏è Fetching weather for XNYS (New York)\n",
      "‚úÖ Batch 6 written to streaming storage with date partitioning\n",
      "üåä Processing streaming batch 7 with 10 records\n",
      "üå§Ô∏è Fetching weather for XLON (London)\n",
      "üå§Ô∏è Fetching weather for XNYS (New York)\n",
      "üå§Ô∏è Fetching weather for XNAS (New York)\n",
      "‚úÖ Batch 7 written to streaming storage with date partitioning\n",
      "üåä Processing streaming batch 8 with 5 records\n",
      "üå§Ô∏è Fetching weather for XLON (London)\n",
      "üå§Ô∏è Fetching weather for XNYS (New York)\n",
      "üå§Ô∏è Fetching weather for XNAS (New York)\n",
      "‚úÖ Batch 8 written to streaming storage with date partitioning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üõë Stopping streaming...\n",
      "‚úÖ Streaming stopped\n"
     ]
    }
   ],
   "source": [
    "# streaming_processor.py\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_json, lit, when, date_format, avg, sum as spark_sum, count\n",
    "from pyspark.sql.types import StructType, StringType, FloatType, IntegerType\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# Weather enrichment\n",
    "exchange_city_map = {\n",
    "    \"XNAS\": \"New York\",\n",
    "    \"XNYS\": \"New York\",\n",
    "    \"XLON\": \"London\",\n",
    "    \"XTKS\": \"Tokyo\"\n",
    "}\n",
    "\n",
    "weather_cache = {}\n",
    "CACHE_TTL_SECONDS = 600\n",
    "\n",
    "def fetch_weather_cached(city):\n",
    "    now = time.time()\n",
    "    cached = weather_cache.get(city)\n",
    "    \n",
    "    if cached and now - cached[\"fetched_at\"] < CACHE_TTL_SECONDS:\n",
    "        return cached[\"data\"]\n",
    "    \n",
    "    try:\n",
    "        url = f\"http://api.openweathermap.org/data/2.5/weather?q={city}&appid=34debbcec75c0e6f7351fcda60b141be&units=metric\"\n",
    "        response = requests.get(url, timeout=5)\n",
    "        data = response.json()\n",
    "        weather = {\n",
    "            \"temperature\": float(data[\"main\"][\"temp\"]),\n",
    "            \"humidity\": int(data[\"main\"][\"humidity\"]),\n",
    "            \"condition\": data[\"weather\"][0][\"description\"]\n",
    "        }\n",
    "        weather_cache[city] = {\"fetched_at\": now, \"data\": weather}\n",
    "        return weather\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Weather fetch failed for {city}: {e}\")\n",
    "        return {\"temperature\": None, \"humidity\": None, \"condition\": \"unknown\"}\n",
    "\n",
    "# Initialize Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"FinTechStreamingPipeline\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.jars.packages\", \n",
    "            \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1,\"\n",
    "            \"io.delta:delta-core_2.12:2.4.0,\"\n",
    "            \"org.apache.hadoop:hadoop-aws:3.3.4,\"\n",
    "            \"com.amazonaws:aws-java-sdk-bundle:1.12.262\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://localstack:4566\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"test\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"test\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "print(\"‚úÖ Spark Streaming session created\")\n",
    "\n",
    "# Schema for incoming data\n",
    "schema = StructType() \\\n",
    "    .add(\"symbol\", StringType()) \\\n",
    "    .add(\"price\", FloatType()) \\\n",
    "    .add(\"volume\", IntegerType()) \\\n",
    "    .add(\"timestamp\", StringType()) \\\n",
    "    .add(\"exchange\", StringType())\n",
    "\n",
    "print(\"üåä Starting streaming from Kafka...\")\n",
    "\n",
    "# STREAMING: Read from Kafka continuously\n",
    "streaming_df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \\\n",
    "    .option(\"subscribe\", \"test-topic\") \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .option(\"failOnDataLoss\", \"false\") \\\n",
    "    .load()\n",
    "\n",
    "# Parse streaming data\n",
    "parsed_stream = streaming_df.selectExpr(\"CAST(value AS STRING) as json_str\") \\\n",
    "    .select(from_json(\"json_str\", schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\") \\\n",
    "    .withColumn(\"processing_time\", lit(int(time.time())))\n",
    "\n",
    "# Enrichment function for streaming batches\n",
    "def enrich_and_store_stream(batch_df, batch_id):\n",
    "    if batch_df.count() == 0:\n",
    "        return\n",
    "    \n",
    "    print(f\"üåä Processing streaming batch {batch_id} with {batch_df.count()} records\")\n",
    "    \n",
    "    # Get weather data for unique exchanges\n",
    "    exchanges = [row['exchange'] for row in batch_df.select(\"exchange\").distinct().collect()]\n",
    "    weather_data = {}\n",
    "    \n",
    "    for ex in exchanges:\n",
    "        city = exchange_city_map.get(ex, \"New York\")\n",
    "        print(f\"üå§Ô∏è Fetching weather for {ex} ({city})\")\n",
    "        weather_data[ex] = fetch_weather_cached(city)\n",
    "    \n",
    "    # ‚úÖ SIMPLIFIED: Only add weather columns (no city)\n",
    "    enriched_df = batch_df\n",
    "    for col_name in [\"temperature\", \"humidity\", \"condition\"]:\n",
    "        enriched_df = enriched_df.withColumn(col_name, lit(None))\n",
    "    \n",
    "    for exchange, weather in weather_data.items():\n",
    "        enriched_df = enriched_df \\\n",
    "            .withColumn(\"temperature\", when(col(\"exchange\") == exchange, lit(weather.get(\"temperature\"))).otherwise(col(\"temperature\"))) \\\n",
    "            .withColumn(\"humidity\", when(col(\"exchange\") == exchange, lit(weather.get(\"humidity\"))).otherwise(col(\"humidity\"))) \\\n",
    "            .withColumn(\"condition\", when(col(\"exchange\") == exchange, lit(weather.get(\"condition\"))).otherwise(col(\"condition\")))\n",
    "    \n",
    "    # ‚úÖ ADD DATE PARTITIONING COLUMN\n",
    "    enriched_df = enriched_df \\\n",
    "        .withColumn(\"date\", date_format(col(\"timestamp\"), \"yyyy-MM-dd\"))\n",
    "    \n",
    "    # ‚úÖ WRITE WITH DATE PARTITIONING ONLY\n",
    "    enriched_df.write.format(\"delta\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .partitionBy(\"date\") \\\n",
    "        .save(\"s3a://test-bucket/delta-tables/streaming_trades\")\n",
    "    \n",
    "    print(f\"‚úÖ Batch {batch_id} written to streaming storage with date partitioning\")\n",
    "\n",
    "# Start the streaming query\n",
    "streaming_query = parsed_stream.writeStream \\\n",
    "    .foreachBatch(enrich_and_store_stream) \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .trigger(processingTime='5 seconds') \\\n",
    "    .option(\"checkpointLocation\", \"s3a://test-bucket/checkpoints/streaming\") \\\n",
    "    .start()\n",
    "\n",
    "print(\"üåä Streaming query started! Processing every 5 seconds...\")\n",
    "print(\"üí° Run the producer in another terminal to see real-time processing\")\n",
    "print(\"üåç Weather data will be fetched for multiple cities based on exchanges\")\n",
    "\n",
    "# Keep streaming running\n",
    "try:\n",
    "    streaming_query.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nüõë Stopping streaming...\")\n",
    "    streaming_query.stop()\n",
    "    print(\"‚úÖ Streaming stopped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b66cb7a-bad9-426b-a906-bb63d913987e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìñ Reading back from Delta Lake:\n",
      "+------+------+------+--------------------+--------+---------------+-----------+--------+---------------+----------+\n",
      "|symbol| price|volume|           timestamp|exchange|processing_time|temperature|humidity|      condition|      date|\n",
      "+------+------+------+--------------------+--------+---------------+-----------+--------+---------------+----------+\n",
      "|   JPM|106.87|   777|2025-08-06T20:27:...|    XNYS|     1754512047|      23.99|      63|          smoke|2025-08-06|\n",
      "|  TSLA|285.96|   684|2025-08-06T20:27:...|    XNAS|     1754512047|      23.99|      63|          smoke|2025-08-06|\n",
      "|  AAPL| 213.2|   125|2025-08-06T20:27:...|    XNAS|     1754512047|      23.99|      63|          smoke|2025-08-06|\n",
      "|   XOM|132.37|   354|2025-08-06T20:27:...|    XNYS|     1754512047|      23.99|      63|          smoke|2025-08-06|\n",
      "|  TSLA|205.22|   795|2025-08-06T20:27:...|    XNAS|     1754512047|      23.99|      63|          smoke|2025-08-06|\n",
      "|  TSLA|124.18|   654|2025-08-06T20:27:...|    XNAS|     1754512047|      23.99|      63|          smoke|2025-08-06|\n",
      "|   BAC|257.97|   377|2025-08-06T20:27:...|    XNYS|     1754512047|      23.99|      63|          smoke|2025-08-06|\n",
      "|  HSBA| 274.6|   608|2025-08-06T20:27:...|    XLON|     1754512047|      19.19|      60|overcast clouds|2025-08-06|\n",
      "|   AZN|175.29|   538|2025-08-06T20:27:...|    XLON|     1754512047|      19.19|      60|overcast clouds|2025-08-06|\n",
      "| GOOGL|121.26|   820|2025-08-06T20:27:...|    XNAS|     1754512047|      23.99|      63|          smoke|2025-08-06|\n",
      "|  MSFT|294.53|    98|2025-08-06T20:27:...|    XNAS|     1754512047|      23.99|      63|          smoke|2025-08-06|\n",
      "|  MSFT|176.57|   194|2025-08-06T20:27:...|    XNAS|     1754512047|      23.99|      63|          smoke|2025-08-06|\n",
      "|    BP|140.86|   281|2025-08-06T20:27:...|    XLON|     1754512047|      19.19|      60|overcast clouds|2025-08-06|\n",
      "|  AAPL|293.63|   539|2025-08-06T20:27:...|    XNAS|     1754512047|      23.99|      63|          smoke|2025-08-06|\n",
      "| GOOGL|155.45|   552|2025-08-06T20:27:...|    XNAS|     1754512047|      23.99|      63|          smoke|2025-08-06|\n",
      "| GOOGL|151.66|   966|2025-08-06T20:27:...|    XNAS|     1754512047|      23.99|      63|          smoke|2025-08-06|\n",
      "| GOOGL|127.95|   880|2025-08-06T20:27:...|    XNAS|     1754512047|      23.99|      63|          smoke|2025-08-06|\n",
      "|  VODL|146.76|    51|2025-08-06T20:27:...|    XLON|     1754512047|      19.19|      60|overcast clouds|2025-08-06|\n",
      "|  VODL|108.03|   114|2025-08-06T20:27:...|    XLON|     1754512047|      19.19|      60|overcast clouds|2025-08-06|\n",
      "+------+------+------+--------------------+--------+---------------+-----------+--------+---------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "    print(\"\\nüìñ Reading back from Delta Lake:\")\n",
    "    delta_df = spark.read.format(\"delta\").load(\"s3a://test-bucket/delta-tables/streaming_trades\")\n",
    "    delta_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8689b720-5374-47df-a8c9-8db3510f0c6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Spark Batch session created\n",
      "üìä Starting batch processing...\n",
      "üìà Processing 19 records from streaming storage\n",
      "üìä Hourly aggregations:\n",
      "+------+--------+-------------+----------+---------------+------------------+---------+---------+------------+-----------+---------------+------------+--------------------+\n",
      "|symbol|exchange|         hour|      date|      condition|         avg_price|min_price|max_price|total_volume|trade_count|avg_temperature|avg_humidity|  batch_processed_at|\n",
      "+------+--------+-------------+----------+---------------+------------------+---------+---------+------------+-----------+---------------+------------+--------------------+\n",
      "|  HSBA|    XLON|2025-08-06 20|2025-08-06|overcast clouds| 274.6000061035156|    274.6|    274.6|         608|          1|          19.19|        60.0|2025-08-06T20:29:...|\n",
      "|   JPM|    XNYS|2025-08-06 20|2025-08-06|          smoke|106.87000274658203|   106.87|   106.87|         777|          1|          23.99|        63.0|2025-08-06T20:29:...|\n",
      "| GOOGL|    XNAS|2025-08-06 20|2025-08-06|          smoke|139.07999992370605|   121.26|   155.45|        3218|          4|          23.99|        63.0|2025-08-06T20:29:...|\n",
      "|   BAC|    XNYS|2025-08-06 20|2025-08-06|          smoke| 257.9700012207031|   257.97|   257.97|         377|          1|          23.99|        63.0|2025-08-06T20:29:...|\n",
      "|   AZN|    XLON|2025-08-06 20|2025-08-06|overcast clouds| 175.2899932861328|   175.29|   175.29|         538|          1|          19.19|        60.0|2025-08-06T20:29:...|\n",
      "|  MSFT|    XNAS|2025-08-06 20|2025-08-06|          smoke| 235.5500030517578|   176.57|   294.53|         292|          2|          23.99|        63.0|2025-08-06T20:29:...|\n",
      "|  VODL|    XLON|2025-08-06 20|2025-08-06|overcast clouds| 127.3949966430664|   108.03|   146.76|         165|          2|          19.19|        60.0|2025-08-06T20:29:...|\n",
      "|  AAPL|    XNAS|2025-08-06 20|2025-08-06|          smoke|253.41500091552734|    213.2|   293.63|         664|          2|          23.99|        63.0|2025-08-06T20:29:...|\n",
      "|    BP|    XLON|2025-08-06 20|2025-08-06|overcast clouds|140.86000061035156|   140.86|   140.86|         281|          1|          19.19|        60.0|2025-08-06T20:29:...|\n",
      "|  TSLA|    XNAS|2025-08-06 20|2025-08-06|          smoke|  205.119997660319|   124.18|   285.96|        2133|          3|          23.99|        63.0|2025-08-06T20:29:...|\n",
      "+------+--------+-------------+----------+---------------+------------------+---------+---------+------------+-----------+---------------+------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "üìÖ Daily aggregations:\n",
      "+------+--------+----------+------------------+---------+---------+------------+-----------+---------------+------------+--------------------+\n",
      "|symbol|exchange|      date|         avg_price|min_price|max_price|total_volume|trade_count|avg_temperature|avg_humidity|  batch_processed_at|\n",
      "+------+--------+----------+------------------+---------+---------+------------+-----------+---------------+------------+--------------------+\n",
      "|   AZN|    XLON|2025-08-06| 175.2899932861328|   175.29|   175.29|         538|          1|          19.19|        60.0|2025-08-06T20:29:...|\n",
      "|  HSBA|    XLON|2025-08-06| 274.6000061035156|    274.6|    274.6|         608|          1|          19.19|        60.0|2025-08-06T20:29:...|\n",
      "|  TSLA|    XNAS|2025-08-06|  205.119997660319|   124.18|   285.96|        2133|          3|          23.99|        63.0|2025-08-06T20:29:...|\n",
      "|   BAC|    XNYS|2025-08-06| 257.9700012207031|   257.97|   257.97|         377|          1|          23.99|        63.0|2025-08-06T20:29:...|\n",
      "|  AAPL|    XNAS|2025-08-06|253.41500091552734|    213.2|   293.63|         664|          2|          23.99|        63.0|2025-08-06T20:29:...|\n",
      "|  MSFT|    XNAS|2025-08-06| 235.5500030517578|   176.57|   294.53|         292|          2|          23.99|        63.0|2025-08-06T20:29:...|\n",
      "|   XOM|    XNYS|2025-08-06| 132.3699951171875|   132.37|   132.37|         354|          1|          23.99|        63.0|2025-08-06T20:29:...|\n",
      "|  VODL|    XLON|2025-08-06| 127.3949966430664|   108.03|   146.76|         165|          2|          19.19|        60.0|2025-08-06T20:29:...|\n",
      "|   JPM|    XNYS|2025-08-06|106.87000274658203|   106.87|   106.87|         777|          1|          23.99|        63.0|2025-08-06T20:29:...|\n",
      "|    BP|    XLON|2025-08-06|140.86000061035156|   140.86|   140.86|         281|          1|          19.19|        60.0|2025-08-06T20:29:...|\n",
      "+------+--------+----------+------------------+---------+---------+------------+-----------+---------------+------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "‚úÖ Batch processing completed with date partitioning!\n"
     ]
    }
   ],
   "source": [
    "# batch_processor.py\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, date_format, avg, sum as spark_sum, count, max as spark_max, min as spark_min, lit\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Initialize Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"FinTechBatchProcessor\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.jars.packages\", \n",
    "            \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1,\"\n",
    "            \"io.delta:delta-core_2.12:2.4.0,\"\n",
    "            \"org.apache.hadoop:hadoop-aws:3.3.4,\"\n",
    "            \"com.amazonaws:aws-java-sdk-bundle:1.12.262\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://localstack:4566\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"test\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"test\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "print(\"‚úÖ Spark Batch session created\")\n",
    "\n",
    "def run_batch_aggregations(target_date=None):\n",
    "    \"\"\"\n",
    "    Run batch aggregations for a specific date.\n",
    "    If no date specified, process all available data.\n",
    "    \"\"\"\n",
    "    print(\"üìä Starting batch processing...\")\n",
    "    \n",
    "    # Read streaming data\n",
    "    streaming_data = spark.read.format(\"delta\") \\\n",
    "        .load(\"s3a://test-bucket/delta-tables/streaming_trades\")\n",
    "    \n",
    "    # Filter by specific date if provided\n",
    "    if target_date:\n",
    "        streaming_data = streaming_data.filter(col(\"date\") == target_date)\n",
    "        print(f\"üìÖ Processing data for date: {target_date}\")\n",
    "    \n",
    "    if streaming_data.count() == 0:\n",
    "        print(\"‚ö†Ô∏è No streaming data found for batch processing\")\n",
    "        return\n",
    "    \n",
    "    print(f\"üìà Processing {streaming_data.count()} records from streaming storage\")\n",
    "    \n",
    "    # ‚úÖ SIMPLIFIED HOURLY AGGREGATIONS (no city column)\n",
    "    hourly_summary = streaming_data \\\n",
    "        .withColumn(\"hour\", date_format(col(\"timestamp\"), \"yyyy-MM-dd HH\")) \\\n",
    "        .groupBy(\"symbol\", \"exchange\", \"hour\", \"date\", \"condition\") \\\n",
    "        .agg(\n",
    "            avg(\"price\").alias(\"avg_price\"),\n",
    "            spark_min(\"price\").alias(\"min_price\"),\n",
    "            spark_max(\"price\").alias(\"max_price\"),\n",
    "            spark_sum(\"volume\").alias(\"total_volume\"),\n",
    "            count(\"*\").alias(\"trade_count\"),\n",
    "            avg(\"temperature\").alias(\"avg_temperature\"),\n",
    "            avg(\"humidity\").alias(\"avg_humidity\")\n",
    "        ) \\\n",
    "        .withColumn(\"batch_processed_at\", lit(datetime.now().isoformat()))\n",
    "    \n",
    "    print(\"üìä Hourly aggregations:\")\n",
    "    hourly_summary.show(10)\n",
    "    \n",
    "    # ‚úÖ WRITE WITH DATE PARTITIONING AND PROPER MODE\n",
    "    if target_date:\n",
    "        # Replace only specific date\n",
    "        hourly_summary.write.format(\"delta\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .option(\"replaceWhere\", f\"date = '{target_date}'\") \\\n",
    "            .save(\"s3a://test-bucket/delta-tables/hourly_summaries\")\n",
    "    else:\n",
    "        # Replace all data\n",
    "        hourly_summary.write.format(\"delta\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .partitionBy(\"date\") \\\n",
    "            .save(\"s3a://test-bucket/delta-tables/hourly_summaries\")\n",
    "    \n",
    "    # ‚úÖ SIMPLIFIED DAILY AGGREGATIONS (no city column)\n",
    "    daily_summary = streaming_data \\\n",
    "        .groupBy(\"symbol\", \"exchange\", \"date\") \\\n",
    "        .agg(\n",
    "            avg(\"price\").alias(\"avg_price\"),\n",
    "            spark_min(\"price\").alias(\"min_price\"),\n",
    "            spark_max(\"price\").alias(\"max_price\"),\n",
    "            spark_sum(\"volume\").alias(\"total_volume\"),\n",
    "            count(\"*\").alias(\"trade_count\"),\n",
    "            avg(\"temperature\").alias(\"avg_temperature\"),\n",
    "            avg(\"humidity\").alias(\"avg_humidity\")\n",
    "        ) \\\n",
    "        .withColumn(\"batch_processed_at\", lit(datetime.now().isoformat()))\n",
    "    \n",
    "    print(\"üìÖ Daily aggregations:\")\n",
    "    daily_summary.show(10)\n",
    "    \n",
    "    # ‚úÖ WRITE WITH DATE PARTITIONING\n",
    "    if target_date:\n",
    "        # Replace only specific date\n",
    "        daily_summary.write.format(\"delta\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .option(\"replaceWhere\", f\"date = '{target_date}'\") \\\n",
    "            .save(\"s3a://test-bucket/delta-tables/daily_summaries\")\n",
    "    else:\n",
    "        # Replace all data\n",
    "        daily_summary.write.format(\"delta\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .partitionBy(\"date\") \\\n",
    "            .save(\"s3a://test-bucket/delta-tables/daily_summaries\")\n",
    "    \n",
    "    print(\"‚úÖ Batch processing completed with date partitioning!\")\n",
    "\n",
    "def run_incremental_batch():\n",
    "    \"\"\"\n",
    "    Process only yesterday's data (typical production scenario)\n",
    "    \"\"\"\n",
    "    yesterday = (datetime.now() - timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "    print(f\"üìÖ Running incremental batch for {yesterday}\")\n",
    "    run_batch_aggregations(target_date=yesterday)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run for all data\n",
    "    run_batch_aggregations()\n",
    "    \n",
    "    # Or run incremental (uncomment this line)\n",
    "    # run_incremental_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bf825d-be9f-475f-a583-10bbc79c7771",
   "metadata": {},
   "source": [
    "separated hourly and daily "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "499fe2d8-44a9-453f-a8ff-80c19b42d74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scripts/daily_processor.py\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, date_format, avg, sum as spark_sum, count, max as spark_max, min as spark_min, lit\n",
    "from datetime import datetime, timedelta\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def create_spark_session():\n",
    "    return SparkSession.builder \\\n",
    "        .appName(\"FinTechDailyProcessor\") \\\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "        .config(\"spark.jars.packages\", \n",
    "                \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1,\"\n",
    "                \"io.delta:delta-core_2.12:2.4.0,\"\n",
    "                \"org.apache.hadoop:hadoop-aws:3.3.4,\"\n",
    "                \"com.amazonaws:aws-java-sdk-bundle:1.12.262\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://localstack:4566\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", \"test\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", \"test\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "def run_daily_processing():\n",
    "    \"\"\"\n",
    "    Process yesterday's complete day for daily summaries.\n",
    "    \"\"\"\n",
    "    spark = create_spark_session()\n",
    "    spark.sparkContext.setLogLevel(\"WARN\")\n",
    "    \n",
    "    try:\n",
    "        # Calculate yesterday\n",
    "        yesterday = (datetime.now() - timedelta(days=0)).strftime(\"%Y-%m-%d\")\n",
    "        \n",
    "        logger.info(f\"üìÖ Processing daily data for: {yesterday}\")\n",
    "        \n",
    "        # Read all streaming data for yesterday\n",
    "        streaming_data = spark.read.format(\"delta\") \\\n",
    "            .load(\"s3a://test-bucket/delta-tables/streaming_trades\") \\\n",
    "            .filter(col(\"date\") == yesterday)\n",
    "        \n",
    "        record_count = streaming_data.count()\n",
    "        \n",
    "        if record_count == 0:\n",
    "            logger.warning(f\"‚ö†Ô∏è No streaming data found for date: {yesterday}\")\n",
    "            return\n",
    "        \n",
    "        logger.info(f\"üìà Processing {record_count} records for date: {yesterday}\")\n",
    "        \n",
    "        # Create daily aggregation\n",
    "        daily_summary = streaming_data \\\n",
    "            .groupBy(\"symbol\", \"exchange\", \"date\") \\\n",
    "            .agg(\n",
    "                avg(\"price\").alias(\"avg_price\"),\n",
    "                spark_min(\"price\").alias(\"min_price\"),\n",
    "                spark_max(\"price\").alias(\"max_price\"),\n",
    "                spark_sum(\"volume\").alias(\"total_volume\"),\n",
    "                count(\"*\").alias(\"trade_count\"),\n",
    "                avg(\"temperature\").alias(\"avg_temperature\"),\n",
    "                avg(\"humidity\").alias(\"avg_humidity\")\n",
    "            ) \\\n",
    "            .withColumn(\"batch_processed_at\", lit(datetime.now().isoformat()))\n",
    "        \n",
    "        daily_count = daily_summary.count()\n",
    "        logger.info(f\"üìÖ Created {daily_count} daily summary records\")\n",
    "        \n",
    "        if daily_count > 0:\n",
    "            # Replace only yesterday's daily summary\n",
    "            daily_summary.write.format(\"delta\") \\\n",
    "                .mode(\"overwrite\") \\\n",
    "                .option(\"replaceWhere\", f\"date = '{yesterday}'\") \\\n",
    "                .save(\"s3a://test-bucket/delta-tables/daily_summaries\")\n",
    "            \n",
    "            logger.info(f\"‚úÖ Daily summary updated for {yesterday}\")\n",
    "        \n",
    "        logger.info(f\"üéâ Daily processing completed for {yesterday}!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Daily processing failed: {str(e)}\")\n",
    "        raise\n",
    "    finally:\n",
    "        spark.stop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_daily_processing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f5427c-2fa6-4092-837e-d3b410ede6c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6ce7a52-e066-45d7-aabf-e966fe3b8385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scripts/hourly_processor.py\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, date_format, avg, sum as spark_sum, count, max as spark_max, min as spark_min, lit\n",
    "from datetime import datetime, timedelta\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def create_spark_session():\n",
    "    return SparkSession.builder \\\n",
    "        .appName(\"FinTechHourlyProcessor\") \\\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "        .config(\"spark.jars.packages\", \n",
    "                \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1,\"\n",
    "                \"io.delta:delta-core_2.12:2.4.0,\"\n",
    "                \"org.apache.hadoop:hadoop-aws:3.3.4,\"\n",
    "                \"com.amazonaws:aws-java-sdk-bundle:1.12.262\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://localstack:4566\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", \"test\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", \"test\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "def run_hourly_processing():\n",
    "    \"\"\"\n",
    "    Process the previous completed hour.\n",
    "    E.g., if it's 10:05, process 09:00-09:59\n",
    "    \"\"\"\n",
    "    spark = create_spark_session()\n",
    "    spark.sparkContext.setLogLevel(\"WARN\")\n",
    "    \n",
    "    try:\n",
    "        # Calculate previous hour\n",
    "        current_time = datetime.now()\n",
    "        previous_hour = current_time.replace(minute=0, second=0, microsecond=0) #- timedelta(hours=1)\n",
    "        target_hour = previous_hour.strftime(\"%Y-%m-%d %H\")\n",
    "        target_date = previous_hour.strftime(\"%Y-%m-%d\")\n",
    "        \n",
    "        logger.info(f\"üïê Processing hourly data for: {target_hour}\")\n",
    "        \n",
    "        # Read streaming data for the specific hour\n",
    "        streaming_data = spark.read.format(\"delta\") \\\n",
    "            .load(\"s3a://test-bucket/delta-tables/streaming_trades\") \\\n",
    "            .filter(col(\"date\") == target_date) \\\n",
    "            .filter(date_format(col(\"timestamp\"), \"yyyy-MM-dd HH\") == target_hour)\n",
    "        \n",
    "        record_count = streaming_data.count()\n",
    "        \n",
    "        if record_count == 0:\n",
    "            logger.warning(f\"‚ö†Ô∏è No streaming data found for hour: {target_hour}\")\n",
    "            return\n",
    "        \n",
    "        logger.info(f\"üìà Processing {record_count} records for hour: {target_hour}\")\n",
    "        \n",
    "        # Create hourly aggregation\n",
    "        hourly_summary = streaming_data \\\n",
    "            .withColumn(\"hour\", lit(target_hour)) \\\n",
    "            .groupBy(\"symbol\", \"exchange\", \"hour\", \"date\", \"condition\") \\\n",
    "            .agg(\n",
    "                avg(\"price\").alias(\"avg_price\"),\n",
    "                spark_min(\"price\").alias(\"min_price\"),\n",
    "                spark_max(\"price\").alias(\"max_price\"),\n",
    "                spark_sum(\"volume\").alias(\"total_volume\"),\n",
    "                count(\"*\").alias(\"trade_count\"),\n",
    "                avg(\"temperature\").alias(\"avg_temperature\"),\n",
    "                avg(\"humidity\").alias(\"avg_humidity\")\n",
    "            ) \\\n",
    "            .withColumn(\"batch_processed_at\", lit(datetime.now().isoformat()))\n",
    "        \n",
    "        hourly_count = hourly_summary.count()\n",
    "        logger.info(f\"üìä Created {hourly_count} hourly summary records\")\n",
    "        \n",
    "        if hourly_count > 0:\n",
    "            # Use merge/upsert to replace only this specific hour\n",
    "            hourly_summary.write.format(\"delta\") \\\n",
    "                .mode(\"overwrite\") \\\n",
    "                .option(\"replaceWhere\", f\"hour = '{target_hour}'\") \\\n",
    "                .save(\"s3a://test-bucket/delta-tables/hourly_summaries\")\n",
    "            \n",
    "            logger.info(f\"‚úÖ Hourly summary updated for {target_hour}\")\n",
    "        \n",
    "        logger.info(f\"üéâ Hourly processing completed for {target_hour}!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Hourly processing failed: {str(e)}\")\n",
    "        raise\n",
    "    finally:\n",
    "        spark.stop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_hourly_processing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "436f36e6-2aec-400d-8293-ba3d3ac683a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìñ Reading back from Delta Lake:\n",
      "+------+--------+----------+------------------+---------+---------+------------+-----------+------------------+------------+--------------------+\n",
      "|symbol|exchange|      date|         avg_price|min_price|max_price|total_volume|trade_count|   avg_temperature|avg_humidity|  batch_processed_at|\n",
      "+------+--------+----------+------------------+---------+---------+------------+-----------+------------------+------------+--------------------+\n",
      "|  VODL|    XLON|2025-08-12|238.65999857584634|   199.63|   294.11|         358|          3|             30.78|        34.0|2025-08-12T17:23:...|\n",
      "|  AAPL|    XNAS|2025-08-12|169.04332987467447|   155.93|   181.64|        2311|          3|31.570000000000004|        50.0|2025-08-12T17:23:...|\n",
      "|   AZN|    XLON|2025-08-12|189.44833374023438|    133.3|    262.4|        4012|          6|             30.78|        34.0|2025-08-12T17:23:...|\n",
      "|   JPM|    XNYS|2025-08-12| 230.6059997558594|   158.82|   277.61|        1810|          5|             31.57|        50.0|2025-08-12T17:23:...|\n",
      "|    GE|    XNYS|2025-08-12| 244.1066640218099|   152.04|   293.03|         702|          3|31.570000000000004|        50.0|2025-08-12T17:23:...|\n",
      "|  TSLA|    XNAS|2025-08-12|140.89749908447266|   103.58|   184.65|        1872|          4|             31.57|        50.0|2025-08-12T17:23:...|\n",
      "|  HSBA|    XLON|2025-08-12|189.52999877929688|   141.56|   221.94|        1462|          3|             30.78|        34.0|2025-08-12T17:23:...|\n",
      "|   XOM|    XNYS|2025-08-12|144.69499969482422|   142.44|   146.95|        1143|          2|             31.57|        50.0|2025-08-12T17:23:...|\n",
      "|    BP|    XLON|2025-08-12|215.06666564941406|   208.22|   221.48|         637|          3|             30.78|        34.0|2025-08-12T17:23:...|\n",
      "| GOOGL|    XNAS|2025-08-12|218.40799865722656|   186.69|   253.01|        2663|          5|             31.57|        50.0|2025-08-12T17:23:...|\n",
      "|  MSFT|    XNAS|2025-08-12|187.86999893188477|   146.94|   298.91|        1966|          4|             31.57|        50.0|2025-08-12T17:23:...|\n",
      "|   BAC|    XNYS|2025-08-12|173.26667022705078|   121.26|   205.91|         560|          3|31.570000000000004|        50.0|2025-08-12T17:23:...|\n",
      "+------+--------+----------+------------------+---------+---------+------------+-----------+------------------+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_json, lit, when, date_format, avg, sum as spark_sum, count\n",
    "from pyspark.sql.types import StructType, StringType, FloatType, IntegerType\n",
    "import requests\n",
    "import time\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"FinTechStreamingPipeline\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.jars.packages\", \n",
    "            \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1,\"\n",
    "            \"io.delta:delta-core_2.12:2.4.0,\"\n",
    "            \"org.apache.hadoop:hadoop-aws:3.3.4,\"\n",
    "            \"com.amazonaws:aws-java-sdk-bundle:1.12.262\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://localstack:4566\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"test\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"test\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .getOrCreate()\n",
    "print(\"\\nüìñ Reading back from Delta Lake:\")\n",
    "delta_df = spark.read.format(\"delta\").load(\"s3a://test-bucket/delta-tables/daily_summaries\")\n",
    "delta_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233eb8eb-9e2b-4c78-b684-6189bd65ce94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
