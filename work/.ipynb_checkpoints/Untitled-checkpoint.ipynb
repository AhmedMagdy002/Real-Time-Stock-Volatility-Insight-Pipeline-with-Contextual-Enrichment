{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c847ac-c457-46ae-95bc-f8e60b2ce4fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Bucket created!\n",
      "✅ Job completed!\n",
      "\n",
      "Files in bucket:\n",
      "  - output.parquet/_SUCCESS\n",
      "  - output.parquet/part-00000-c1138f74-f87f-4e02-ba73-140ad38069cf-c000.snappy.parquet\n",
      "  - output.parquet/part-00001-c1138f74-f87f-4e02-ba73-140ad38069cf-c000.snappy.parquet\n",
      "  - output.parquet/part-00002-c1138f74-f87f-4e02-ba73-140ad38069cf-c000.snappy.parquet\n",
      "  - output.parquet/part-00003-c1138f74-f87f-4e02-ba73-140ad38069cf-c000.snappy.parquet\n",
      "  - output.parquet/part-00004-c1138f74-f87f-4e02-ba73-140ad38069cf-c000.snappy.parquet\n",
      "  - output.parquet/part-00005-c1138f74-f87f-4e02-ba73-140ad38069cf-c000.snappy.parquet\n",
      "  - output.parquet/part-00006-c1138f74-f87f-4e02-ba73-140ad38069cf-c000.snappy.parquet\n",
      "  - output.parquet/part-00007-c1138f74-f87f-4e02-ba73-140ad38069cf-c000.snappy.parquet\n",
      "  - output.parquet/part-00008-c1138f74-f87f-4e02-ba73-140ad38069cf-c000.snappy.parquet\n",
      "  - output.parquet/part-00009-c1138f74-f87f-4e02-ba73-140ad38069cf-c000.snappy.parquet\n",
      "  - output.parquet/part-00010-c1138f74-f87f-4e02-ba73-140ad38069cf-c000.snappy.parquet\n",
      "  - output.parquet/part-00011-c1138f74-f87f-4e02-ba73-140ad38069cf-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "%run /home/jovyan/work/testing.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adafd712-f11d-44a0-a186-9a8be0131af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Spark is ready!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TestS3\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.4\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://localstack:4566\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"test\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"test\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"✅ Spark is ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40e1617f-8d0c-407a-afa0-564c5638f5be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| id|doubled|\n",
      "+---+-------+\n",
      "| 16|     32|\n",
      "| 17|     34|\n",
      "| 18|     36|\n",
      "| 19|     38|\n",
      "| 20|     40|\n",
      "| 21|     42|\n",
      "| 22|     44|\n",
      "| 23|     46|\n",
      "| 24|     48|\n",
      "| 41|     82|\n",
      "| 42|     84|\n",
      "| 43|     86|\n",
      "| 44|     88|\n",
      "| 45|     90|\n",
      "| 46|     92|\n",
      "| 47|     94|\n",
      "| 48|     96|\n",
      "| 49|     98|\n",
      "| 66|    132|\n",
      "| 67|    134|\n",
      "+---+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read back the data\n",
    "df_read = spark.read.parquet(\"s3a://test-bucket/output.parquet\")\n",
    "df_read.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560e00eb-5b68-4ed3-9a87-11f5cedf56a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2268dd3-131b-41c5-a457-f13df5397c87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1b35b47a",
   "metadata": {},
   "source": [
    "### Kafka spark\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfc6d8c4-b887-4bad-add8-345b975247fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting boto3\n",
      "  Obtaining dependency information for boto3 from https://files.pythonhosted.org/packages/97/0e/f0cb4f71c40ba07e6ed5b47699a737a080d3c4f4b7b26657d5671de48621/boto3-1.40.1-py3-none-any.whl.metadata\n",
      "  Downloading boto3-1.40.1-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting botocore<1.41.0,>=1.40.1 (from boto3)\n",
      "  Obtaining dependency information for botocore<1.41.0,>=1.40.1 from https://files.pythonhosted.org/packages/d4/c1/aa7922c9bf74b6d6594d2430af6f854d234faff23187e269aaba89c326c8/botocore-1.40.1-py3-none-any.whl.metadata\n",
      "  Downloading botocore-1.40.1-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from boto3)\n",
      "  Obtaining dependency information for jmespath<2.0.0,>=0.7.1 from https://files.pythonhosted.org/packages/31/b4/b9b800c45527aadd64d5b442f9b932b00648617eb5d63d2c7a6587b7cafc/jmespath-1.0.1-py3-none-any.whl.metadata\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting s3transfer<0.14.0,>=0.13.0 (from boto3)\n",
      "  Obtaining dependency information for s3transfer<0.14.0,>=0.13.0 from https://files.pythonhosted.org/packages/6d/4f/d073e09df851cfa251ef7840007d04db3293a0482ce607d2b993926089be/s3transfer-0.13.1-py3-none-any.whl.metadata\n",
      "  Downloading s3transfer-0.13.1-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.11/site-packages (from botocore<1.41.0,>=1.40.1->boto3) (2.8.2)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /opt/conda/lib/python3.11/site-packages (from botocore<1.41.0,>=1.40.1->boto3) (2.0.4)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.41.0,>=1.40.1->boto3) (1.16.0)\n",
      "Downloading boto3-1.40.1-py3-none-any.whl (139 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.9/139.9 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading botocore-1.40.1-py3-none-any.whl (13.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.9/13.9 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Downloading s3transfer-0.13.1-py3-none-any.whl (85 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.3/85.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: jmespath, botocore, s3transfer, boto3\n",
      "Successfully installed boto3-1.40.1 botocore-1.40.1 jmespath-1.0.1 s3transfer-0.13.1\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests) (2023.7.22)\n"
     ]
    }
   ],
   "source": [
    "!pip install boto3\n",
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74aca7f8-c8c9-4153-b5c4-4438a157b5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket created!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import boto3\n",
    "s3 = boto3.client('s3', endpoint_url='http://localstack:4566', \n",
    "                  aws_access_key_id='test', aws_secret_access_key='test')\n",
    "s3.create_bucket(Bucket='test-bucket')\n",
    "print(\"Bucket created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b1d9a14-bfb5-4afd-bd8d-b9d8e6fd0446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kafka-python\n",
      "  Obtaining dependency information for kafka-python from https://files.pythonhosted.org/packages/e6/35/e8bfed5425e8fe685bd03ec3f5135ee8b88c11558baa59c0d12fbd2a20ae/kafka_python-2.2.15-py2.py3-none-any.whl.metadata\n",
      "  Downloading kafka_python-2.2.15-py2.py3-none-any.whl.metadata (10.0 kB)\n",
      "Downloading kafka_python-2.2.15-py2.py3-none-any.whl (309 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.8/309.8 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: kafka-python\n",
      "Successfully installed kafka-python-2.2.15\n"
     ]
    }
   ],
   "source": [
    "!pip install kafka-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb6f4b82-00e4-4f5a-9704-4db910db3bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic created!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from kafka.admin import KafkaAdminClient, NewTopic\n",
    "\n",
    "admin = KafkaAdminClient(bootstrap_servers='kafka:29092')\n",
    "topic = NewTopic(name='test-topic', num_partitions=1, replication_factor=1)\n",
    "admin.create_topics([topic])\n",
    "print(\"Topic created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e47686bf-f20a-4b75-814e-4115b52267cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent: {'id': 0, 'name': 'Message 0', 'value': 0}\n",
      "Sent: {'id': 1, 'name': 'Message 1', 'value': 100}\n",
      "Sent: {'id': 2, 'name': 'Message 2', 'value': 200}\n",
      "Sent: {'id': 3, 'name': 'Message 3', 'value': 300}\n",
      "Sent: {'id': 4, 'name': 'Message 4', 'value': 400}\n",
      "Sent: {'id': 5, 'name': 'Message 5', 'value': 500}\n",
      "Sent: {'id': 6, 'name': 'Message 6', 'value': 600}\n",
      "Sent: {'id': 7, 'name': 'Message 7', 'value': 700}\n",
      "Sent: {'id': 8, 'name': 'Message 8', 'value': 800}\n",
      "Sent: {'id': 9, 'name': 'Message 9', 'value': 900}\n",
      "Done sending messages!\n"
     ]
    }
   ],
   "source": [
    "%run /home/jovyan/work/simple_producer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "538079dc-4609-433e-8b3b-c2aabcfb1f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Spark session created\n",
      "📖 Reading from Kafka...\n",
      "\n",
      "📊 Data from Kafka:\n",
      "+---+---------+-----+\n",
      "| id|     name|value|\n",
      "+---+---------+-----+\n",
      "|  0|Message 0|    0|\n",
      "|  1|Message 1|  100|\n",
      "|  2|Message 2|  200|\n",
      "|  3|Message 3|  300|\n",
      "|  4|Message 4|  400|\n",
      "|  5|Message 5|  500|\n",
      "|  6|Message 6|  600|\n",
      "|  7|Message 7|  700|\n",
      "|  8|Message 8|  800|\n",
      "|  9|Message 9|  900|\n",
      "+---+---------+-----+\n",
      "\n",
      "\n",
      "💾 Writing to Delta Lake at: s3a://test-bucket/delta-tables/my_table\n",
      "✅ Data saved to Delta Lake!\n",
      "\n",
      "📖 Reading back from Delta Lake:\n",
      "+---+---------+-----+\n",
      "| id|     name|value|\n",
      "+---+---------+-----+\n",
      "|  0|Message 0|    0|\n",
      "|  1|Message 1|  100|\n",
      "|  2|Message 2|  200|\n",
      "|  3|Message 3|  300|\n",
      "|  4|Message 4|  400|\n",
      "|  5|Message 5|  500|\n",
      "|  6|Message 6|  600|\n",
      "|  7|Message 7|  700|\n",
      "|  8|Message 8|  800|\n",
      "|  9|Message 9|  900|\n",
      "+---+---------+-----+\n",
      "\n",
      "\n",
      "✅ Test completed successfully!\n"
     ]
    }
   ],
   "source": [
    "%run /home/jovyan/work/simple_spark_consumer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587d7973-640b-4f56-8ba0-334748ae3c63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f91ad21-0237-4470-95b1-76e21da7a99c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fc5e868-5c51-445a-b6c7-cec0af444859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in s3://test-bucket/delta-table/:\n",
      " - delta-table/_delta_log/00000000000000000000.json (1248 bytes)\n",
      " - delta-table/part-00000-354a5e3f-e191-438b-bbac-aec33463ff8b-c000.snappy.parquet (1177 bytes)\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "# Configure boto3 for LocalStack\n",
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    endpoint_url='http://localstack:4566',\n",
    "    aws_access_key_id='test',\n",
    "    aws_secret_access_key='test',\n",
    "    region_name='us-east-1'\n",
    ")\n",
    "\n",
    "# List objects in the Delta table path\n",
    "try:\n",
    "    response = s3_client.list_objects_v2(Bucket='test-bucket', Prefix='delta-table/')\n",
    "    if 'Contents' in response:\n",
    "        print(\"Files in s3://test-bucket/delta-table/:\")\n",
    "        for obj in response['Contents']:\n",
    "            print(f\" - {obj['Key']} ({obj['Size']} bytes)\")\n",
    "    else:\n",
    "        print(\"No files found in s3://test-bucket/delta-table/ or bucket does not exist.\")\n",
    "except s3_client.exceptions.NoSuchBucket:\n",
    "    print(\"Bucket 'test-bucket' does not exist. Creating it...\")\n",
    "    s3_client.create_bucket(Bucket='test-bucket')\n",
    "    print(\"Bucket created. Rerun the Spark job to write the Delta table.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3461fa-5cdc-4560-8e78-7e020b78a94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark session with Delta support\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SimpleTest\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.jars.packages\", \n",
    "            \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1,\"\n",
    "            \"io.delta:delta-core_2.12:2.4.0,\"\n",
    "            \"org.apache.hadoop:hadoop-aws:3.3.4,\"\n",
    "            \"com.amazonaws:aws-java-sdk-bundle:1.12.262\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://localstack:4566\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"test\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"test\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .getOrCreate()\n",
    "output_path = \"s3a://test-bucket/delta-table\"\n",
    "print(\"\\n📖 Reading back from Delta Lake:\")\n",
    "delta_df = spark.read.format(\"delta\").load(output_path)\n",
    "delta_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3258614a-3415-4299-8156-089c4961974c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdec18f-eea3-4b91-853d-be3bfd2b6b02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "227f47f6-f8b5-4403-8286-087807389ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Spark session created\n",
      "📖 Reading from Kafka...\n",
      "\n",
      "📊 Data from Kafka:\n",
      "+---+---------+-----+\n",
      "| id|     name|value|\n",
      "+---+---------+-----+\n",
      "|  0|Message 0|    0|\n",
      "|  1|Message 1|  100|\n",
      "|  2|Message 2|  200|\n",
      "|  3|Message 3|  300|\n",
      "|  4|Message 4|  400|\n",
      "|  5|Message 5|  500|\n",
      "|  6|Message 6|  600|\n",
      "|  7|Message 7|  700|\n",
      "|  8|Message 8|  800|\n",
      "|  9|Message 9|  900|\n",
      "+---+---------+-----+\n",
      "\n",
      "\n",
      "💾 Writing to Delta Lake at: s3a://test-bucket/delta-tables/my_table\n",
      "✅ Data saved to Delta Lake!\n",
      "\n",
      "📖 Reading back from Delta Lake:\n",
      "+---+---------+-----+\n",
      "| id|     name|value|\n",
      "+---+---------+-----+\n",
      "|  0|Message 0|    0|\n",
      "|  1|Message 1|  100|\n",
      "|  2|Message 2|  200|\n",
      "|  3|Message 3|  300|\n",
      "|  4|Message 4|  400|\n",
      "|  5|Message 5|  500|\n",
      "|  6|Message 6|  600|\n",
      "|  7|Message 7|  700|\n",
      "|  8|Message 8|  800|\n",
      "|  9|Message 9|  900|\n",
      "+---+---------+-----+\n",
      "\n",
      "\n",
      "✅ Test completed successfully!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Create Spark session with Delta support\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SimpleTest\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.jars.packages\", \n",
    "            \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1,\"\n",
    "            \"io.delta:delta-core_2.12:2.4.0,\"\n",
    "            \"org.apache.hadoop:hadoop-aws:3.3.4,\"\n",
    "            \"com.amazonaws:aws-java-sdk-bundle:1.12.262\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://localstack:4566\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"test\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"test\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"✅ Spark session created\")\n",
    "\n",
    "# Read from Kafka\n",
    "print(\"📖 Reading from Kafka...\")\n",
    "df = spark \\\n",
    "    .read \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \\\n",
    "    .option(\"subscribe\", \"test-topic\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "# Parse JSON and select fields\n",
    "parsed_df = df.select(\n",
    "    col(\"value\").cast(\"string\")\n",
    ").select(\n",
    "    get_json_object(col(\"value\"), \"$.id\").alias(\"id\"),\n",
    "    get_json_object(col(\"value\"), \"$.name\").alias(\"name\"),\n",
    "    get_json_object(col(\"value\"), \"$.value\").alias(\"value\")\n",
    ")\n",
    "\n",
    "# Show data\n",
    "print(\"\\n📊 Data from Kafka:\")\n",
    "parsed_df.show()\n",
    "\n",
    "# Write to Delta Lake on S3\n",
    "#output_path = \"s3a://test-bucket/delta-table\"\n",
    "output_path = \"s3a://test-bucket/delta-tables/my_table\"\n",
    "print(f\"\\n💾 Writing to Delta Lake at: {output_path}\")\n",
    "parsed_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(output_path)\n",
    "\n",
    "print(\"✅ Data saved to Delta Lake!\")\n",
    "\n",
    "# Read back from Delta to verify\n",
    "print(\"\\n📖 Reading back from Delta Lake:\")\n",
    "delta_df = spark.read.format(\"delta\").load(output_path)\n",
    "delta_df.show()\n",
    "\n",
    "print(\"\\n✅ Test completed successfully!\")\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e5da1b-26c2-401d-8aa6-d8e11343e62f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c1fb65-c847-42f8-82a2-873c301aba04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "228c6eac-f4fd-4a63-a868-f99cdef7bc52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Sent: {'symbol': 'GOOGL', 'price': 289.41, 'volume': 877, 'timestamp': '2025-08-04T15:39:32.196996', 'exchange': 'XNAS'}\n",
      "✅ Sent: {'symbol': 'GOOGL', 'price': 298.55, 'volume': 36, 'timestamp': '2025-08-04T15:39:33.355669', 'exchange': 'XNAS'}\n",
      "✅ Sent: {'symbol': 'MSFT', 'price': 275.08, 'volume': 850, 'timestamp': '2025-08-04T15:39:34.356757', 'exchange': 'XNAS'}\n",
      "✅ Sent: {'symbol': 'GOOGL', 'price': 205.54, 'volume': 737, 'timestamp': '2025-08-04T15:39:35.358286', 'exchange': 'XNAS'}\n",
      "✅ Sent: {'symbol': 'TSLA', 'price': 152.65, 'volume': 780, 'timestamp': '2025-08-04T15:39:36.359188', 'exchange': 'XNAS'}\n",
      "✅ Sent: {'symbol': 'TSLA', 'price': 246.13, 'volume': 401, 'timestamp': '2025-08-04T15:39:37.359791', 'exchange': 'XNAS'}\n",
      "✅ Sent: {'symbol': 'MSFT', 'price': 209.88, 'volume': 114, 'timestamp': '2025-08-04T15:39:38.360518', 'exchange': 'XNAS'}\n",
      "✅ Sent: {'symbol': 'AAPL', 'price': 189.29, 'volume': 299, 'timestamp': '2025-08-04T15:39:39.361496', 'exchange': 'XNAS'}\n",
      "✅ Sent: {'symbol': 'AAPL', 'price': 231.23, 'volume': 487, 'timestamp': '2025-08-04T15:39:40.362404', 'exchange': 'XNAS'}\n",
      "✅ Sent: {'symbol': 'TSLA', 'price': 291.49, 'volume': 32, 'timestamp': '2025-08-04T15:39:41.363468', 'exchange': 'XNAS'}\n",
      "✅ Sent: {'symbol': 'GOOGL', 'price': 197.71, 'volume': 962, 'timestamp': '2025-08-04T15:39:42.364053', 'exchange': 'XNAS'}\n",
      "✅ Sent: {'symbol': 'GOOGL', 'price': 265.85, 'volume': 746, 'timestamp': '2025-08-04T15:39:43.364715', 'exchange': 'XNAS'}\n",
      "✅ Sent: {'symbol': 'AAPL', 'price': 291.7, 'volume': 691, 'timestamp': '2025-08-04T15:39:44.365142', 'exchange': 'XNAS'}\n",
      "✅ Sent: {'symbol': 'GOOGL', 'price': 219.37, 'volume': 608, 'timestamp': '2025-08-04T15:39:45.366003', 'exchange': 'XNAS'}\n",
      "✅ Sent: {'symbol': 'MSFT', 'price': 112.68, 'volume': 619, 'timestamp': '2025-08-04T15:39:46.367047', 'exchange': 'XNAS'}\n",
      "✅ Sent: {'symbol': 'AAPL', 'price': 280.35, 'volume': 32, 'timestamp': '2025-08-04T15:39:47.367468', 'exchange': 'XNAS'}\n",
      "✅ Sent: {'symbol': 'TSLA', 'price': 203.66, 'volume': 769, 'timestamp': '2025-08-04T15:39:48.368080', 'exchange': 'XNAS'}\n",
      "✅ Sent: {'symbol': 'GOOGL', 'price': 217.16, 'volume': 553, 'timestamp': '2025-08-04T15:39:49.370209', 'exchange': 'XNAS'}\n",
      "✅ Sent: {'symbol': 'MSFT', 'price': 248.3, 'volume': 812, 'timestamp': '2025-08-04T15:39:50.370690', 'exchange': 'XNAS'}\n",
      "✅ Sent: {'symbol': 'GOOGL', 'price': 167.79, 'volume': 883, 'timestamp': '2025-08-04T15:39:51.371120', 'exchange': 'XNAS'}\n",
      "🚀 Finished sending simulated trades.\n"
     ]
    }
   ],
   "source": [
    "%run /home/jovyan/work/prod.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52012d4f-71c9-4045-adae-f856a982fe24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Spark session created\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'StructField' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/work/stock_consumer.py:65\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Spark session created\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Define schema matching prod.py\u001b[39;00m\n\u001b[1;32m     64\u001b[0m schema \u001b[38;5;241m=\u001b[39m StructType([\n\u001b[0;32m---> 65\u001b[0m     \u001b[43mStructField\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msymbol\u001b[39m\u001b[38;5;124m\"\u001b[39m, StringType()),\n\u001b[1;32m     66\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprice\u001b[39m\u001b[38;5;124m\"\u001b[39m, FloatType()),\n\u001b[1;32m     67\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvolume\u001b[39m\u001b[38;5;124m\"\u001b[39m, IntegerType()),\n\u001b[1;32m     68\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m, StringType()),\n\u001b[1;32m     69\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexchange\u001b[39m\u001b[38;5;124m\"\u001b[39m, StringType())\n\u001b[1;32m     70\u001b[0m ])\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# Read stream from Kafka\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m📖 Reading stream from Kafka...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'StructField' is not defined"
     ]
    }
   ],
   "source": [
    "%run /home/jovyan/work/stock_consumer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "04ff9ddf-f933-47ae-a218-836ef3b750a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Spark session created\n",
      "📖 Reading from Kafka...\n",
      "\n",
      "🔍 Raw Kafka data:\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "|raw_value                                                                                                         |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "|{\"symbol\": \"MSFT\", \"price\": 154.31, \"volume\": 622, \"timestamp\": \"2025-08-04T17:39:06.433532\", \"exchange\": \"XNAS\"} |\n",
      "|{\"symbol\": \"GOOGL\", \"price\": 188.03, \"volume\": 656, \"timestamp\": \"2025-08-04T17:39:07.545608\", \"exchange\": \"XNAS\"}|\n",
      "|{\"symbol\": \"TSLA\", \"price\": 174.87, \"volume\": 519, \"timestamp\": \"2025-08-04T17:39:08.546064\", \"exchange\": \"XNAS\"} |\n",
      "|{\"symbol\": \"AAPL\", \"price\": 166.18, \"volume\": 881, \"timestamp\": \"2025-08-04T17:39:09.546507\", \"exchange\": \"XNAS\"} |\n",
      "|{\"symbol\": \"TSLA\", \"price\": 168.09, \"volume\": 163, \"timestamp\": \"2025-08-04T17:39:10.547317\", \"exchange\": \"XNAS\"} |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "📊 Parsed data:\n",
      "+------+------+------+--------------------+--------+\n",
      "|symbol| price|volume|           timestamp|exchange|\n",
      "+------+------+------+--------------------+--------+\n",
      "|  MSFT|154.31|   622|2025-08-04T17:39:...|    XNAS|\n",
      "| GOOGL|188.03|   656|2025-08-04T17:39:...|    XNAS|\n",
      "|  TSLA|174.87|   519|2025-08-04T17:39:...|    XNAS|\n",
      "|  AAPL|166.18|   881|2025-08-04T17:39:...|    XNAS|\n",
      "|  TSLA|168.09|   163|2025-08-04T17:39:...|    XNAS|\n",
      "|  AAPL|293.89|   883|2025-08-04T17:39:...|    XNAS|\n",
      "|  MSFT|125.53|   881|2025-08-04T17:39:...|    XNAS|\n",
      "| GOOGL|212.36|   853|2025-08-04T17:39:...|    XNAS|\n",
      "|  TSLA|172.61|   370|2025-08-04T17:39:...|    XNAS|\n",
      "|  MSFT| 274.2|   276|2025-08-04T17:39:...|    XNAS|\n",
      "|  TSLA|248.99|   898|2025-08-04T17:39:...|    XNAS|\n",
      "|  MSFT|232.12|   656|2025-08-04T17:39:...|    XNAS|\n",
      "| GOOGL|183.06|   261|2025-08-04T17:39:...|    XNAS|\n",
      "|  TSLA|250.19|   127|2025-08-04T17:39:...|    XNAS|\n",
      "|  TSLA|216.91|   636|2025-08-04T17:39:...|    XNAS|\n",
      "|  AAPL|114.12|   259|2025-08-04T17:39:...|    XNAS|\n",
      "|  AAPL|104.07|   685|2025-08-04T17:39:...|    XNAS|\n",
      "|  MSFT|206.21|   795|2025-08-04T17:39:...|    XNAS|\n",
      "|  AAPL|233.73|   236|2025-08-04T17:39:...|    XNAS|\n",
      "|  TSLA|258.76|   837|2025-08-04T17:39:...|    XNAS|\n",
      "+------+------+------+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "📦 Processing 80 records\n",
      "🌍 Exchanges found: ['XNAS']\n",
      "🌤️ Fetching weather for XNAS (New York)\n",
      "\n",
      "🌟 Enriched data:\n",
      "+------+------+------+--------------------+--------+-----------+--------+---------+\n",
      "|symbol| price|volume|           timestamp|exchange|temperature|humidity|condition|\n",
      "+------+------+------+--------------------+--------+-----------+--------+---------+\n",
      "|  MSFT|154.31|   622|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "| GOOGL|188.03|   656|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "|  TSLA|174.87|   519|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "|  AAPL|166.18|   881|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "|  TSLA|168.09|   163|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "|  AAPL|293.89|   883|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "|  MSFT|125.53|   881|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "| GOOGL|212.36|   853|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "|  TSLA|172.61|   370|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "|  MSFT| 274.2|   276|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "|  TSLA|248.99|   898|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "|  MSFT|232.12|   656|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "| GOOGL|183.06|   261|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "|  TSLA|250.19|   127|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "|  TSLA|216.91|   636|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "|  AAPL|114.12|   259|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "|  AAPL|104.07|   685|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "|  MSFT|206.21|   795|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "|  AAPL|233.73|   236|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "|  TSLA|258.76|   837|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "+------+------+------+--------------------+--------+-----------+--------+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "\n",
      "💾 Writing to Delta Lake...\n",
      "✅ Data written to Delta Lake!\n",
      "\n",
      "📖 Reading back from Delta Lake:\n",
      "+------+------+------+--------------------+--------+-----------+--------+---------+\n",
      "|symbol| price|volume|           timestamp|exchange|temperature|humidity|condition|\n",
      "+------+------+------+--------------------+--------+-----------+--------+---------+\n",
      "|  MSFT|154.31|   622|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "| GOOGL|188.03|   656|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "|  TSLA|174.87|   519|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "|  AAPL|166.18|   881|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "|  TSLA|168.09|   163|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "|  AAPL|293.89|   883|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "|  MSFT|125.53|   881|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "| GOOGL|212.36|   853|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "|  TSLA|172.61|   370|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "|  MSFT| 274.2|   276|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "|  TSLA|248.99|   898|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "|  MSFT|232.12|   656|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "| GOOGL|183.06|   261|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "|  TSLA|250.19|   127|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "|  TSLA|216.91|   636|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "|  AAPL|114.12|   259|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "|  AAPL|104.07|   685|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "|  MSFT|206.21|   795|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "|  AAPL|233.73|   236|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "|  TSLA|258.76|   837|2025-08-04T17:39:...|    XNAS|      30.52|      39|clear sky|\n",
      "+------+------+------+--------------------+--------+-----------+--------+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "✅ Processing completed!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_json, expr\n",
    "from pyspark.sql.types import StructType, StringType, FloatType, IntegerType\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# Exchange → City mapping\n",
    "exchange_city_map = {\n",
    "    \"XNAS\": \"New York\",\n",
    "    \"XNYS\": \"New York\", \n",
    "    \"XLON\": \"London\",\n",
    "    \"XTKS\": \"Tokyo\"\n",
    "}\n",
    "\n",
    "# Cache for weather per city\n",
    "weather_cache = {}\n",
    "CACHE_TTL_SECONDS = 600  # 10 minutes\n",
    "\n",
    "def fetch_weather_cached(city):\n",
    "    now = time.time()\n",
    "    cached = weather_cache.get(city)\n",
    "\n",
    "    if cached and now - cached[\"fetched_at\"] < CACHE_TTL_SECONDS:\n",
    "        return cached[\"data\"]\n",
    "\n",
    "    try:\n",
    "        url = f\"http://api.openweathermap.org/data/2.5/weather?q={city}&appid=34debbcec75c0e6f7351fcda60b141be&units=metric\"\n",
    "        response = requests.get(url, timeout=5)\n",
    "        data = response.json()\n",
    "        weather = {\n",
    "            \"temperature\": data[\"main\"][\"temp\"],\n",
    "            \"humidity\": data[\"main\"][\"humidity\"],\n",
    "            \"condition\": data[\"weather\"][0][\"description\"]\n",
    "        }\n",
    "        # Save to cache\n",
    "        weather_cache[city] = {\n",
    "            \"fetched_at\": now,\n",
    "            \"data\": weather\n",
    "        }\n",
    "        return weather\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Weather fetch failed for {city}: {e}\")\n",
    "        return {\"temperature\": None, \"humidity\": None, \"condition\": None}\n",
    "\n",
    "# Initialize Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"StockWeatherEnrichment\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.jars.packages\", \n",
    "            \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1,\"\n",
    "            \"io.delta:delta-core_2.12:2.4.0,\"\n",
    "            \"org.apache.hadoop:hadoop-aws:3.3.4,\"\n",
    "            \"com.amazonaws:aws-java-sdk-bundle:1.12.262\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://localstack:4566\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"test\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"test\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "print(\"✅ Spark session created\")\n",
    "\n",
    "# CORRECT schema matching your producer data\n",
    "schema = StructType() \\\n",
    "    .add(\"symbol\", StringType()) \\\n",
    "    .add(\"price\", FloatType()) \\\n",
    "    .add(\"volume\", IntegerType()) \\\n",
    "    .add(\"timestamp\", StringType()) \\\n",
    "    .add(\"exchange\", StringType())\n",
    "\n",
    "print(\"📖 Reading from Kafka...\")\n",
    "\n",
    "# Read from Kafka (batch mode for testing)\n",
    "df = spark \\\n",
    "    .read \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \\\n",
    "    .option(\"subscribe\", \"test-topic\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "# First, let's see the raw data\n",
    "print(\"\\n🔍 Raw Kafka data:\")\n",
    "df.selectExpr(\"CAST(value AS STRING) as raw_value\").show(5, truncate=False)\n",
    "\n",
    "# Parse JSON with correct schema\n",
    "parsed_df = df.selectExpr(\"CAST(value AS STRING) as json_str\") \\\n",
    "    .select(from_json(\"json_str\", schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\")\n",
    "\n",
    "print(\"\\n📊 Parsed data:\")\n",
    "parsed_df.show()\n",
    "\n",
    "# Test enrichment function\n",
    "def enrich_with_weather_batch(input_df):\n",
    "    if input_df.count() == 0:\n",
    "        print(\"⚠️ No data to process\")\n",
    "        return input_df\n",
    "    \n",
    "    print(f\"📦 Processing {input_df.count()} records\")\n",
    "    \n",
    "    # Get unique exchanges\n",
    "    exchanges = [row['exchange'] for row in input_df.select(\"exchange\").distinct().collect()]\n",
    "    print(f\"🌍 Exchanges found: {exchanges}\")\n",
    "    \n",
    "    # Fetch weather for each exchange\n",
    "    weather_data = {}\n",
    "    for ex in exchanges:\n",
    "        city = exchange_city_map.get(ex, \"New York\")\n",
    "        print(f\"🌤️ Fetching weather for {ex} ({city})\")\n",
    "        weather_data[ex] = fetch_weather_cached(city)\n",
    "    \n",
    "    # Add weather columns using broadcast\n",
    "    from pyspark.sql.functions import lit, when\n",
    "    \n",
    "    enriched_df = input_df\n",
    "    \n",
    "    # Add weather columns based on exchange\n",
    "    for exchange, weather in weather_data.items():\n",
    "        enriched_df = enriched_df \\\n",
    "            .withColumn(\"temperature\", \n",
    "                when(col(\"exchange\") == exchange, lit(weather.get(\"temperature\")))\n",
    "                .otherwise(col(\"temperature\") if \"temperature\" in enriched_df.columns else lit(None))) \\\n",
    "            .withColumn(\"humidity\",\n",
    "                when(col(\"exchange\") == exchange, lit(weather.get(\"humidity\")))\n",
    "                .otherwise(col(\"humidity\") if \"humidity\" in enriched_df.columns else lit(None))) \\\n",
    "            .withColumn(\"condition\",\n",
    "                when(col(\"exchange\") == exchange, lit(weather.get(\"condition\")))\n",
    "                .otherwise(col(\"condition\") if \"condition\" in enriched_df.columns else lit(None)))\n",
    "    \n",
    "    return enriched_df\n",
    "\n",
    "# Process the data\n",
    "if parsed_df.count() > 0:\n",
    "    enriched_df = enrich_with_weather_batch(parsed_df)\n",
    "    \n",
    "    print(\"\\n🌟 Enriched data:\")\n",
    "    enriched_df.show()\n",
    "    \n",
    "    # Write to Delta Lake\n",
    "    print(\"\\n💾 Writing to Delta Lake...\")\n",
    "    enriched_df.write.format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .save(\"s3a://test-bucket/delta-tables/enriched_trades\")\n",
    "    \n",
    "    print(\"✅ Data written to Delta Lake!\")\n",
    "    \n",
    "    # Read back to verify\n",
    "    print(\"\\n📖 Reading back from Delta Lake:\")\n",
    "    delta_df = spark.read.format(\"delta\").load(\"s3a://test-bucket/delta-tables/enriched_trades\")\n",
    "    delta_df.show()\n",
    "    \n",
    "else:\n",
    "    print(\"⚠️ No data found in Kafka topic\")\n",
    "\n",
    "print(\"✅ Processing completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bf84ea-c0d7-4ea7-896f-d02a0f8ade87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
